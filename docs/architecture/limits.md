# ğŸš§ Limitations & Tradeâ€‘offs

This page lists the most important constraints and design tradeâ€‘offs so you can plan deployments and avoid surprises. Each item links to the code or related docs for verification.

## ğŸ’¾ Durability and Recovery

- No WAL recovery: There is no writeâ€‘ahead log to replay after a crash. Durability boundaries are explicit `flushAndWait()` and `close()`. Writes still in the index write buffer (inâ€‘memory `UniqueCache`) are not durable until flushed. See Recovery.
- Perâ€‘file atomicity only: Writers use temp files + atomic rename; groups of files (e.g., SST + scarce index + bloom) commit in a safe order but not as a single atomic unit. Readers remain consistent because old files stay in place until each rename. See SegmentFullWriterTx, BloomFilterWriterTx.
- Filesystem requirement: Crash safety relies on sameâ€‘directory atomic `rename`. Use local filesystems; be cautious with network filesystems that may not guarantee strict atomicity.
- Stale lock files: A crash can leave `.lock` behind, preventing open until removed. See `directory/FsFileLock.java` and IndexState*. Remove the file only when certain no process still uses the directory.

## ğŸ”’ Concurrency

- SegmentIndex is threadâ€‘safe and not globally serialized; heavy concurrent writes can increase contention in shared caches and per-segment state machines.
- No serialized adapter is provided; enforce strict ordering externally if needed.
- Optimistic iteration: Segment iterators may stop early if a write bumps the version during a scan (by design). Reâ€‘open the iterator to continue. See `EntryIteratorWithLock`.

## ğŸ“ Size and Addressing Limits

- Perâ€‘segment SST size bounded by 32â€‘bit positions: Sparse index stores an `Integer` position and readers cast to `int` (`ChunkEntryFile#openIteratorAtPosition((int)position)`). Keep a single `.index` file below ~2 GiB. Use multiple segments to scale. Code: `chunkentryfile/ChunkEntryFile.java`, `scarceindex/*`.
- Dataâ€‘block and cell sizing constraints: `diskIoBufferSize` must be divisible by 1024; chunk cell size is fixed at 16 bytes. Payloads pad to whole cells (space overhead). Code: `Vldtn#requireIoBufferSize`, `chunkstore/CellPosition.java`.

## ğŸ§± Configuration Immutability

Once an index is created, several properties cannot be changed when reopening with a config:

- Type descriptors (key/value serialization)
- Sparse index cadence (`maxNumberOfKeysInSegmentChunk`)
- Segment sizing limits (e.g., `maxNumberOfKeysInSegment`)
- Bloom filter sizing and hash functions
- Encoding/decoding filter lists (order and membership)

Attempts to change these raise an error in `IndexConfigurationManager.validateThatWasntChanged`. To change them, create a new index and bulkâ€‘copy data (read + write) or export/import. See `segmentindex/IndexConfigurationManager.java`.

## ğŸ§  Data Model and Semantics

- Strictly increasing keys: All onâ€‘disk structures assume ascending order; compaction and consistency checks enforce it. Incorrect comparators or inconsistent key ordering will fail. Code: `segment/SegmentConsistencyChecker.java`.
- Tombstones: Deletes are tombstones until compaction merges and drops them. Heavy delete workloads without compaction may grow delta files and increase read work.
- No multiâ€‘key transactions: Writes are perâ€‘key; there is no crossâ€‘key atomic batch. Use applicationâ€‘level coordination if needed.

## ğŸ” Security Posture

- XOR filter is not encryption: `ChunkFilterXorEncrypt` provides reversible obfuscation only; do not use as security. For encryption at rest, place HestiaStore on an encrypted volume or add a real crypto layer above. See `chunkstore/ChunkFilterXor*`.
- No authentication/authorization: HestiaStore is an embedded library and relies on your process/container isolation.

## âœ… Workloads That Fit Well

- Highâ€‘throughput append/update workloads where readâ€‘afterâ€‘write visibility matters and periodic flush/compaction is acceptable.
- Point lookups and ordered scans with predictable latency (Bloom + sparse index bound I/O).

## ğŸš« Antiâ€‘patterns

- Expecting durability without flush/close.
- Relying on WAL replay (not implemented).
- Very large single segments (>2 GiB `.index`); split into more segments.
- Heavy mixed concurrent reads/writes with strict lowâ€‘latency tail in synchronized mode (coarse locking).

## ğŸ› ï¸ Mitigations and Best Practices

- Plan periodic `flushAndWait()` and `compact()` windows; after crashes run consistency check and optionally compact.
- Size Bloom filters for your negativeâ€‘lookup rate; monitor `BloomFilterStats`.
- Tune `maxNumberOfKeysInSegmentChunk` to balance read scan length vs. sparse index size.
- Use multiple segments to stay under perâ€‘segment limits and to improve compaction parallelism (future).

## ğŸ”— Related Docs

- Recovery: `architecture/recovery.md`
- Concurrency: `architecture/concurrency.md`
- Filters & Integrity: `architecture/filters.md`
- Onâ€‘Disk Layout: `architecture/on-disk-layout.md`
