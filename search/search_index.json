{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>HestiaStore is a lightweight, embeddable key\u2011value storage engine optimized for billions of records, designed to run in a single directory with high performance and minimal configuration.</p> <p>Features:</p> <pre><code> \u2022 Pure Java (no native dependencies), easy to embed\n \u2022 200k+ ops/s; predictable I/O with configurable buffering\n \u2022 In\u2011memory or file\u2011backed storage, zero\u2011config setup\n \u2022 Pluggable filters: Snappy compression, CRC32 integrity, magic-number validation\n \u2022 Bloom filter for fast negative lookups (tunable false-positive rate)\n \u2022 Segmented structure with sparse index for efficient range scans\n \u2022 Custom key/value types via type descriptors\n \u2022 Single\u2011writer, multi\u2011reader (optional synchronized mode)\n \u2022 Test-friendly MemDirectory for fast, isolated tests\n \u2022 Roadmap: write-ahead logging and advanced compaction\n</code></pre>"},{"location":"#performance-comparison","title":"\ud83d\ude80 Performance Comparison","text":"<p>All tests ran on a 2024 Mac mini with 16 GB RAM. Absolute numbers vary between runs, so focus on relative differences.</p>"},{"location":"#benchmark-write-throughput-opss-higher-is-better","title":"Benchmark <code>write</code> throughput (ops/s, higher is better)","text":"<p>The following benchmark compares similar products by writing simple key-value pairs into a map. It includes a 3-minute warm-up to prime caches, followed by a 4-minute measurement period.</p> <p></p> <p>Detailed methodology and full benchmark artifacts are available at benchmark results.</p>"},{"location":"#benchmark-read-throughput-opss-higher-is-better","title":"Benchmark <code>read</code> throughput (ops/s, higher is better)","text":"<p>The read benchmark measures random lookups over the same pre-populated dataset produced by the write test. Each engine is opened on that data and a single client issues random reads of existing keys (no deletes). A 3-minute warm-up primes OS and engine caches, followed by a 4-minute measurement window.</p> <p></p> <p>Detailed methodology and full benchmark artifacts are available at benchmark results.</p>"},{"location":"#benchmark-sequential-read-throughput-opss-higher-is-better","title":"Benchmark <code>sequential read</code> throughput (ops/s, higher is better)","text":"<p>The sequential read benchmark scans the same pre\u2011populated dataset in key order using each engine\u2019s iterator. Each engine is opened on that data and a single client performs a forward scan across all entries. A 3\u2011minute warm\u2011up primes OS and engine caches, followed by a 4\u2011minute measurement window.</p> <p></p> <p>Detailed methodology and full benchmark artifacts are available at benchmark results.</p>"},{"location":"#feature-comparison","title":"\ud83d\udcca Feature Comparison","text":"<p>Architecture &amp; Concurrency</p> Engine Storage/Index Concurrency Background Work HestiaStore Segmented on-disk structure Single-writer, multi-reader (optional synchronized) Periodic segment flush/merge RocksDB LSM tree (leveled/uni) Highly concurrent Compaction + flush threads LevelDB LSM tree Single-writer, multi-reader Compaction MapDB B-tree/H-tree Thread-safe (synchronized) Periodic commits ChronicleMap Off-heap mmap hash map Lock-free/low-lock None (no compaction) H2 B-tree Concurrent (MVCC) Checkpoint/auto-vacuum <p>Durability &amp; Fit</p> Engine Durability Compression Runtime Deps Typical Fit HestiaStore File-backed; commit on close Snappy Pure Java (JAR-only) Embedded KV with simple ops, large datasets RocksDB WAL + checkpoints (optional transactions) Snappy/Zstd/LZ4 Native library High write throughput, low-latency reads LevelDB File-backed; no transactions Snappy JAR-only port/native bindings Lightweight LSM, smaller footprints MapDB File-backed; optional TX None/limited Pure Java (JAR-only) Simple embedded maps/sets ChronicleMap Memory-mapped persistence; no ACID TX None Pure Java (JAR-only) Ultra-low latency shared maps H2 WAL + MVCC transactions Optional Pure Java (JAR-only) SQL + transactional workloads <p>Notes</p> <ul> <li>\u201cConcurrency\u201d describes the general access model; specifics depend on configuration and workload.</li> <li>HestiaStore focuses on predictable file I/O with configurable buffering; WAL/transactions are on the roadmap.</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Please read our Contributing Guidelines before submitting a pull request.</p>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>Architecture overview</li> <li>Segment architecture</li> <li>Getting started with a quick start and examples</li> <li>Configuration \u2014 properties overview and guidance</li> <li>Logging \u2014 how to set up logging</li> <li>Releases \u2014 versioning and release process</li> </ul>"},{"location":"#installation-and-basic-usage","title":"\ud83d\udce6 Installation and Basic Usage","text":"<p>To include HestiaStore in your Maven project, add the following dependency to your <code>pom.xml</code>:</p> <pre><code>&lt;dependencies&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;org.hestiastore.index&lt;/groupId&gt;\n    &lt;artifactId&gt;core&lt;/artifactId&gt;\n    &lt;version&gt;&lt;!--latest verson--&gt;&lt;/version&gt;\n  &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> <p>Replace the version number with the latest available from Maven Central org.hestiastore.index:core.</p> <p>Note: HestiaStore requires Java 17 or newer.</p> <p>You can create a new index using the builder pattern as shown below:</p> <pre><code>// Create an in-memory file system abstraction\nDirectory directory = new MemDirectory();\n\n// Prepare index configuration\nIndexConfiguration&lt;String, String&gt; conf = IndexConfiguration\n        .&lt;String, String&gt;builder()//\n        .withKeyClass(String.class)//\n        .withValueClass(String.class)//\n        .withName(\"test_index\") //\n        .build();\n\n// Create a new index\nSegmentIndex&lt;String, String&gt; index = SegmentIndex.&lt;String, String&gt;create(directory, conf);\n\n// Perform basic operations\nindex.put(\"Hello\", \"World\");\n\nString value = index.get(\"Hello\");\nSystem.out.println(\"Value for 'Hello': \" + value);\n\nindex.close();\n</code></pre> <p>For more integration details, see the Getting Started section.</p>"},{"location":"#roadmap","title":"\ud83d\uddfa\ufe0f Roadmap","text":"<p>Planned improvements include:</p> <ul> <li>Full Multithreaded Storage Engine \u2013 Currently this is the biggest performance limitation. Disk I/O consumes only about 40% of available CPU, leaving significant unused capacity.</li> <li>Implement Asynchronous I/O \u2013 Explore alternative approaches for file system access. Replace the current java.io-based implementation.</li> <li>Example Application \u2013 Provide a simple, easy-to-run demo application that demonstrates HestiaStore\u2019s capabilities. For detailed tasks and progress, see the GitHub Issues page.</li> </ul>"},{"location":"#need-help-or-have-questions","title":"\u2753 Need Help or Have Questions?","text":"<p>If you encounter a bug, have a feature request, or need help using HestiaStore, please create an issue.</p>"},{"location":"CHANGELOG/","title":"\ud83d\uddd3\ufe0f Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog.</p>"},{"location":"CHANGELOG/#005","title":"\ud83c\udff7\ufe0f 0.0.5","text":""},{"location":"CHANGELOG/#added","title":"\u2728 Added","text":"<ul> <li>Data blocks were introduced and the on-disk storage format was significantly improved.</li> <li>All disk writes now use a temporary file followed by an atomic rename; all streams are correctly closed.</li> <li>Data storage is configurable via the application configuration.</li> <li>Data in chunks and data blocks are validated using a magic number and CRC32.</li> <li>Added support for Snappy compression.</li> </ul>"},{"location":"CHANGELOG/#004","title":"\ud83c\udff7\ufe0f 0.0.4","text":""},{"location":"CHANGELOG/#added_1","title":"\u2728 Added","text":"<ul> <li>Add recovery support to rebuild indexes after failures. (#22)</li> <li>Introduce pages for segment-based indexing. (#31)</li> <li>Create <code>Directory</code> implementation using <code>java.nio</code>. (#50)</li> <li>Add a performance comparison framework for benchmark testing. (#60)</li> <li>Add integration tests for deletion and graceful degradation. (#76, #63)</li> <li>Add a test class for long-running index operations. (#49)</li> </ul>"},{"location":"CHANGELOG/#changed","title":"\ud83d\udd27 Changed","text":"<ul> <li>Improve design of the <code>sorteddatafile</code> package for better modularity. (#59)</li> <li>Enhance index configuration validation and parameter consistency. (#81)</li> <li>Introduce limits on the number of delta files to prevent unbounded growth. (#75)</li> </ul>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"\ud83e\udd1d Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"\ud83c\udf1f Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others\u2019 private information, such as a physical or email address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"\ud83d\udccb Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"\ud83d\udee1\ufe0f Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team via project issue. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"\ud83d\udcce Attribution","text":"<p>This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by the Citizen Code of Conduct.</p> <p>For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq.</p> <p>I\u2019ll personally make every effort to answer your questions and explain anything that\u2019s unclear. We are committed to discussing any feedback or questions you may have and will explain everything in detail to ensure transparency and understanding.</p>"},{"location":"CONTRIBUTING/","title":"Contributing to HestiaStore","text":"<p>Thank you for your interest in contributing to HestiaStore! We're excited to welcome your ideas, improvements, and bug fixes. Please follow the guidelines below to ensure a smooth and productive collaboration.</p>"},{"location":"CONTRIBUTING/#before-you-start","title":"\ud83e\udded Before You Start","text":"<p>Please make sure there is an existing issue or create a new one that describes your intended change or feature. This helps us track and discuss proposals before any code is written.</p>"},{"location":"CONTRIBUTING/#code-style","title":"\ud83e\uddd1\u200d\ud83d\udcbb Code Style","text":"<p>We follow a consistent Java coding style defined by the Eclipse formatter settings in <code>./eclipse-formatter.xml</code>. Please configure your IDE to use this formatter to keep the codebase consistent.</p>"},{"location":"CONTRIBUTING/#code-quality-checks","title":"\ud83e\uddea Code Quality Checks","text":"<p>Before submitting your code, please verify the following:</p> <ul> <li>\u2705 Run Site Checks   Execute <code>mvn clean site</code> to generate the project site and perform static analysis. This will highlight issues reported by:</li> <li>PMD</li> <li>Checkstyle</li> <li> <p>SpotBugs (formerly FindBugs)   Please ensure your changes do not introduce new warnings or violations.</p> </li> <li> <p>\u2705 Test Coverage   All new code should be covered by unit tests. We use JUnit. Run tests and verify that your code is being exercised by checking the line coverage in the site reports.</p> </li> <li> <p>\u2705 Javadoc Comments   Public methods, classes, and significant internal logic should be documented using Javadoc. Clear documentation helps others understand and maintain the project.</p> </li> </ul>"},{"location":"CONTRIBUTING/#commit-and-submit","title":"\ud83d\udee0 Commit and Submit","text":"<ol> <li>Make your changes in a separate branch.</li> <li>Push your branch to your fork or the main repo (if you have access).</li> <li>Open a Pull Request with a clear title and description.</li> <li>Link to the related issue or ticket.</li> </ol>"},{"location":"CONTRIBUTING/#review-process","title":"\u23f3 Review Process","text":"<p>Once submitted, your PR will be reviewed by a maintainer. We may request changes or ask clarifying questions. Please be patient \u2014 reviews are important to keep the project healthy.</p>"},{"location":"CONTRIBUTING/#thanks","title":"\ud83d\ude4c Thanks","text":"<p>We appreciate your contribution, whether it's code, documentation, or ideas. Your support makes HestiaStore better!</p> <p>\ud83d\udcac For questions, feel free to open a GitHub Issue.</p>"},{"location":"SECURITY/","title":"HestiaStore Security","text":"<p>Security and quality are important considerations in the HestiaStore project. While HestiaStore is a library (not a network-exposed service), several tools are in place to monitor and improve code and dependency safety.</p>"},{"location":"SECURITY/#dependency-scanning","title":"\ud83e\uddea Dependency Scanning","text":"<p>HestiaStore uses the OWASP Dependency-Check Maven plugin to automatically scan project dependencies for known vulnerabilities. The scan is performed during the Maven <code>verify</code> phase. This helps detect issues in third-party libraries such as outdated or vulnerable versions of common libraries.</p> <p>The OWASP dependency report is also included in the Maven Site documentation.</p>"},{"location":"SECURITY/#data-storage-security","title":"\ud83d\udcbe Data Storage Security","text":"<p>Currently, HestiaStore does not support a persistent, remote or encrypted storage backend. All data is stored in the local file system or memory, depending on the <code>Directory</code> implementation (e.g. <code>FsDirectory</code> or <code>MemDirectory</code>). Support for more advanced persistent stores with security features like encryption may be added in the future.</p>"},{"location":"SECURITY/#static-code-analysis","title":"\ud83d\udd75\ufe0f Static Code Analysis","text":"<p>HestiaStore uses the following tools to enforce code quality and detect potential bugs:</p> <ul> <li>PMD: Checks for common coding errors, best practices violations, and potential bugs.</li> <li>SpotBugs (formerly FindBugs): Performs bytecode-level bug detection for possible concurrency issues, null pointer dereferences, etc.</li> </ul> <p>Both reports are available through the Maven Site (<code>mvn site</code>).</p>"},{"location":"SECURITY/#testing-and-coverage","title":"\u2705 Testing and Coverage","text":"<p>The project includes a comprehensive suite of unit tests. Test coverage is measured using JaCoCo, and the coverage report is also published as part of the Maven Site.</p> <pre><code>mvn clean verify site\n</code></pre> <p>This will generate the full set of reports under <code>target/site/</code>.</p>"},{"location":"SECURITY/#threat-model","title":"\ud83d\udd10 Threat Model","text":"<p>HestiaStore is designed to run as a component within a trusted local application. It does not expose network interfaces or provide internal access control mechanisms. As such, it assumes that:</p> <ul> <li>The host operating environment is trusted.</li> <li>Filesystem access is managed by the application or OS.</li> <li>Inputs to the library are trusted or validated upstream.</li> </ul>"},{"location":"SECURITY/#known-risks","title":"\u26a0\ufe0f Known Risks","text":"Threat Mitigated? Notes Malicious input data \u274c No input sanitization is performed Unauthorized file access \u274c No access control; relies on OS permissions File corruption \ud83d\udeab No WAL; durability depends on flush/close and atomic renames Memory data leakage \u274c JVM memory is not encrypted or zeroed SegmentIndex inconsistency \u26a0\ufe0f Recovery possible using <code>checkAndRepairConsistency()</code>"},{"location":"SECURITY/#trust-boundaries","title":"\ud83d\udee1\ufe0f Trust Boundaries","text":"<p>HestiaStore does not define security boundaries within its API. Instead, it assumes that:</p> <ul> <li>The file system used by <code>FsDirectory</code> is controlled by the same principal as the application.</li> <li>Memory content is considered volatile and not protected against memory inspection.</li> <li>The user is responsible for isolating the library appropriately in containerized or multi-tenant environments.</li> </ul>"},{"location":"SECURITY/#data-integrity","title":"\ud83d\udd0d Data Integrity","text":"<p>HestiaStore provides limited protections:</p> <ul> <li>No Write-Ahead Logging (WAL); durability is tied to explicit flush/close.</li> <li>Manual compaction and <code>checkAndRepairConsistency()</code> assist in recovery from logical inconsistencies.</li> <li>No built-in checksums or MACs are currently used.</li> </ul>"},{"location":"SECURITY/#encryption","title":"\ud83d\udd12 Encryption","text":"<p>HestiaStore does not implement:</p> <ul> <li>Encryption at rest</li> <li>Encryption in memory</li> <li>Encrypted segment files</li> </ul> <p>Users requiring data confidentiality should enable full-disk encryption or isolate the storage backend appropriately.</p>"},{"location":"SECURITY/#denial-of-service-considerations","title":"\ud83c\udfd7\ufe0f Denial of Service Considerations","text":"<p>While HestiaStore is efficient, certain usage patterns may degrade system performance:</p> <ul> <li>Inserting excessive data without flushing may exhaust memory.</li> <li>Large segment files may incur slow read or compaction times.</li> <li>Thread-safe operations may incur additional locking overhead under heavy concurrency.</li> </ul>"},{"location":"SECURITY/#security-responsibilities-of-integrators","title":"\ud83d\udc77 Security Responsibilities of Integrators","text":"<p>Users embedding HestiaStore must take responsibility for:</p> <ul> <li>Validating inputs</li> <li>Managing access to the directory path</li> <li>Applying memory and disk usage quotas externally</li> <li>Protecting against unauthorized runtime access</li> </ul>"},{"location":"SECURITY/#future-work","title":"\ud83d\udd27 Future Work","text":"<p>Planned or considered improvements include:</p> <ul> <li>Optional encryption of segment data</li> <li>Checksumming of stored values</li> <li>Sandboxed key/value type descriptors</li> </ul>"},{"location":"SECURITY/#summary","title":"\ud83d\udccb Summary","text":"<ul> <li>\u2705 Vulnerability scanning via OWASP Dependency Check</li> <li>\u2705 Static analysis via PMD and SpotBugs</li> <li>\u2705 Unit tests with coverage reporting via JaCoCo</li> <li>\u23f3 Persistent encrypted storage is not yet supported</li> <li>\u2705 Basic threat model documented</li> <li>\u26a0\ufe0f Assumes trusted host environment (no access control or encryption)</li> <li>\ud83d\udea7 Future improvements under consideration (checksums, encryption)</li> </ul> <p>If you encounter any problems, discover vulnerabilities, or have questions, please report them by opening an issue in the project's GitHub repository.</p>"},{"location":"refactor-backlog/","title":"Refactor backlog","text":""},{"location":"refactor-backlog/#active","title":"Active","text":"<p>[ ] 59.1 Concurrency: remove lock-order inversion in core ops (Risk: HIGH)     - <code>SegmentIndexCore.get/put</code>: avoid holding key-map read lock while calling       <code>SegmentRegistry.getSegment</code> or touching segments.     - Use key-map snapshot + version re-check on retry/BUSY paths.     - Tests: <code>IntegrationSegmentIndexConcurrencyTest</code> + new split/put stress.</p>"},{"location":"refactor-backlog/#planned","title":"Planned","text":""},{"location":"refactor-backlog/#high","title":"High","text":"<p>[ ] 78 Monitoring/Management platform rollout (Risk: HIGH)     - Goal: evolve from in-process counters to multi-JVM monitoring and control       without forcing Micrometer/Prometheus dependencies into core.     - Delivery model: phase-gated rollout where each phase is releasable and       backward compatible.     - Constraints:       - Core package must not depend on Micrometer, Prometheus, servlet stacks,         or UI classes.       - Runtime control endpoints must be explicit allowlist operations only         (no generic \"execute command\" style endpoint).       - All mutating management operations must be auditable.</p> <p>[ ] 78.1 Define source/module boundaries and package contracts (Risk: HIGH)     - Target logical modules/packages:       - <code>org.hestiastore.index.*</code> (core)       - <code>org.hestiastore.monitoring.*</code> (metrics model + exporter adapters)       - <code>org.hestiastore.management.api.*</code> (shared DTOs/contracts)       - <code>org.hestiastore.management.agent.*</code> (node-local REST API in index JVM)       - <code>org.hestiastore.console.*</code> (web UI / control plane)     - Start in single-module codebase with strict package boundaries to keep       later physical split low risk.     - Add architecture doc with allowed dependency direction:       <code>core &lt;- monitoring &lt;- management.agent &lt;- console</code> and       <code>management.api</code> shared by agent/console.     - Acceptance:       - No core imports from monitoring/agent/console packages.       - Checkstyle/ArchUnit (or similar) rule blocks forbidden imports.</p> <p>[ ] 78.2 Add stable core metrics snapshot API (Risk: HIGH)     - Introduce immutable public snapshot types in core for index/segment       metrics (e.g. op counters, bloom stats, segment counts, state).     - Add <code>SegmentIndex.metricsSnapshot()</code> (or equivalent read-only API).     - Keep existing behavior intact while wiring current counters into snapshot.     - Make counters thread-safe (<code>LongAdder</code>/<code>AtomicLong</code>) where currently not.     - Define compatibility policy:       - new fields may be added,       - existing field names/semantics cannot silently change.     - Acceptance:       - Unit/integration tests for snapshot consistency under concurrent load.       - Docs page with metric field definitions and semantics.</p> <p>[ ] 78.3 Build monitoring bridge layer (Micrometer/Prometheus/JMX) (Risk: HIGH)     - Implement monitoring adapters in <code>org.hestiastore.monitoring.*</code>:       - Micrometer binder reading from core snapshot API.       - Prometheus exposition support (via Micrometer registry or direct bridge).       - Optional JMX MBean exporter mapped from the same snapshot model.     - Ensure adapters can be created/removed without restarting index       (where runtime allows).     - Define metric naming/tag conventions (<code>hestiastore_*</code>, stable tag set).     - Acceptance:       - Prometheus scrape returns expected metrics and labels.       - Zero adapter overhead when monitoring package is not used.</p> <p>[ ] 78.4 Add management API contracts and versioning (Risk: HIGH)     - Create <code>org.hestiastore.management.api.*</code> DTOs:       - <code>NodeStateResponse</code>, <code>MetricsResponse</code>, <code>ActionRequest/Response</code>,         <code>ConfigPatchRequest</code>, <code>ErrorResponse</code>.     - Version endpoints from start (<code>/api/v1/...</code>) and define deprecation rules.     - Include idempotency and safety semantics for actions:       - <code>flush</code>, <code>compact</code>, selected config patch operations.     - Acceptance:       - OpenAPI (or equivalent) published with examples.       - Contract tests verify backward-compatible serialization.</p> <p>[ ] 78.5 Implement node-local management agent (Risk: HIGH)     - Add lightweight REST server integration for index JVM process:       - <code>GET /api/v1/state</code>       - <code>GET /api/v1/metrics</code>       - <code>POST /api/v1/actions/flush</code>       - <code>POST /api/v1/actions/compact</code>       - <code>PATCH /api/v1/config</code> (allowlist runtime-safe keys only)     - Include health and readiness endpoints for deployment integration.     - Add per-request audit logging for mutating endpoints.     - Acceptance:       - End-to-end test: invoke actions and verify effect on index state.       - Negative tests for forbidden config keys and invalid state transitions.</p> <p>[ ] 78.6 Implement central console web application (Risk: HIGH)     - Build <code>org.hestiastore.console.*</code> with capabilities:       - register/manage multiple index JVM nodes,       - poll agent APIs and display key read/write/latency/segment metrics,       - trigger safe operations (flush/compact) with confirmation UX,       - show recent audit/event log entries.     - Keep UI read-first: write controls separated and permission-gated.     - Define minimal dashboard first; defer advanced analytics to later items.     - Acceptance:       - Multi-node dashboard works for at least 3 registered nodes.       - Action execution shows pending/success/failure lifecycle.</p> <p>[ ] 78.7 Secure transport, authz, and audit trail (Risk: HIGH)     - Agent &lt;-&gt; console transport:       - enforce TLS (prefer mTLS in production profiles),       - token- or cert-based authn,       - role-based authz (<code>read</code>, <code>operate</code>, <code>admin</code>).     - Add immutable audit records for mutating calls:       actor, target node, endpoint, payload digest, result, timestamp.     - Add rate limits and retry/backoff policy for control operations.     - Acceptance:       - Security integration tests for unauthorized/forbidden scenarios.       - Audit log verification tests for all mutating endpoints.</p> <p>[ ] 78.8 Packaging, release strategy, and migration path (Risk: HIGH)     - Release artifacts initially from same repo:       - <code>hestiastore</code> (core)       - <code>hestiastore-monitoring</code> (bridges/exporters)       - <code>hestiastore-management-agent</code>       - <code>hestiastore-console</code>     - Keep aligned versions per release line (for example <code>0.2.x</code> for all).     - Document migration from single-module to multi-module build:       move packages with no API break using prior boundary rules from 78.1.     - Acceptance:       - Build produces separate jars and integration tests across artifacts pass.       - Release docs include compatibility matrix and upgrade notes.</p> <p>[ ] 78.9 Rollout stages with explicit quality gates (Risk: HIGH)     - Stage A: core snapshot API only; no external exporters.     - Stage B: monitoring bridge with Prometheus scrape + docs.     - Stage C: node agent endpoints (read-only first, then mutating).     - Stage D: console UI for multi-node visibility, then controlled actions.     - Required gates per stage:       - load/perf regression budget defined and met,       - concurrency tests for stats correctness,       - failure-mode tests (node down, timeout, partial responses),       - operational docs/runbook updated.     - Acceptance:       - Each stage releasable independently.       - Rollback procedure documented and tested.</p>"},{"location":"refactor-backlog/#medium","title":"Medium","text":"<p>[ ] 54 Dedicated executor for index async ops (Risk: MEDIUM)     - Use a dedicated, bounded executor for <code>SegmentIndexImpl.runAsyncTracked</code>       (no common pool).     - Define rejection policy: map saturation to BUSY/error with clear message.     - Ensure close waits for in\u2011flight async work or cancels safely.     - Tests: saturation/backpressure, close ordering, no caller\u2011thread IO.</p> <p>[ ] 55 Replace busy spin loops with retry + jitter (Risk: MEDIUM)     - Replace <code>Thread.onSpinWait</code>/busy loops in split iterator open and other       retry paths with <code>IndexRetryPolicy</code> + jitter.     - Make timeouts explicit and surface <code>IndexException</code> with operation name.     - Tests: BUSY retry exits on READY, timeout path, interrupt handling.</p> <p>[ ] 56 Key\u2011to\u2011segment map read contention reduction (Risk: MEDIUM)     - Evaluate snapshot\u2011based reads or <code>StampedLock</code> for high\u2011read workloads.     - Keep version validation semantics intact for split/extend paths.     - Tests: concurrent get/put under splits, no missing mappings, no deadlocks.</p> <p>[ ] 57 Streaming iterators without full materialization (Risk: MEDIUM)     - Replace list materialization in <code>getStream</code>/FULL_ISOLATION with streaming       merge iterators over write/delta caches and segment files.     - Ensure iterator close releases resources and does not leak locks.     - Tests: large data set memory profile, iterator isolation correctness.</p> <p>[ ] 5 Stop materializing merged cache lists on read (Risk: MEDIUM)     - Problem: <code>SegmentReadPath.openIterator</code> calls <code>getAsSortedList</code>, building       full merged lists for each iterator.     - Fix: provide streaming merge iterator over delta/write caches without       full list materialization.     - Options:       - Option A (recommended): switch <code>UniqueCache</code> to <code>TreeMap</code> /         <code>ConcurrentSkipListMap</code>, add a sorted iterator API, and merge cache         iterators (write/frozen/delta) with <code>MergedEntryIterator</code> in the         FULL_ISOLATION path.       - Option B: keep <code>HashMap</code> / <code>ConcurrentHashMap</code> for get/put and maintain         a sorted key index (<code>TreeSet</code> / <code>ConcurrentSkipListSet</code>) for iteration;         expose a sorted iterator over keys + map values and merge like Option A. [ ] 6 Stream compaction without full cache snapshot (Risk: MEDIUM)     - Problem: compaction snapshots the full cache list in memory.     - Fix: stream from iterators or chunk snapshot to bounded buffers. [ ] 7 Stream split without full cache snapshot (Risk: MEDIUM)     - Problem: split uses FULL_ISOLATION iterator backed by full list snapshot.     - Fix: use streaming iterator or chunked splitting to cap memory. [ ] 8 Avoid full materialization in <code>IndexInternalConcurrent.getStream</code> (Risk: MEDIUM)     - Problem: method loads all entries into a list before returning a stream.     - Fix: return a streaming spliterator tied to iterator close. [ ] 9 Add eviction for heavy segment resources (Risk: MEDIUM)     - Problem: <code>SegmentResourcesImpl</code> caches bloom/scarce forever.     - Fix: tie resource lifetime to segment eviction or add per-resource LRU;       ensure invalidate/close releases memory.</p>"},{"location":"refactor-backlog/#low","title":"Low","text":"<p>[ ] 10 Allow cache shrink after peaks (Risk: LOW)     - Problem: <code>UniqueCache.clear()</code> keeps underlying <code>HashMap</code> capacity.     - Fix: rebuild map on clear when size exceeds a threshold; add tests.</p>"},{"location":"refactor-backlog/#other-refactors-non-oom","title":"Other refactors (non-OOM)","text":"<p>[ ] 13 Implement a real registry lock (Risk: MEDIUM)     - Add an explicit lock around registry mutations + file ops.     - Replace/rename <code>executeWithRegistryLock</code> to actually serialize callers.     - Add tests for split/compact interleaving and segment visibility. [ ] 14 Replace common-pool async with dedicated executor + backpressure (Risk: MEDIUM)     - Add/configure a dedicated executor for async API calls.     - Track in-flight tasks and wait on close; add queue/backpressure limits.     - Add tests for saturation, cancellation, and close ordering. [ ] 15 Define <code>IndexAsyncAdapter.close()</code> behavior (Risk: MEDIUM)     - Decide on wait vs non-blocking close and document it.     - Add tests that match the chosen contract. [ ] 16 Replace busy-spin loops with retry+backoff+timeout (Risk: MEDIUM)     - Use <code>IndexRetryPolicy</code> in <code>SegmentsIterator</code> and split iterator open.     - Add interrupt handling and timeout paths with clear error messaging.     - Add tests for BUSY loops and timeout behavior. [ ] 17 Stop returning <code>null</code> on CLOSED in <code>SegmentIndexImpl.get</code> (Risk: MEDIUM)     - Decide API surface (exception vs status/Optional).     - Update callers and docs to distinguish \"missing\" vs \"closed\".     - Add tests for CLOSED/ERROR paths. [ ] 19 Propagate MDC context to async ops and stream consumption (Risk: LOW)     - Capture MDC context on submit and reapply in async tasks.     - Wrap stream/iterator consumption with MDC scope; clear on close.     - Add tests asserting <code>index.name</code> appears in async logs. [ ] 41 Unify async execution for segment index (Risk: MEDIUM)     - Route <code>SegmentIndexImpl.runAsyncTracked</code> and <code>IndexAsyncAdapter.runAsyncTracked</code>       through a shared, dedicated executor (no common pool).     - Decide whether to keep both async layers or make one delegate to the other.     - Align async close behavior and document rejection/backpressure outcomes. [ ] 42 Revisit <code>SegmentAsyncExecutor</code> rejection policy (Risk: MEDIUM)     - Ensure maintenance IO never runs on caller threads.     - Choose <code>AbortPolicy</code> + BUSY/error mapping or custom handler.     - Update docs and metrics if behavior changes. [ ] 43 Replace registry close polling with completion signal (Risk: MEDIUM)     - Add a close completion handle or signal in <code>Segment</code>.     - Update <code>SegmentRegistry.closeSegmentIfNeeded</code> to wait on completion rather       than polling <code>getState()</code>.     - Ensure close-from-maintenance thread does not deadlock. [ ] 44 Normalize split close/eviction flow (Risk: MEDIUM)     - Centralize segment close/eviction in <code>SegmentRegistry</code>.     - Remove direct <code>segment.close()</code> calls from split coordinator.     - Ensure split outcome updates mapping, eviction, and close are ordered. [ ] 45 Replace spin-wait in <code>SegmentConcurrencyGate.awaitNoInFlight</code> (Risk: LOW)     - Use <code>wait/notify</code> or <code>ManagedBlocker</code> with timeout.     - Preserve FREEZE semantics and early exit on state change.     - Add tests for drain behavior under load. [ ] 46 Align iterator isolation naming and semantics (Risk: LOW)     - Choose between <code>FAIL_FAST</code>/<code>FULL_ISOLATION</code> and the legacy       <code>INTERRUPT_FAST</code>/<code>STOP_FAST</code> terminology.     - Update docs, comments, and any mapping code consistently. [ ] 47 Consolidate BUSY/CLOSED retry loops (Risk: LOW)     - Extract shared retry helper for segmentindex operations.     - Replace ad-hoc loops in <code>SegmentRegistry</code>, <code>SegmentSplitCoordinator</code>,       and <code>SegmentIndexImpl</code>.     - Keep backoff/timeout semantics and error messages consistent.</p>"},{"location":"refactor-backlog/#testingquality","title":"Testing/Quality","text":"<p>[ ] 48 Test executor saturation and backpressure paths (Risk: MEDIUM)     - Add tests for <code>SegmentAsyncExecutor</code> queue saturation and rejection handling.     - Add tests for <code>SplitAsyncExecutor</code> rejection and in-flight cleanup.     - Verify maintenance IO never runs on caller threads. [ ] 49 Test close path interactions (Risk: MEDIUM)     - Close while segment is <code>MAINTENANCE_RUNNING</code> and ensure backoff/timeout works.     - Close during async operations should fail fast with clear error.     - Assert no deadlock when waiting for segment READY/CLOSED. [ ] 50 Test split failure cleanup (Risk: MEDIUM)     - Force exceptions in split steps and assert <code>splitsInFlight</code> clears.     - Validate directory swap and key-to-segment map remain consistent.     - Ensure resources/locks are released on failure. [ ] 51 Test maintenance failure transitions (Risk: MEDIUM)     - Inject failures in maintenance IO and publish phases.     - Assert segment moves to <code>ERROR</code> and callers see ERROR status.     - Verify rejection handling does not leave the segment in FREEZE.</p>"},{"location":"refactor-backlog/#ready","title":"Ready","text":"<ul> <li>(move items here when they are scoped and ready to execute)</li> </ul>"},{"location":"refactor-backlog/#deferred-segment-scope-do-not-touch-now","title":"Deferred (segment scope, do not touch now)","text":""},{"location":"refactor-backlog/#maintenance-tasks","title":"Maintenance tasks","text":"<p>[ ] M37 Audit <code>segment</code> package for unused or test-only code (Risk: LOW)     - Limit class, method and variables visiblity     - Identify unused classes/methods/fields.     - Remove code only referenced by tests or move test helpers into test scope.     - Ensure public API docs and tests remain consistent after cleanup. [ ] M38 Review <code>segment</code> package for test and Javadoc coverage (Risk: LOW)     - Ensure each class has a JUnit test or document why coverage is excluded.     - Ensure each public class/method has Javadoc; add missing docs. [ ] M39 Audit <code>segmentindex</code> package for unused or test-only code (Risk: LOW)     - Limit class, method and variables visiblity     - Identify unused classes/methods/fields.     - Remove code only referenced by tests or move test helpers into test scope.     - Ensure public API docs and tests remain consistent after cleanup. [ ] M40 Review <code>segmentindex</code> package for test and Javadoc coverage (Risk: LOW)     - Ensure each class has a JUnit test or document why coverage is excluded.     - Ensure each public class/method has Javadoc; add missing docs. [ ] M41 Audit <code>segmentregistry</code> package for unused or test-only code (Risk: LOW)     - Limit class, method and variables visiblity     - Identify unused classes/methods/fields.     - Remove code only referenced by tests or move test helpers into test scope.     - Ensure public API docs and tests remain consistent after cleanup. [ ] M42 Review <code>segmentregistry</code> package for test and Javadoc coverage (Risk: LOW)     - Ensure each class has a JUnit test or document why coverage is excluded.     - Ensure each public class/method has Javadoc; add missing docs.     - See <code>docs/development/segmentregistry-audit.md</code> for audit notes.</p>"},{"location":"refactor-backlog/#done-archive","title":"Done (Archive)","text":"<ul> <li>(keep completed items here; do not delete)</li> </ul> <p>[x] 61.1 Wire <code>SegmentHandler</code> into key-to-segment map usage (Risk: HIGH)     - Replace direct segment references in key-to-segment map paths with       <code>SegmentHandler</code> usage.     - Ensure handlers are used consistently for segment access in index flows.</p> <p>[x] 61.2 Refactor split algorithm around handler locks (Risk: HIGH)     - When a segment is eligible for split: acquire handler lock, re-check       eligibility under lock, then either unlock or proceed with split.     - Split apply ordering: update map on disk first, then in-memory map,       then close old segment, delete files, and finally unlock.     - Ensure failures unlock the handler and clean up temporary segments.     - Update <code>docs/architecture/registry/registry.md</code> to reflect handler-based locking.</p> <p>[x] 61.3 Simplify <code>SegmentHandler</code> lock API (Risk: MEDIUM)     - Keep internal handler state as <code>READY</code>/<code>LOCKED</code>.     - <code>lock()</code> returns <code>SegmentHandlerLockStatus</code> with <code>OK</code> or <code>BUSY</code>.     - Replace token-based lock/unlock usage across registry + split flows.     - Update handler-related tests to match the new API.</p> <p>[x] 60 Move registry implementation to <code>segmentregistry</code> package (Risk: MEDIUM)     - Move <code>SegmentRegistryImpl</code>, <code>SegmentRegystryState</code>, <code>SegmentRegistryCache</code>,       <code>SegmentRegistryState</code>, and <code>SegmentRegistryResult</code>       to <code>org.hestiastore.index.segmentregistry</code>.     - Update imports/usages in <code>segmentindex</code> and tests.     - Keep public API surface the same; verify no package-private access leaks.</p> <p>[x] M41 Audit <code>segmentregistry</code> package for unused or test-only code (Risk: LOW)     - Limit class, method and variables visiblity     - Identify unused classes/methods/fields.     - Remove code only referenced by tests or move test helpers into test scope.     - Ensure public API docs and tests remain consistent after cleanup. [x] M42 Review <code>segmentregistry</code> package for test and Javadoc coverage (Risk: LOW)     - Ensure each class has a JUnit test or document why coverage is excluded.     - Ensure each public class/method has Javadoc; add missing docs.</p> <p>[x] 59 Introduce <code>SegmentHandler</code> lock gate in segmentindex (Risk: HIGH)     - Add <code>SegmentHandler</code> with <code>getSegment()</code> returning <code>SegmentHandlerResult</code>:       <code>OK</code> (segment), <code>LOCKED</code>, and handler states <code>READY</code>/<code>LOCKED</code>.     - <code>lock()</code> returns a privileged handle/token that allows access to the       underlying segment while handler state is <code>LOCKED</code>.     - <code>getSegment()</code> must return <code>LOCKED</code> while locked for all non-privileged       callers (no segment exposure during lock).     - Wire split flow to lock via handler before opening <code>FULL_ISOLATION</code>       iterator, then unlock after apply/cleanup.     - Add tests: <code>LOCKED</code> is returned during lock; lock holder can operate;       unlock restores <code>OK</code>.</p> <p>[x] 59.2 Concurrency: reduce redundant key-map read locks (Risk: MEDIUM)     - Make <code>KeyToSegmentMapSynchronizedAdapter.snapshot()</code> lock-free       (volatile snapshot + AtomicLong version).     - Keep read locks only for map-only operations; do not wrap segment calls.     - Tests: snapshot consistency + existing <code>KeyToSegmentMapTest</code>.</p> <p>[x] 59.3 Concurrency: limit registry FREEZE to split apply (Risk: MEDIUM)     - Remove <code>FreezeGuard</code> usage from <code>SegmentRegistryImpl.getSegment</code> create/       eviction path; keep cache lock for LRU safety.     - Reserve registry <code>FREEZE</code> for split apply only.     - Tests: split + eviction concurrency (<code>SegmentRegistryCacheTest</code>,       <code>SegmentSplitCoordinatorConcurrencyTest</code>, integration stress).</p> <p>[x] 52 Remove automatic compaction from <code>segmentindex</code> (Risk: MEDIUM)     - Drop pre-split compaction in <code>SegmentSplitCoordinator</code> and remove       <code>SegmentSplitterPolicy.shouldBeCompactedBeforeSplitting</code> + related retry       logic.     - Simplify split planning to use estimated key counts directly (remove       compaction/tombstone hints from <code>SegmentSplitterPolicy</code> or replace with a       minimal estimate helper).     - Keep <code>SegmentIndex.compact</code> / <code>compactAndWait</code> as the only       segmentindex-triggered compaction entry point; update Javadocs to reflect       compaction being handled inside the segment package otherwise.     - Update tests that construct <code>SegmentSplitterPolicy</code> and add coverage that       split does not call <code>Segment.compact</code> while user-invoked compaction still       does.</p> <p>[x] 1 everiwhere rename maxNumberOfKeysInSegmentWriteCacheDuringFlush to maxNumberOfKeysInSegmentWriteCacheDuringMaintenance including all configurations setter getter all all posssible usages. [x] 2 Wnen write cache reach size as maxNumberOfKeysInSegmentWriteCacheDuringMaintenance than response to put with BUSY. [x] 3 UniqueCache should not use read/write reentrant lock. It's property of concurrent hash map. [x] 4 Enforce <code>maxNumberOfSegmentsInCache</code> in <code>SegmentRegistry</code> (Risk: MEDIUM)     - Problem: segments are cached unbounded; memory grows as segments grow.     - Fix: implement LRU or size-bounded cache; evict + close segments and       invalidate resources on eviction. [x] 18 Provide index-level FULL_ISOLATION streaming (Risk: MEDIUM)     - Add overload or option to request FULL_ISOLATION on index iterators.     - Implement iterator that holds exclusivity across segments safely.     - Add tests for long-running scans during maintenance. [x] 23 Refactor <code>Segment.close()</code> to async fire-and-forget with READY-only entry (Risk: MEDIUM)     - Change <code>Segment</code> to drop <code>CloseableResource</code> and return       <code>SegmentResult&lt;Void&gt;</code> from <code>close()</code>.     - Close starts only in <code>READY</code>: transition to <code>FREEZE</code>, drain, optionally       flush write cache, then run close work on maintenance thread.     - Completion marks <code>CLOSED</code>, releases locks/resources, and stops admissions.     - Move close-state tracking into segment index (avoid <code>Segment.wasClosed()</code>).     - Update state machine/gate/docs/tests to match the new close lifecycle. [x] 24 Add integration test: in-memory segment lock prevents double-open (Risk: LOW)     - Create an integration test that opens a segment in a directory and       asserts a second open in the same directory fails (lock enforcement). [x] 25 Simplify <code>Segment.flush()</code>/<code>compact()</code> to return status only (Risk: MEDIUM)     - Remove <code>CompletionStage</code> return values from <code>flush()</code> and <code>compact()</code>.     - Operation completion is observable when segment state returns to <code>READY</code>.     - Update callers, docs, and tests that wait on completion stages. [x] 25 Create directory API and layout helpers (Risk: HIGH)     - Add <code>Directory.openSubDirectory(String)</code> + <code>AsyncDirectory.openSubDirectory(String)</code>       and lifecycle helpers <code>Directory.mkdir(String)</code> / <code>Directory.rmdir(String)</code>.     - Implement in <code>FsDirectory</code>, <code>AsyncDirectoryAdapter</code>, and in-memory       <code>MemDirectory</code> equivalents; define semantics for non-empty rmdir.     - Add <code>SegmentDirectoryLayout</code> (or similar) that builds names for:       index, scarce, bloom, delta, properties, and lock files.     - Add tests for directory creation and layout mapping.</p> <p>[x] 26 Introduce segment-rooted <code>SegmentFiles</code> (Risk: HIGH)     - Add a <code>SegmentFiles</code> constructor that accepts a segment root       <code>AsyncDirectory</code> (instead of a flat base directory + id).     - Keep legacy flat layout working (auto-detect existing files, or flag in       <code>SegmentBuilder</code>).     - Update <code>SegmentBuilder</code> to create/use the segment root directory.     - Add tests that both layouts open the same data correctly.</p> <p>[x] 27 Add per-segment <code>.lock</code> file (Risk: MEDIUM)     - Add <code>segment.lock</code> (or <code>.lock</code>) inside the segment directory.     - Acquire lock on segment open; release on close. Fail fast on lock held.     - Add stale-lock recovery policy (manual delete or metadata timestamp).     - Add tests for lock contention and cleanup.</p> <p>[x] 28 Shared properties file structure (Risk: MEDIUM)     - Introduce a common property schema used by segment + segmentindex       packages (e.g. <code>IndexPropertiesSchema</code>).     - Store schema version and required keys; add migration helpers.     - Update <code>SegmentPropertiesManager</code> and <code>IndexConfiguratonStorage</code>       to use the shared schema.</p> <p>[x] 29 Compact flow for directory layout (publish protocol) (Risk: HIGH)     - IO phase (<code>MAINTENANCE_RUNNING</code>):       - Create a new directory, e.g. <code>segment-00001.next/</code> or versioned         <code>segment-00001/v2/</code>.       - Write new index/scarce/bloom/cache files there.       - Write properties with state <code>PREPARED</code> + metadata.     - Publish phase (short <code>FREEZE</code>):       - Mark new directory as <code>ACTIVE</code> in properties (or update a pointer         file <code>segment-00001.active</code>).       - Reload <code>SegmentFiles</code>/<code>SegmentResources</code> to the new root.       - Bump version and return to <code>READY</code>.     - Cleanup:       - Delete old directory only after publish and resource reload.       - Add startup recovery for <code>PREPARED</code> without <code>ACTIVE</code>.     - Align with items 11/12 (atomic swaps + map updates).</p> <p>[x] 30 Split + replace updates (Risk: HIGH)     - Update split/rename logic to use directory swaps or pointer updates.     - Ensure registry + <code>segmentindex</code> metadata remain consistent.     - Add tests for crash recovery and partial swaps. [x] 31 Segment layout uses versioned file names in a single directory (Risk: HIGH)     - Name index/scarce/bloom/delta as <code>vNN-*</code> (for example <code>v01-index.sst</code>,       <code>v01-scarce.sst</code>, <code>v01-bloom-filter.bin</code>, <code>v01-delta-0000.cache</code>).     - Store the active version and counters in <code>manifest.txt</code> (no <code>.active</code>       pointer).     - Use zero-padded 2-digit versions and 4-digit delta ids. [x] 32 Builder/files treat the provided directory as the segment home (Risk: HIGH)     - Require <code>Segment.builder(AsyncDirectory)</code> for construction.     - Lock + properties live inside the segment directory.     - Resolve active version from properties or detected index files. [x] 33 Compaction/flush publish is memory-only (Risk: HIGH)     - IO phase writes versioned files and property updates.     - Publish swaps in-memory version/resources and bumps iterator version.     - Cleanup old version files asynchronously. [x] 34 Registry/tests align with single-directory versioning (Risk: MEDIUM)     - Registry passes segment directories; no active-directory switching.     - Update tests to accept versioned names and per-segment directories. [x] 35 Remove unused close monitor in <code>SegmentConcurrencyGate</code> (Risk: LOW)     - Remove <code>closeMonitor</code> and <code>signalCloseMonitor</code> since nothing waits on it.     - Keep drain behavior in <code>awaitNoInFlight()</code> unchanged. [x] 36 Consolidate in-flight read/write counters in <code>SegmentConcurrencyGate</code> (Risk: LOW)     - Replace <code>inFlightReads</code>/<code>inFlightWrites</code> with a single counter.     - Keep admission rules and drain behavior unchanged.     - Update any stats or tests that rely on read/write split (if introduced). [x] 11 Remove <code>segmentState</code> from segment properties schema (Risk: MEDIUM)     - Remove <code>SegmentKeys.SEGMENT_STATE</code> from <code>IndexPropertiesSchema</code>.     - Update <code>SegmentPropertiesManager</code> to drop <code>getState</code>/<code>setState</code> usage.     - Decide migration behavior for existing properties files. [x] 12 Add <code>getMaxNumberOfDeltaCacheFiles()</code> to <code>Segment</code> (Risk: LOW)     - Implement in <code>SegmentImpl</code>.     - Update any callers/tests that need the accessor. [x] 13 Add <code>maxNumberOfDeltaCacheFiles</code> to <code>IndexConfiguration</code> + builder (Risk: MEDIUM)     - Add config property, validation, defaults, and persistence.     - Plumb through <code>SegmentBuilder</code>/<code>SegmentConf</code> as needed. [x] 14 Wire delta cache file cap into <code>SegmentMaintenancePolicyThreshold</code> (Risk: MEDIUM)     - Add the max file count to policy constructor/state.     - Pass the value from configuration. [x] 15 Enforce delta cache file cap in policy (Risk: MEDIUM)     - In <code>SegmentMaintenancePolicyThreshold</code> (~line 44), trigger maintenance       when delta cache file count exceeds the cap. [x] 16 Enforce segment lock test on open (Risk: MEDIUM)     - Add a test that opening a segment with an existing <code>.lock</code> fails.     - Cover both in-memory and filesystem-backed directories. [x] 17 Document locked-directory behavior in <code>SegmentBuilder</code> (Risk: LOW)     - Clarify how builder reacts when the segment directory is already locked. [x] 18 Acquire segment lock before <code>prepareBuildContext()</code> (Risk: MEDIUM) [x] 19 Add <code>SegmentRegistryResult</code> + status + adapters (Risk: MEDIUM)     - Define result/status types and adapters to/from <code>SegmentResult</code>.     - Unit tests only; no wiring. [x] 20 Add registry state enum + gate (Risk: MEDIUM)     - Define <code>SegmentRegistryState</code> and a small gate/state holder.     - Unit tests only; no integration. [x] 21 Introduce <code>SegmentRegistry</code> interface + <code>SegmentRegistryImpl</code> (Risk: MEDIUM)     - Keep interface minimal and keep <code>SegmentResult</code> returns for now.     - Rename existing class to impl and update call sites in same step. [x] 22 Add <code>SegmentRegistrySyncAdapter</code> with BUSY retry (Risk: MEDIUM)     - Wrap <code>SegmentRegistry</code> and retry BUSY (use <code>IndexRetryPolicy</code>). [x] 23 Wire state gate into impl (Risk: HIGH)     - BUSY only from registry state; FREEZE only around map changes.     - Keep <code>SegmentResult</code> API to avoid broad changes. [x] 24 Switch registry API to <code>SegmentRegistryResult</code> (Risk: HIGH)     - Introduce <code>SegmentRegistryLegacyAdapter</code> to keep old callers working.     - Migrate call sites/tests, then remove legacy adapter. [x] 53.1 Split \u201capply\u201d DTO (Risk: LOW)     - Introduce a small DTO for split apply (oldId, lowerId, upperId,       min/max keys, status).     - Unit tests for DTO invariants. [x] 53.2 Split worker extraction (Risk: MEDIUM)     - Refactor split execution to: open FULL_ISOLATION iterator, run split on       maintenance executor, return DTO without touching registry or map.     - Ensure iterator is closed in all paths.     - Unit tests for result wiring. [x] 53.3 Registry apply entry point (Risk: MEDIUM)     - Add registry apply method that (a) FREEZE, (b) update cache       (remove old, add new ids), (c) exit FREEZE.     - Keep key\u2011map lock separate.     - Unit tests for cache mutation under FREEZE. [x] 53.4 Key\u2011map persistence (Risk: MEDIUM)     - Update key\u2011to\u2011segment map using its own lock/adapter.     - Persist map file after in\u2011memory registry apply.     - Tests that map persistence order is enforced. [x] 53.5 Old segment deletion (Risk: MEDIUM)     - Delete old segment directory only after map persistence and after       iterator/segment locks are released.     - Tests that deletion never happens before map persistence. [x] 53.6 Lock order contract (Risk: LOW)     - Enforce lock order (segment \u2192 registry \u2192 map; release map \u2192 registry       \u2192 segment) and document in code.     - Add a small test or assertion helper to catch order violations. [x] 53.7 Split concurrency scenarios (Risk: HIGH)     - Tests:       - split does not run under registry FREEZE (short window)       - split returns BUSY on lock conflict and retries safely       - concurrent get/put during split never sees missing segment mapping [x] 58.1 Split: keep split IO outside registry freeze (Risk: HIGH)     - <code>SegmentSplitCoordinator.split(...)</code>: ensure all IO (iterator open, writes)       happens before any registry <code>FREEZE</code>.     - <code>SegmentSplitStepOpenIterator</code>: keep <code>FULL_ISOLATION</code> acquisition once per split.     - <code>SegmentSplitCoordinator.hasLiveEntries(...)</code>: now uses <code>FAIL_FAST</code> to       avoid a second <code>FULL_ISOLATION</code> lock.     - Tests may fail if ordering assumptions change; fix after step 58.4. [x] 58.2 Split: invert lock order for apply phase (Risk: HIGH)     - <code>SegmentSplitCoordinator.applySplitPlan(...)</code>: remove outer       <code>keyToSegmentMap.withWriteLock(...)</code>.     - <code>SegmentRegistryImpl.applySplitPlan(...)</code>: acquire registry freeze first,       then call <code>onApplied</code> which acquires key-map write lock.     - Update lock-order enforcement flags to match registry -&gt; key-map. [x] 58.3 Split: propagate lock-order flags into key-map adapter (Risk: MEDIUM)     - <code>KeyToSegmentMapSynchronizedAdapter</code>: set/clear <code>keyMapLockHeld</code> around       write-lock acquisition when enforcement is enabled.     - Ensure registry checks validate <code>registryLockHeld</code> before key-map lock. [x] 58.4 Split: finalize apply/cleanup ordering (Risk: MEDIUM)     - Ensure apply evicts old segment instance and closes it via       <code>SegmentRegistryImpl.closeSegmentInstance(...)</code>.     - Keep key-map flush outside registry freeze:       <code>keyToSegmentMap.optionalyFlush()</code> only after apply OK.     - Delete old segment files only after apply succeeds and locks released. [x] 58.5 Split: test alignment (Risk: MEDIUM)     - Add/update tests to assert no directory swap in split flow.     - Add tests for enforced lock order (registry -&gt; key-map).     - Add tests for split failure cleanup of new segments. [x] 63 SegmentIdAllocator in segmentregistry (Risk: MEDIUM)     - Add <code>SegmentIdAllocator</code> interface and directory-backed implementation.     - Scan <code>AsyncDirectory.getFileNamesAsync()</code> for segment directories named       <code>segment-00001</code> (prefix <code>segment-</code> + 5 digits) and initialize next id       to max+1 (or 1 when none found).     - Allocate ids with thread-safe counter.</p> <p>[x] 64 Include directories in <code>Directory.getFileNames()</code> (Risk: LOW)     - Ensure <code>Directory.getFileNames()</code> returns subdirectory names as well.     - Update <code>MemDirectory</code> to include subdirectory names in its stream.     - Verify no tests rely on file-only behavior.</p> <p>[x] 65 Remove id allocation from key-to-segment map (Risk: MEDIUM)     - Remove <code>nextSegmentId</code> and <code>findNewSegmentId()</code> from <code>KeyToSegmentMap</code>       and its synchronized adapter.     - Remove updates to <code>nextSegmentId</code> in <code>tryExtendMaxKey</code>/<code>updateMaxKey</code>.</p> <p>[x] 66 Wire allocator into registry + index (Risk: MEDIUM)     - Update <code>SegmentRegistryImpl</code> to use <code>SegmentIdAllocator</code> instead of       supplier.     - Update <code>SegmentIndexImpl</code> wiring and split coordinator to use registry       allocation only.     - Update tests to stub allocator or use directory-backed allocator.</p> <p>[x] 67 Tests + docs for allocator move (Risk: LOW)     - Add allocator tests (empty dir, max id, thread-safety).     - Update <code>docs/architecture/registry/registry.md</code> to reflect registry allocator.</p> <p>[x] 62 Add <code>SegmentRegistryBuilder</code> modeled after <code>Segment.builder(...)</code> (Risk: MEDIUM)     - Add <code>SegmentRegistryBuilder</code> in <code>segmentregistry</code> with required inputs       (directory, type descriptors, config, maintenance executor).     - Provide optional setters for <code>SegmentIdAllocator</code> and <code>SegmentFactory</code>.     - Add static factory <code>SegmentRegistry.builder(...)</code> (or on impl) to return builder.     - Move default wiring (factory + allocator creation) into builder.     - Keep <code>SegmentRegistryImpl</code> constructor with full DI for tests.     - Update <code>SegmentIndexImpl</code> (and other callers) to use the builder.     - Add unit tests for missing required fields and default wiring.</p> <p>[x] 68 Align split apply with registry FREEZE + lock-order enforcement (Risk: MEDIUM)     - Expose registry FREEZE in <code>SegmentRegistryAccess</code> (or equivalent) so       split apply can run under FREEZE while holding handler + key-map locks.     - While FREEZE is active, set <code>hestiastore.registryLockHeld=true</code> so       key-map lock order enforcement can be enabled safely.     - Wrap key-map apply + cache eviction inside the FREEZE window.</p> <p>[x] 69 Separate cache eviction from file deletion in split apply (Risk: MEDIUM)     - Add registry operation to evict a specific segment from cache while the       handler lock is held (no file deletion).     - After apply: evict old segment under handler+FREEZE, release iterator,       unlock handler, then delete old segment files via registry helper.     - Keep <code>deleteSegment</code> behavior for general callers unchanged.</p> <p>[x] 70 Apply-failure should mark registry ERROR (Risk: LOW)     - When split apply fails mid-update, set registry gate to ERROR and       surface the failure (avoid silent BUSY loops).     - Add tests for apply-failure transitions.</p> <p>[x] 71 SegmentRegistry: expose NOT_FOUND for missing segments (Risk: LOW)     - Add <code>NOT_FOUND</code> to <code>SegmentRegistryResultStatus</code> + factory method.     - Return NOT_FOUND when <code>getSegment</code> targets a missing directory.     - Keep <code>createSegment</code> creating new segments even when others exist.     - Tests: missing-segment lookup, status plumbing.</p> <p>[x] 72 SegmentRegistryBuilder: configure only via <code>with*</code> methods (Risk: LOW)     - Remove constructor parameters from <code>SegmentRegistryBuilder</code>.     - Ensure all required inputs are set via <code>with...</code> methods.     - Update call sites and tests to use the builder setters.</p> <p>[x] 73 SegmentRegistry handler-backed cache (Risk: MEDIUM)     - Make <code>SegmentRegistryCache</code> store <code>SegmentHandler</code> per <code>SegmentId</code>       (segment + lock state as one entry).     - Keep <code>SegmentRegistry.getSegment</code> returning <code>SegmentRegistryResult</code>       to signal registry state; map LOCKED to BUSY.     - Add internal accessors for handler-only flows (split/evict) without       exposing handler in the public registry API.     - Update eviction logic to skip LOCKED handlers and keep cache/handler       in sync.     - Tests: locked entry not evicted, handler/segment consistency, BUSY       returned when handler locked.</p> <p>[x] 74 RegistryAccess: lock via <code>SegmentHandler</code> (Risk: MEDIUM)     - Add internal accessor that returns the <code>SegmentHandler</code> for a       <code>segmentId</code> + expected segment instance (BUSY/ERROR when mismatch).     - Remove <code>lockSegmentHandler</code>/<code>unlockSegmentHandler</code> from       <code>SegmentRegistryLocking</code> and <code>SegmentRegistryAccess</code>.     - Update <code>SegmentRegistryAccessAdapter</code> to expose handler instead of       lock/unlock methods.</p> <p>[x] 75 Split flow: use handler lock directly (Risk: MEDIUM)     - In <code>SegmentSplitCoordinator</code>, acquire handler via registry access and       call <code>handler.lock()</code>/<code>handler.unlock()</code> directly.     - Keep BUSY mapping when handler is locked.     - Ensure eviction path still validates handler instance + state.</p> <p>[x] 76 Tests + cleanup for handler locking (Risk: LOW)     - Update tests that currently call registry lock/unlock to use handler       locking instead.     - Remove unused lock methods from <code>SegmentRegistryImpl</code>.     - Verify eviction skips locked handlers and BUSY is returned when locked.</p> <p>[x] 77 SegmentRegistry target-state rollout from <code>docs/architecture/registry/registry.md</code> (Risk: HIGH)     - Goal: make implementation fully match the documented registry model       (state gate + per-key <code>Entry</code> state machine + single-flight load +       bounded cache eviction + unload semantics).     - Global rule: every step in 77.x must preserve behavioral parity with       <code>docs/architecture/registry/registry.md</code>. If behavior must change, update       <code>registry.md</code> and diagrams first in the same PR before code changes.     - Hard constraints:       - no global lock in <code>get</code> hot path       - unrelated keys must not block each other       - per-key wait only on the same <code>Entry</code>       - <code>LOADING</code> waits, <code>UNLOADING</code> maps to <code>BUSY</code>       - load/open failures are exception-driven     - Exit criteria:       - behavior parity with <code>docs/architecture/registry/registry.md</code> and         <code>docs/architecture/images/registry-seq*.plantuml</code>       - all new/updated tests green       - no flakiness in repeated concurrency runs</p> <p>[x] 77.1 Freeze target contract and remove ambiguity (Risk: HIGH)     - Pin <code>docs/architecture/registry/registry.md</code> + diagrams as source of truth.     - Explicitly list non-negotiable runtime rules in code comments/Javadocs:       - state gate mapping: <code>READY</code> normal, <code>FREEZE</code> -&gt; <code>BUSY</code>,         <code>CLOSED</code> -&gt; <code>CLOSED</code>, <code>ERROR</code> -&gt; <code>ERROR</code>       - cache state mapping: <code>LOADING</code> wait, <code>UNLOADING</code> -&gt; <code>BUSY</code>       - failed unload leaves <code>UNLOADING</code> (documented behavior)     - Acceptance:       - no contradictory comments/Javadocs in <code>segmentregistry</code> package       - docs and code contracts use same method names</p> <p>[x] 77.2 Implement/align per-key <code>Entry</code> API contract (Risk: HIGH)     - Ensure <code>SegmentRegistryCache.Entry</code> exposes and follows:       - <code>tryStartLoad()</code>       - <code>waitWhileLoading(currentAccessCx)</code>       - <code>finishLoad(value)</code>       - <code>fail(exception)</code>       - <code>tryStartUnload()</code>       - <code>finishUnload()</code>       - <code>getEvictionOrder()</code>     - Ensure lock/condition is strictly per-entry (no cross-key monitor).     - Acceptance:       - transitions only: <code>MISSING-&gt;LOADING-&gt;READY-&gt;UNLOADING-&gt;MISSING</code>       - invalid transitions return fast/fail predictably</p> <p>[x] 77.3 Align <code>get(key)</code> miss path to single-flight semantics (Risk: HIGH)     - Use <code>putIfAbsent</code> race semantics correctly:       - winner: <code>entryInMap == null</code> then load       - loser: wait on the existing entry from map     - Ensure wait target is the entry stored in map, not a local temporary.     - Ensure load failure path calls <code>fail(exception)</code>, wakes waiters, and       removes the expected entry from map.     - Acceptance:       - exactly one loader execution per key under high contention       - all losers observe winner result or propagated exception</p> <p>[x] 77.4 Align <code>get(key)</code> hit path semantics (Risk: HIGH)     - READY: immediate return + recency update.     - LOADING: block only on same entry until READY/failure.     - UNLOADING: do not wait; return BUSY to caller.     - Acceptance:       - no waiting on keys in <code>UNLOADING</code>       - no blocking between unrelated keys</p> <p>[x] 77.5 Implement bounded eviction flow per docs (Risk: HIGH)     - Keep capacity enforcement in cache layer.     - Candidate selection:       - LRU by <code>accessCx</code>       - exclude requested key in <code>removeLastRecentUsedSegment(exceptSegmentId)</code>       - only READY candidates can move to UNLOADING     - Start close asynchronously, remove only after close success.     - Acceptance:       - eviction never unloads <code>exceptSegmentId</code>       - failed <code>tryStartUnload</code> retries candidate selection without global stall</p> <p>[x] 77.6 Lifecycle executor behavior and failure handling (Risk: HIGH)     - Verify load/open and close/unload execution contexts follow design:       - load for seq03 scenario in caller thread       - close/unload on lifecycle executor thread     - Define exact reaction to close failure:       - keep entry <code>UNLOADING</code>       - subsequent <code>get</code> returns BUSY       - do not remove cache entry     - Acceptance:       - no caller-thread close IO       - failed close path is deterministic and test-covered</p> <p>[x] 77.7 Registry gate lifecycle alignment (Risk: MEDIUM)     - Ensure startup: <code>FREEZE -&gt; READY</code>.     - Ensure close flow: <code>READY -&gt; FREEZE -&gt; CLOSED</code>.     - Ensure idempotent close and terminal ERROR semantics.     - Acceptance:       - gate transitions are atomic and race-safe under concurrent calls       - status mapping is consistent for all operations</p> <p>[x] 77.8 API/status cleanup to match exception-driven load policy (Risk: MEDIUM)     - Preserve <code>SegmentRegistryAccess</code> for status-oriented flows.     - Keep load/open failure as propagated runtime exception from registry       load paths (per docs).     - Remove or deprecate status branches that conflict with this policy.     - Acceptance:       - no mixed behavior where same failure is sometimes status, sometimes throw</p> <p>[x] 77.9 Unit tests for Entry/cache state machine (Risk: HIGH)     - Extend <code>SegmentRegistryCacheTest</code> with deterministic tests:       - single-flight: same key, many threads -&gt; loader called once       - wait-on-loading: loser threads block and then return same value       - load failure wakeup: all waiters receive same failure       - unloading maps to BUSY (no waiting)       - eviction excludes <code>exceptSegmentId</code>       - close failure leaves <code>UNLOADING</code>     - Use <code>CountDownLatch</code>/<code>CyclicBarrier</code> to force races.     - Add <code>@Timeout</code> to every concurrency-sensitive test.</p> <p>[x] 77.10 Registry-level behavior tests (Risk: HIGH)     - Update/add tests in:       - <code>SegmentRegistryImplTest</code>       - <code>SegmentRegistryStateMachineTest</code>       - <code>SegmentRegistryAccessImplTest</code>     - Verify:       - gate mapping (<code>FREEZE/BUSY</code>, <code>CLOSED/CLOSED</code>, <code>ERROR/ERROR</code>)       - startup transition (<code>FREEZE-&gt;READY</code>)       - <code>getSegment</code> behavior across READY/LOADING/UNLOADING       - exception propagation on load/open failure</p> <p>[x] 77.11 High-concurrency integration verification (Risk: HIGH)     - Extend/execute:       - <code>IntegrationSegmentIndexConcurrencyTest</code>       - <code>SegmentIndexImplConcurrencyTest</code>       - <code>SegmentSplitCoordinatorConcurrencyTest</code>     - Add focused registry stress tests (new class):       - many threads on same key (single-flight proof)       - many threads on different keys (independence proof)       - eviction + concurrent gets + split coordinator interaction     - Run repeated stress cycles to catch flakes.     - Completed:       - Added and executed         <code>src/test/java/org/hestiastore/index/segmentindex/SegmentRegistryConcurrencyStressTest.java</code>.       - Passed:         <code>mvn -q -Dtest=IntegrationSegmentIndexConcurrencyTest,SegmentIndexImplConcurrencyTest,SegmentSplitCoordinatorConcurrencyTest,SegmentRegistryConcurrencyStressTest test</code>       - Flake gate passed: 20/20 repeated runs with 0 failures.</p> <p>[x] 77.12 Quality gates and release checklist (Risk: HIGH)     - Mandatory local gates before merge:       - targeted unit tests:         <code>mvn -q -Dtest=SegmentRegistryCacheTest,SegmentRegistryImplTest,SegmentRegistryStateMachineTest test</code>       - concurrency/integration tests:         <code>mvn -q -Dtest=IntegrationSegmentIndexConcurrencyTest,SegmentIndexImplConcurrencyTest,SegmentSplitCoordinatorConcurrencyTest test</code>       - full verification:         <code>mvn verify</code>     - Flake gate:       - rerun concurrency suite N times (recommended N=20) and require 0 flakes.     - Code quality gate:       - no TODO/FIXME left in touched files       - Javadocs reflect final behavior       - diagrams and <code>registry.md</code> updated if behavior changed     - Completed:       - Passed targeted unit tests:         <code>mvn -q -Dtest=SegmentRegistryCacheTest,SegmentRegistryImplTest,SegmentRegistryStateMachineTest test</code>       - Passed concurrency/integration tests:         <code>mvn -q -Dtest=IntegrationSegmentIndexConcurrencyTest,SegmentIndexImplConcurrencyTest,SegmentSplitCoordinatorConcurrencyTest,SegmentRegistryConcurrencyStressTest test</code>       - Passed full verification:         <code>mvn verify</code>       - <code>TODO/FIXME</code> scan on touched files: none found.</p> <p>[x] 77.13 Rollout and fallback plan (Risk: MEDIUM)     - Deliver in small PRs matching 77.1-77.12 order.     - After each PR:       - run targeted regression suite       - update <code>docs/architecture/registry/registry.md</code> if contract changed     - Keep a temporary feature flag only if needed for safe migration.     - Remove fallback/compatibility code when final parity is reached.     - Completed:       - Work delivered incrementally following 77.1 -&gt; 77.12 sequence.       - Regression suites executed after key steps and before final merge gate.       - No temporary feature flag required for this rollout.</p>"},{"location":"architecture/","title":"\ud83e\udded Architecture","text":"<p>This section is organized by responsibility to keep related topics together and to centralize segment internals in one place.</p>"},{"location":"architecture/#sections","title":"Sections","text":"<ul> <li>General \u2014 cross-cutting format, integrity, recovery,   limits, and glossary.</li> <li>SegmentIndex \u2014 top-level index orchestration:   read/write paths, caching, performance, and index concurrency.</li> <li>Segment \u2014 central place for segment internals:   file layout, delta cache, Bloom filter, sparse/scarce index, and segment   lifecycle.</li> <li>Registry \u2014 segment registry state machine, cache-entry   model, and concurrent loading/unloading flows.</li> </ul>"},{"location":"architecture/general/","title":"\ud83e\udde9 General Architecture","text":"<p>Cross-cutting architecture topics that apply to the whole engine (not to one component only).</p>"},{"location":"architecture/general/#topics","title":"Topics","text":"<ul> <li>Data Block Format \u2014 low-level block and chunk structure.</li> <li>Filters &amp; Integrity \u2014 chunk filter pipeline and validation.</li> <li>Chain of Filters \u2014 shared filter-chain helper.</li> <li>Concurrency Model \u2014 index-wide synchronization model.</li> <li>Consistency &amp; Recovery \u2014 crash-safety and recovery model.</li> <li>Limitations &amp; Trade-offs \u2014 current constraints and risks.</li> <li>Glossary \u2014 shared terminology.</li> </ul>"},{"location":"architecture/general/chain-of-filters/","title":"\ud83d\udd17 Chain of Filters","text":"<p><code>AbstractChainOfFilters</code> is the small engine that runs ordered pipelines inside the index. It accepts an immutable context object plus a mutable result carrier and iterates over a list of <code>Filter&lt;Context, Result&gt;</code> steps. Each step returns <code>true</code> to continue or <code>false</code> to short-circuit, which is how we cheaply exit when work is already finished (cache hits) or impossible (invalid plan).</p>"},{"location":"architecture/general/chain-of-filters/#mechanics","title":"\u2699\ufe0f Mechanics","text":"<ul> <li>The base class keeps the ordered <code>List&lt;Filter&lt;Context, Result&gt;&gt;</code> and exposes a single <code>filter(context, result)</code> method that drives the loop.</li> <li>Steps see the immutable context and may update the mutable result; they must return <code>false</code> once they have produced a terminal outcome so the chain stops immediately.</li> <li>Subclasses wrap the call to <code>filter</code> so they can prepare state/result objects and handle cleanup. Example: <code>SegmentSplitPipeline#run</code> closes the iterator in a <code>finally</code> block even when a step short-circuits.</li> <li>Filters are regular classes that implement <code>Filter&lt;Context, Result&gt;</code> and can be reused or composed in different orders when building a pipeline.</li> </ul>"},{"location":"architecture/general/chain-of-filters/#typical-usage","title":"\ud83d\udee0\ufe0f Typical Usage","text":"<p>1) Define small, focused filters:</p> <pre><code>final class ValidateInput implements Filter&lt;RequestCtx, ProcessingState&gt; {\n    @Override\n    public boolean filter(final RequestCtx ctx, final ProcessingState state) {\n        if (!ctx.isValid()) {\n            state.fail(\"missing fields\");\n            return false; // stop the chain early\n        }\n        return true;\n    }\n}\n</code></pre> <p>2) Compose them into a pipeline subclass that owns setup/teardown:</p> <pre><code>final class ProcessingPipeline\n        extends AbstractChainOfFilters&lt;RequestCtx, ProcessingState&gt; {\n\n    ProcessingPipeline(\n            final List&lt;Filter&lt;RequestCtx, ProcessingState&gt;&gt; steps) {\n        super(List.copyOf(steps));\n    }\n\n    ProcessingState run(final RequestCtx ctx) {\n        final ProcessingState state = new ProcessingState();\n        filter(ctx, state); // short-circuit rules apply\n        return state;\n    }\n}\n</code></pre>"},{"location":"architecture/general/chain-of-filters/#where-it-is-used","title":"\ud83d\udccd Where It Is Used","text":""},{"location":"architecture/general/chain-of-filters/#segment-searcher-pipeline-read-path","title":"Segment Searcher Pipeline (read path)","text":"<p><code>SegmentSearcherPipeline</code> (<code>src/main/java/org/hestiastore/index/segment/SegmentSearcherPipeline.java</code>) runs a three-step lookup when serving <code>SegmentSearcher#get</code>:</p> <pre><code>final SegmentSearcherPipeline&lt;K, V&gt; pipeline = new SegmentSearcherPipeline&lt;&gt;(\n        List.of(\n            new SegmentSearcherStepDeltaCache&lt;&gt;(valueTypeDescriptor), // stop if cached hit or tombstone\n            new SegmentSearcherStepBloomFilter&lt;&gt;(),                   // stop if bloom says \u201cdefinitely not\u201d\n            new SegmentSearcherStepIndexFile&lt;&gt;()));                   // stop after sparse-index + on-disk read\n\npipeline.run(ctx, result);\nreturn result.getValue();\n</code></pre> <ul> <li>Short-circuits: cache hit/tombstone, Bloom filter negative, or sparse-index probe missing/false-positive correction.</li> <li>Context/result types: <code>SegmentSearcherContext</code> carries the key plus caches/searcher; <code>SegmentSearcherResult</code> holds the found value (or <code>null</code> when absent).</li> </ul>"},{"location":"architecture/general/chain-of-filters/#segment-split-pipeline-splitcompaction-path","title":"Segment Split Pipeline (split/compaction path)","text":"<p><code>SegmentSplitPipeline</code> (<code>src/main/java/org/hestiastore/index/segment/SegmentSplitPipeline.java</code>) orchestrates the per-segment split flow inside <code>SegmentSplitter#split</code>:</p> <pre><code>final SegmentSplitPipeline&lt;K, V&gt; pipeline = new SegmentSplitPipeline&lt;&gt;(\n        List.of(\n            new SegmentSplitStepValidateFeasibility&lt;&gt;(),      // throws if plan is invalid\n            new SegmentSplitStepOpenIterator&lt;&gt;(),             // opens streaming iterator\n            new SegmentSplitStepCreateLowerSegment&lt;&gt;(),       // prepares target segment\n            new SegmentSplitStepFillLowerUntilTarget&lt;&gt;(),     // writes first half\n            new SegmentSplitStepEnsureLowerNotEmpty&lt;&gt;(),      // guardrail for tiny splits\n            new SegmentSplitStepReplaceIfNoRemaining&lt;&gt;(segmentReplacer), // compaction path, may stop here\n            new SegmentSplitStepWriteRemainingToCurrent&lt;&gt;()   // split path, always stops after writing\n        ));\n\nfinal SegmentSplitterResult&lt;K, V&gt; result = pipeline.run(ctx);\n</code></pre> <ul> <li>Short-circuits: compaction path (<code>SegmentSplitStepReplaceIfNoRemaining</code>) replaces the original segment and stops; the final write step stops after producing the split result.</li> <li>Context/result types: <code>SegmentSplitContext</code> carries the segment, version controller, plan, and IDs; <code>SegmentSplitState</code> owns the iterator, lower-segment handle, and final <code>SegmentSplitterResult</code>.</li> <li>Safety: <code>SegmentSplitPipeline#run</code> wraps <code>filter</code> so the opened iterator closes even when a step throws or short-circuits.</li> </ul>"},{"location":"architecture/general/chain-of-filters/#guidelines-for-adding-new-pipelines","title":"\u2705 Guidelines for Adding New Pipelines","text":"<ul> <li>Keep steps single-purpose and side-effect aware; ensure they set a final result before returning <code>false</code>.</li> <li>Copy the incoming steps list (<code>List.copyOf(...)</code>) to avoid accidental reordering at runtime.</li> <li>When resources need cleanup, wrap the <code>filter</code> invocation in a <code>try/finally</code> inside your subclass.</li> <li>Pair unit tests with short-circuit cases; see <code>src/test/java/org/hestiastore/index/AbstractChainOfFiltersTest.java</code> for a minimal example.</li> </ul>"},{"location":"architecture/general/concurrency/","title":"Concurrency Model","text":""},{"location":"architecture/general/concurrency/#overview","title":"Overview","text":"<p>SegmentIndex is thread-safe and does not use a global read/write lock. Sync operations execute on the caller thread; concurrency is controlled by per-segment state machines and minimal shared-structure locks (mapping, registry).</p> <p>Segment-level concurrency does not require external locks. <code>SegmentImpl</code> enforces its own lock-free admission gate (see <code>docs/architecture/segment/segment-concurrency.md</code>).</p>"},{"location":"architecture/general/concurrency/#concurrency-invariants-target-design","title":"Concurrency Invariants (Target Design)","text":"<p>These invariants must hold for the current per-segment concurrency design:</p> <ul> <li>Mapping integrity: the key\u2192segment map must always point to an existing   segment; updates are atomic and visible to readers.</li> <li>Split atomicity: a segment split must either be fully applied (new   segments + updated map) or not applied at all.</li> <li>Cache visibility: reads observe the latest cached writes (read-after-write   visibility) and flushes operate on a consistent snapshot.</li> <li>Lifecycle linearity: once close starts, no new operations are accepted;   operations in flight either complete before close returns or fail deterministically.</li> <li>No use-after-close: evicted/closed segment data must not be accessed by   concurrent readers.</li> <li>Stats consistency: counters reflect all completed operations (exact or   eventually consistent by design).</li> </ul>"},{"location":"architecture/general/concurrency/#shared-state-that-must-be-protected","title":"Shared State That Must Be Protected","text":"<p>These structures are shared across threads and require synchronization:</p> <ul> <li><code>UniqueCache</code> (index write buffer)</li> <li><code>KeyToSegmentMap</code> (key\u2192segment map)</li> <li><code>SegmentRegistry</code> and <code>SegmentDataCache</code> (segment lookup + cached data)</li> <li><code>SegmentSplitCoordinator</code> (topology updates)</li> <li><code>IndexState</code> (open/close state) and <code>Stats</code> (counters)</li> <li>Any <code>TypeDescriptor</code> implementation with mutable internal state</li> </ul>"},{"location":"architecture/general/concurrency/#ordering","title":"Ordering","text":"<ul> <li>With multiple threads, operation order is not guaranteed.</li> <li>If strict ordering is required, apply external synchronization at the caller   level.</li> <li>Across segments, operations can complete in any order.</li> </ul>"},{"location":"architecture/general/concurrency/#threads","title":"Threads","text":"<ul> <li>Sync operations run on caller threads.</li> <li>Async operations run on background threads via <code>IndexAsyncAdapter</code>.</li> <li>Segment maintenance runs on the segment-index maintenance executor.</li> </ul>"},{"location":"architecture/general/concurrency/#implications","title":"Implications","text":"<ul> <li>Write/write and read/write conflicts are handled by per-segment state   machines and shared-structure locks, not a global lock.</li> <li>Read-heavy workloads benefit from parallelism.</li> <li>Callers should not rely on global ordering across keys.</li> </ul>"},{"location":"architecture/general/datablock/","title":"\ud83d\udce6 Data Block and Chunk Design in HestiaStore","text":"<p>This document describes the structure and purpose of <code>Block</code> and <code>Chunk</code> objects in the HestiaStore storage engine.</p> <p></p>"},{"location":"architecture/general/datablock/#block","title":"\ud83e\uddf1 Block","text":"<p>A Block is the lowest-level physical unit of storage. It has a fixed size (typically a multiple of 4KB) and is directly written to disk.</p>"},{"location":"architecture/general/datablock/#key-characteristics","title":"Key Characteristics:","text":"<ul> <li>Fixed Size: Determined by <code>BlockFile#getBlockSize()</code>.</li> <li>Header: Each block includes a header with metadata (e.g. magic number, CRC32 checksum, data length).</li> <li>Payload: The remaining portion of the block contains user data (<code>getPayloadSize()</code> returns the usable size).</li> </ul>"},{"location":"architecture/general/datablock/#block-header-format","title":"Block Header Format:","text":"Offset Size Field Description 0 4 B <code>magic</code> Identifier for block integrity check 4 4 B <code>crc32</code> CRC checksum for payload verification 8 4 B <code>dataLength</code> Actual size of data in the block 12+ N/A <code>payload</code> User data payload <p>Blocks are stored and retrieved via the <code>BlockFile</code> abstraction.</p>"},{"location":"architecture/general/datablock/#chunk","title":"\ud83d\udce6 Chunk","text":"<p>A Chunk represents a variable-sized, logical data unit stored inside a block. It is used to store optionally compressed sets of key-value entries.</p>"},{"location":"architecture/general/datablock/#key-characteristics_1","title":"Key Characteristics:","text":"<ul> <li>Variable Size: Can be smaller or span multiple blocks depending on compression.</li> <li>Stored Inside Blocks: Uses the <code>BlockFile</code> to persist data.</li> <li>Compressible: Designed for efficient compression and decompression.</li> <li>Encapsulated Metadata: Chunks also have a header to ensure validity and interpretability.</li> </ul>"},{"location":"architecture/general/datablock/#chunk-header-format","title":"Chunk Header Format:","text":"Offset Size Field Description 0 4 B <code>magic</code> Chunk type signature 4 4 B <code>crc32</code> CRC of compressed payload 8 4 B <code>compressedLength</code> Length of compressed data 12 4 B <code>uncompressedLength</code> Length of original (uncompressed) data 16+ N/A <code>payload</code> Compressed chunk data <p>Chunks are managed through the <code>ChunkFileStore</code> and written using <code>ChunkWriter</code>.</p>"},{"location":"architecture/general/datablock/#relationships","title":"\ud83d\udd17 Relationships","text":"<ul> <li><code>BlockFile</code> provides the persistent storage mechanism.</li> <li><code>ChunkFileStore</code> maps chunk positions to blocks and provides higher-level access.</li> <li>CRC validation is used in both blocks and chunks to ensure data consistency and detect corruption.</li> </ul>"},{"location":"architecture/general/filters/","title":"\ud83e\uddea Filters &amp; Integrity","text":"<p>HestiaStore persists segment data in chunked files. Each chunk carries a small header and a payload processed by an ordered filter pipeline. Filters provide integrity (magic number, CRC32), optional compression, and optional reversible transformations. The same concept exists on both the write path (encoding pipeline) and read path (decoding pipeline).</p> <p>This page summarizes what the filters do, how they are ordered, and how to configure them.</p>"},{"location":"architecture/general/filters/#chunk-header-and-flags","title":"\ud83c\udff7\ufe0f Chunk Header and Flags","text":"<p>Every chunk has a header with these fields:</p> <ul> <li>Magic number \u2014 constant identifying HestiaStore chunk format</li> <li>Version \u2014 current format version (presently 1)</li> <li>Payload length \u2014 number of payload bytes (unpadded)</li> <li>CRC32 \u2014 checksum of payload bytes (see ordering recommendations below)</li> <li>Flags \u2014 bit field describing which filters transformed the payload</li> </ul> <p>Flag bit positions (see <code>src/main/java/org/hestiastore/index/chunkstore/ChunkFilter.java</code>):</p> <ul> <li>0 \u2014 magic number present</li> <li>1 \u2014 CRC32 present (bit reserved; validation uses the header value)</li> <li>3 \u2014 Snappy compression</li> <li>4 \u2014 XOR encryption (reversible obfuscation)</li> </ul>"},{"location":"architecture/general/filters/#encoding-pipeline-write-path","title":"\u270d\ufe0f Encoding Pipeline (Write Path)","text":"<p>Write path constructs a <code>ChunkData</code> and passes it through a <code>ChunkProcessor</code> configured with encoding filters. The writer then combines the resulting header and (possibly transformed) payload and writes padded bytes to the underlying cell store.</p> <ul> <li>Implementation: <code>chunkstore/ChunkStoreWriterImpl#write</code></li> <li>Pipeline wrapper: <code>chunkstore/ChunkProcessor</code> with encoding filters</li> <li>Typical defaults: CRC32 \u2192 MagicNumber</li> <li>With compression/encryption enabled, recommended order:</li> <li>CRC32 writing \u2192 Magic number writing \u2192 Snappy compression \u2192 XOR encrypt</li> </ul> <p>Why this order: - CRC32 computed on the plaintext payload gives a strong data\u2011integrity check after decoding (you must decompress/decrypt before CRC validation on read). - Magic\u2011number header flag is a quick consistency guard before attempting other transforms.</p>"},{"location":"architecture/general/filters/#decoding-pipeline-read-path","title":"\ud83d\udcd6 Decoding Pipeline (Read Path)","text":"<p>Read path pulls a raw chunk, parses the header, then applies the decoding filters in order. The final <code>ChunkData</code> is used to rebuild a consistent <code>Chunk</code> instance with the validated header and payload.</p> <ul> <li>Implementation: <code>chunkstore/ChunkStoreReaderImpl#read</code></li> <li>Pipeline wrapper: <code>chunkstore/ChunkProcessor</code> with decoding filters</li> <li>Typical defaults: MagicNumber validation \u2192 CRC32 validation</li> <li>With compression/encryption enabled, recommended order:</li> <li>MagicNumber validation \u2192 XOR decrypt \u2192 Snappy decompress \u2192 CRC32 validation</li> </ul> <p>Notes: - Validation filters check the corresponding header flag (when provided) and throw an exception if the precondition fails (e.g., \u201cnot marked as compressed\u201d). - CRC validation recomputes CRC32 on the current payload and compares to the header value.</p>"},{"location":"architecture/general/filters/#available-filters","title":"\ud83e\uddf0 Available Filters","text":"<p>Integrity - Magic number writing/validation: <code>ChunkFilterMagicNumberWriting</code>, <code>ChunkFilterMagicNumberValidation</code>   - Sets and validates the fixed magic number; toggles bit 0 in flags. - CRC32 writing/validation: <code>ChunkFilterCrc32Writing</code>, <code>ChunkFilterCrc32Validation</code>   - Computes/stores CRC over the payload; validation recomputes and compares.</p> <p>Compression - Snappy compress/decompress: <code>ChunkFilterSnappyCompress</code>, <code>ChunkFilterSnappyDecompress</code>   - Fast compression; sets/clears bit 3 in flags.</p> <p>Transformations - XOR encrypt/decrypt: <code>ChunkFilterXorEncrypt</code>, <code>ChunkFilterXorDecrypt</code>   - Lightweight reversible obfuscation; sets/clears bit 4 in flags.</p> <p>Utility - No\u2011op: <code>ChunkFilterDoNothing</code> (testing/bench harnesses)</p>"},{"location":"architecture/general/filters/#configuration","title":"\u2699\ufe0f Configuration","text":"<p>Filters are configured on the index through the fluent builder and then stored in the index configuration:</p> <ul> <li>API: <code>segmentindex/IndexConfigurationBuilder</code></li> <li><code>addEncodingFilter(ChunkFilter)</code> / <code>addEncodingFilter(Class&lt;? extends ChunkFilter&gt;)</code></li> <li><code>addDecodingFilter(ChunkFilter)</code> / <code>addDecodingFilter(Class&lt;? extends ChunkFilter&gt;)</code></li> <li><code>withEncodingFilters(Collection&lt;ChunkFilter&gt;)</code></li> <li><code>withDecodingFilters(Collection&lt;ChunkFilter&gt;)</code></li> <li>Defaults (when you don\u2019t specify any):</li> <li>Encoding: CRC32 writing \u2192 Magic number writing</li> <li>Decoding: Magic number validation \u2192 CRC32 validation</li> </ul> <p>The filter sequences are propagated into segment I/O via <code>SegmentFiles</code>, used by: - Writer side: <code>ChunkStoreWriterTx</code> \u2192 <code>ChunkStoreWriterImpl</code> - Reader side: <code>ChunkStoreReaderImpl</code></p> <p>Constraints: - Filter lists must not be empty; <code>ChunkProcessor</code> enforces this. - Decoding order must mirror the inverse of encoding for transforms like compression/encryption. If you enable Snappy or XOR, include the matching decode filters in the correct order.</p>"},{"location":"architecture/general/filters/#error-handling-and-safety","title":"\ud83d\udee1\ufe0f Error Handling and Safety","text":"<ul> <li>Validation failures (wrong magic, CRC mismatch, missing flags) throw exceptions and abort the read; no partial state is committed.</li> <li>Writes are protected by transactional temp\u2011file + atomic rename; a failed write never exposes a partially written chunk to readers.</li> </ul>"},{"location":"architecture/general/filters/#examples","title":"\ud83d\udccb Examples","text":"<p>Enable Snappy compression with correct decode order:</p> <pre><code>IndexConfiguration&lt;Integer, String&gt; conf = IndexConfiguration.&lt;Integer, String&gt;builder()\n    // ... types and other settings ...\n    .addEncodingFilter(new ChunkFilterCrc32Writing())\n    .addEncodingFilter(new ChunkFilterMagicNumberWriting())\n    .addEncodingFilter(new ChunkFilterSnappyCompress())\n    .addDecodingFilter(new ChunkFilterMagicNumberValidation())\n    .addDecodingFilter(new ChunkFilterSnappyDecompress())\n    .addDecodingFilter(new ChunkFilterCrc32Validation())\n    .build();\n</code></pre> <p>Add XOR obfuscation on top of compression:</p> <pre><code>builder\n  .addEncodingFilter(new ChunkFilterXorEncrypt())\n  .addDecodingFilter(new ChunkFilterXorDecrypt());\n</code></pre>"},{"location":"architecture/general/filters/#code-pointers","title":"\ud83e\udde9 Code Pointers","text":"<ul> <li>Pipeline engine: <code>src/main/java/org/hestiastore/index/chunkstore/ChunkProcessor.java</code></li> <li>Filters: <code>src/main/java/org/hestiastore/index/chunkstore/ChunkFilter*.java</code></li> <li>Writer path: <code>src/main/java/org/hestiastore/index/chunkstore/ChunkStoreWriterImpl.java</code></li> <li>Reader path: <code>src/main/java/org/hestiastore/index/chunkstore/ChunkStoreReaderImpl.java</code></li> <li>Configuration defaults: <code>src/main/java/org/hestiastore/index/segmentindex/IndexConfigurationContract.java</code></li> </ul>"},{"location":"architecture/general/filters/#related-glossary","title":"\ud83d\udd17 Related Glossary","text":"<ul> <li>Filters</li> <li>Chunk</li> <li>Main SST</li> </ul>"},{"location":"architecture/general/glossary/","title":"\ud83d\udcd6 Glossary","text":"<p>Concise definitions of terms used across HestiaStore\u2019s architecture, with links and code pointers.</p>"},{"location":"architecture/general/glossary/#segment","title":"\ud83e\uddf1 Segment","text":"<p>Bounded shard of the index stored on disk with its own files: main SST (<code>vNN-index.sst</code>), sparse index (<code>vNN-scarce.sst</code>), Bloom filter (<code>vNN-bloom-filter.bin</code>), <code>manifest.txt</code>, optional delta caches (<code>vNN-delta-NNNN.cache</code>), and <code>.lock</code>. See also: On\u2011Disk Layout. Code: <code>segment/*</code>, <code>segmentindex/SegmentRegistry.java</code>.</p>"},{"location":"architecture/general/glossary/#segmentid","title":"\ud83c\udff7\ufe0f SegmentId","text":"<p>Stable integer id rendered as <code>segment-00000</code>, used to name per\u2011segment files. Code: <code>segment/SegmentId.java</code>.</p>"},{"location":"architecture/general/glossary/#key-to-segment-map","title":"\ud83d\uddfa\ufe0f Key-to-Segment Map","text":"<p>Global sorted map of max key \u2192 SegmentId that routes lookups and flushes. Persisted as <code>index.map</code>. Code: <code>segmentindex/KeyToSegmentMap.java</code>.</p>"},{"location":"architecture/general/glossary/#main-sst","title":"\ud83d\udcda Main SST","text":"<p>On\u2011disk, chunked Sorted String Table containing sorted key/value entries for a segment. Code: <code>chunkentryfile/*</code>, <code>chunkstore/*</code>.</p>"},{"location":"architecture/general/glossary/#chunk","title":"\ud83d\udce6 Chunk","text":"<p>Fixed\u2011cell payload plus a small header (magic, version, payload length, CRC, flags). Filters may transform payload on write and are inverted on read. Code: <code>chunkstore/Chunk*.java</code>.</p>"},{"location":"architecture/general/glossary/#delta-cache","title":"\ud83e\uddee Delta Cache","text":"<p>Per\u2011segment overlay of recent updates, materialized as sorted <code>.cache</code> files and an in\u2011memory <code>UniqueCache</code> when loaded. Code: <code>segment/SegmentDeltaCache*.java</code>.</p>"},{"location":"architecture/general/glossary/#uniquecache","title":"\ud83e\uddf0 UniqueCache","text":"<p>In\u2011memory map that keeps only the latest value per key. Used at the index\u2011level write buffer and inside the delta overlay. Code: <code>cache/UniqueCache*.java</code>.</p>"},{"location":"architecture/general/glossary/#flush","title":"\ud83d\udebf Flush","text":"<p>Drains the index\u2011level write buffer, routes entries to per\u2011segment delta caches, and updates <code>index.map</code>. <code>flush()</code> schedules the work; <code>flushAndWait()</code> waits for completion. Code: <code>segmentindex/SegmentIndexImpl#flush()</code>, <code>segmentindex/CompactSupport.java</code>.</p>"},{"location":"architecture/general/glossary/#compaction","title":"\ud83e\uddf9 Compaction","text":"<p>Segment rewrite that merges main SST with delta caches into fresh <code>vNN-index.sst</code>, <code>vNN-scarce.sst</code>, and <code>vNN-bloom-filter.bin</code> files, then clears delta caches. Code: <code>segment/SegmentCompacter.java</code>, <code>segment/SegmentFullWriter*.java</code>.</p>"},{"location":"architecture/general/glossary/#split","title":"\u2702\ufe0f Split","text":"<p>When a segment grows beyond <code>maxNumberOfKeysInSegment</code>, it is split into two; <code>index.map</code> is updated with a new max key and SegmentId. Code: <code>segmentindex/SegmentSplitCoordinator.java</code>, <code>segment/SegmentSplitter*.java</code>.</p>"},{"location":"architecture/general/glossary/#sparse-index-scarce-index","title":"\ud83d\udd0d Sparse Index (Scarce Index)","text":"<p>Per\u2011segment, sorted sample of keys that points to chunk start positions in the main SST to bound local scans. Code: <code>scarceindex/*</code>.</p>"},{"location":"architecture/general/glossary/#bloom-filter","title":"\ud83c\udf38 Bloom Filter","text":"<p>Per\u2011segment probabilistic set that quickly proves absence and reduces on\u2011disk probes; rebuilt during compaction. Code: <code>bloomfilter/*</code>.</p>"},{"location":"architecture/general/glossary/#filters-chunk-filters","title":"\ud83e\uddea Filters (Chunk Filters)","text":"<p>Pluggable transformations applied to chunk payloads on write and inverted on read (magic number, CRC32, Snappy, XOR). Configured per index. Code: <code>chunkstore/ChunkFilter*.java</code>; config via <code>segmentindex/IndexConfigurationBuilder</code>.</p>"},{"location":"architecture/general/glossary/#tombstone","title":"\ud83e\udea6 Tombstone","text":"<p>Special value denoting deletion; read path treats it as absent and compaction drops obsolete values. Provided by the value type descriptor. Code: <code>datatype/TypeDescriptor#getTombstone()</code>, used in <code>segmentindex/SegmentIndexImpl#delete()</code>.</p>"},{"location":"architecture/general/glossary/#entry","title":"\ud83e\uddfe Entry","text":"<p>Immutable key/value pair used across iterators and writers. Code: <code>index/Entry.java</code>.</p>"},{"location":"architecture/general/glossary/#entryiterator","title":"\ud83d\udd01 EntryIterator","text":"<p>Forward iterator over entries; variants exist for merging overlays and for safe iteration under writes (optimistic lock). Code: <code>index/EntryIterator.java</code>, <code>segment/MergeDeltaCacheWithIndexIterator.java</code>, <code>index/EntryIteratorWithLock.java</code>.</p>"},{"location":"architecture/general/glossary/#write-transaction","title":"\u270d\ufe0f Write Transaction","text":"<p>Pattern that enforces open \u2192 close \u2192 commit, guaranteeing atomic file replacement. Code: <code>index/GuardedWriteTransaction.java</code>, <code>index/WriteTransaction.java</code>.</p>"},{"location":"architecture/general/glossary/#directory-abstraction","title":"\ud83d\udcc1 Directory (Abstraction)","text":"<p>File I/O backend (FS, memory, zip) providing readers/writers and atomic rename. Code: <code>directory/*</code>.</p>"},{"location":"architecture/general/glossary/#segmentdata-and-provider","title":"\ud83e\udde9 SegmentData and Provider","text":"<p>Lazy containers and providers for per\u2011segment heavyweight structures (delta cache, Bloom, sparse index). Often cached via LRU. Code: <code>segment/SegmentData*.java</code>, <code>segmentindex/SegmentDataCache.java</code>.</p>"},{"location":"architecture/general/glossary/#segmentwindow","title":"\ud83e\ude9f SegmentWindow","text":"<p>Offset/limit window for streaming across segments, analogous to SQL OFFSET/LIMIT. Code: <code>segmentindex/SegmentWindow.java</code>.</p>"},{"location":"architecture/general/glossary/#stats","title":"\ud83d\udcca Stats","text":"<p>Simple counters for get/put/delete to observe workload shape. Code: <code>segmentindex/Stats.java</code>.</p>"},{"location":"architecture/general/glossary/#consistency-checker","title":"\ud83e\uddf0 Consistency Checker","text":"<p>Utilities to verify sortedness and segment/map coherence after unexpected shutdowns; can repair certain metadata issues. Code: <code>segmentindex/IndexConsistencyChecker.java</code>, <code>segment/SegmentConsistencyChecker.java</code>.</p>"},{"location":"architecture/general/glossary/#logging-context","title":"\ud83d\uddd2\ufe0f Logging Context","text":"<p>Optional MDC enrichment that sets <code>index.name</code> for log correlation when enabled.</p>"},{"location":"architecture/general/limits/","title":"\ud83d\udea7 Limitations &amp; Trade\u2011offs","text":"<p>This page lists the most important constraints and design trade\u2011offs so you can plan deployments and avoid surprises. Each item links to the code or related docs for verification.</p>"},{"location":"architecture/general/limits/#durability-and-recovery","title":"\ud83d\udcbe Durability and Recovery","text":"<ul> <li>No WAL recovery: There is no write\u2011ahead log to replay after a crash. Durability boundaries are explicit <code>flushAndWait()</code> and <code>close()</code>. Writes still in the index write buffer (in\u2011memory <code>UniqueCache</code>) are not durable until flushed. See Recovery.</li> <li>Per\u2011file atomicity only: Writers use temp files + atomic rename; groups of files (e.g., SST + scarce index + bloom) commit in a safe order but not as a single atomic unit. Readers remain consistent because old files stay in place until each rename. See SegmentFullWriterTx, BloomFilterWriterTx.</li> <li>Filesystem requirement: Crash safety relies on same\u2011directory atomic <code>rename</code>. Use local filesystems; be cautious with network filesystems that may not guarantee strict atomicity.</li> <li>Stale lock files: A crash can leave <code>.lock</code> behind, preventing open until removed. See <code>directory/FsFileLock.java</code> and IndexState*. Remove the file only when certain no process still uses the directory.</li> </ul>"},{"location":"architecture/general/limits/#concurrency","title":"\ud83d\udd12 Concurrency","text":"<ul> <li>SegmentIndex is thread\u2011safe and not globally serialized; heavy concurrent writes can increase contention in shared caches and per-segment state machines.</li> <li>No serialized adapter is provided; enforce strict ordering externally if needed.</li> <li>Optimistic iteration: Segment iterators may stop early if a write bumps the version during a scan (by design). Re\u2011open the iterator to continue. See <code>EntryIteratorWithLock</code>.</li> </ul>"},{"location":"architecture/general/limits/#size-and-addressing-limits","title":"\ud83d\udccf Size and Addressing Limits","text":"<ul> <li>Per\u2011segment SST size bounded by 32\u2011bit positions: Sparse index stores an <code>Integer</code> position and readers cast to <code>int</code> (<code>ChunkEntryFile#openIteratorAtPosition((int)position)</code>). Keep a single <code>vNN-index.sst</code> file below ~2 GiB. Use multiple segments to scale. Code: <code>chunkentryfile/ChunkEntryFile.java</code>, <code>scarceindex/*</code>.</li> <li>Data\u2011block and cell sizing constraints: <code>diskIoBufferSize</code> must be divisible by 1024; chunk cell size is fixed at 16 bytes. Payloads pad to whole cells (space overhead). Code: <code>Vldtn#requireIoBufferSize</code>, <code>chunkstore/CellPosition.java</code>.</li> </ul>"},{"location":"architecture/general/limits/#configuration-immutability","title":"\ud83e\uddf1 Configuration Immutability","text":"<p>Once an index is created, several properties cannot be changed when reopening with a config:</p> <ul> <li>Type descriptors (key/value serialization)</li> <li>Sparse index cadence (<code>maxNumberOfKeysInSegmentChunk</code>)</li> <li>Segment sizing limits (e.g., <code>maxNumberOfKeysInSegment</code>)</li> <li>Bloom filter sizing and hash functions</li> <li>Encoding/decoding filter lists (order and membership)</li> </ul> <p>Attempts to change these raise an error in <code>IndexConfigurationManager.validateThatWasntChanged</code>. To change them, create a new index and bulk\u2011copy data (read + write) or export/import. See <code>segmentindex/IndexConfigurationManager.java</code>.</p>"},{"location":"architecture/general/limits/#data-model-and-semantics","title":"\ud83e\udde0 Data Model and Semantics","text":"<ul> <li>Strictly increasing keys: All on\u2011disk structures assume ascending order; compaction and consistency checks enforce it. Incorrect comparators or inconsistent key ordering will fail. Code: <code>segment/SegmentConsistencyChecker.java</code>.</li> <li>Tombstones: Deletes are tombstones until compaction merges and drops them. Heavy delete workloads without compaction may grow delta files and increase read work.</li> <li>No multi\u2011key transactions: Writes are per\u2011key; there is no cross\u2011key atomic batch. Use application\u2011level coordination if needed.</li> </ul>"},{"location":"architecture/general/limits/#security-posture","title":"\ud83d\udd10 Security Posture","text":"<ul> <li>XOR filter is not encryption: <code>ChunkFilterXorEncrypt</code> provides reversible obfuscation only; do not use as security. For encryption at rest, place HestiaStore on an encrypted volume or add a real crypto layer above. See <code>chunkstore/ChunkFilterXor*</code>.</li> <li>No authentication/authorization: HestiaStore is an embedded library and relies on your process/container isolation.</li> </ul>"},{"location":"architecture/general/limits/#workloads-that-fit-well","title":"\u2705 Workloads That Fit Well","text":"<ul> <li>High\u2011throughput append/update workloads where read\u2011after\u2011write visibility matters and periodic flush/compaction is acceptable.</li> <li>Point lookups and ordered scans with predictable latency (Bloom + sparse index bound I/O).</li> </ul>"},{"location":"architecture/general/limits/#antipatterns","title":"\ud83d\udeab Anti\u2011patterns","text":"<ul> <li>Expecting durability without flush/close.</li> <li>Relying on WAL replay (not implemented).</li> <li>Very large single segments (&gt;2 GiB <code>vNN-index.sst</code>); split into more segments.</li> <li>Heavy mixed concurrent reads/writes with strict low\u2011latency tail in synchronized mode (coarse locking).</li> </ul>"},{"location":"architecture/general/limits/#mitigations-and-best-practices","title":"\ud83d\udee0\ufe0f Mitigations and Best Practices","text":"<ul> <li>Plan periodic <code>flushAndWait()</code> and <code>compact()</code> windows; after crashes run consistency check and optionally compact.</li> <li>Size Bloom filters for your negative\u2011lookup rate; monitor <code>BloomFilterStats</code>.</li> <li>Tune <code>maxNumberOfKeysInSegmentChunk</code> to balance read scan length vs. sparse index size.</li> <li>Use multiple segments to stay under per\u2011segment limits and to improve compaction parallelism (future).</li> </ul>"},{"location":"architecture/general/limits/#related-docs","title":"\ud83d\udd17 Related Docs","text":"<ul> <li>Recovery: <code>architecture/general/recovery.md</code></li> <li>Concurrency: <code>architecture/general/concurrency.md</code></li> <li>Filters &amp; Integrity: <code>architecture/general/filters.md</code></li> <li>On\u2011Disk Layout: <code>architecture/segment/on-disk-layout.md</code></li> </ul>"},{"location":"architecture/general/recovery/","title":"\ud83d\udedf Consistency &amp; Recovery","text":"<p>This page explains HestiaStore\u2019s crash safety model and commit semantics. There is no WAL\u2011based crash recovery or multi\u2011operation transactions. Durability is driven by explicit flushes and by the fact that all data files are written via temporary files and atomically renamed on commit.</p>"},{"location":"architecture/general/recovery/#scope-and-guarantees","title":"\ud83d\udcdc Scope and Guarantees","text":"<ul> <li>No automatic recovery: the system does not replay a WAL or roll back partial groups of operations after a crash.</li> <li>Durability boundary: calling <code>flushAndWait()</code> (or closing the index) persists all writes that happened before the call. <code>flush()</code> only schedules maintenance; wait for completion if you need a durability guarantee.</li> <li>Atomic file replacement: data files are written to <code>*.tmp</code> and made visible via <code>rename</code> only after the writer is closed and the transaction is committed. A crash cannot produce partially written visible files.</li> </ul>"},{"location":"architecture/general/recovery/#where-writes-become-durable","title":"\ud83d\udcbe Where Writes Become Durable","text":"<ul> <li>Index\u2011level buffer \u2192 disk: <code>SegmentIndex.flush()</code> schedules draining of the in\u2011memory unique buffer into segment delta cache files. <code>flushAndWait()</code> (and close) wait for completion.</li> <li>Segment merge/compaction: when a segment compacts, the new main SST, sparse index, and Bloom filter are built via transactional writers; on commit they atomically replace the old ones.</li> <li>Key\u2192segment map (<code>index.map</code>): persisted via a transactional sorted data writer during flush or when updated.</li> </ul> <p>Relevant code: <code>segmentindex/SegmentIndexImpl#flush()</code>, <code>segmentindex/CompactSupport</code>, <code>segmentindex/KeyToSegmentMap#optionalyFlush()</code>.</p>"},{"location":"architecture/general/recovery/#transactional-write-primitives","title":"\u270d\ufe0f Transactional Write Primitives","text":"<p>All main data files follow the same pattern: write to a temporary file, then atomically rename on <code>commit()</code>.</p> <ul> <li>Guarded transactions: <code>GuardedWriteTransaction</code> requires the resource to be closed before <code>commit()</code> and prevents double\u2011commit.</li> <li>Single\u2011call helper: <code>WriteTransaction.execute(writer -&gt; { \u2026 })</code> does open \u2192 write \u2192 close \u2192 commit.</li> </ul> <p>Key classes: - <code>unsorteddatafile/UnsortedDataFileWriterTx</code> \u2192 <code>rename(temp, final)</code> on commit - <code>sorteddatafile/SortedDataFileWriterTx</code> \u2192 <code>rename(temp, final)</code> on commit - <code>datablockfile/DataBlockWriterTx</code> \u2192 used by chunk store writers - <code>chunkstore/ChunkStoreWriterTx</code> and <code>chunkentryfile/ChunkEntryFileWriterTx</code> \u2192 layered over <code>DataBlockWriterTx</code> - <code>bloomfilter/BloomFilterWriterTx</code> \u2192 writes new filter and swaps it in on commit</p>"},{"location":"architecture/general/recovery/#file-types-and-commit-paths","title":"\ud83d\uddc2\ufe0f File Types and Commit Paths","text":"<ul> <li>Segment delta cache files</li> <li>Writer: <code>segment/SegmentDeltaCacheWriter</code></li> <li>Mechanism: <code>SortedDataFileWriterTx.execute(\u2026)</code></li> <li> <p>Naming: manifest counter assigns <code>vNN-delta-NNNN.cache</code> before write; if a crash happens before commit, the reader treats missing files as empty, so boot remains safe.</p> </li> <li> <p>Main SST (chunked) + sparse index (\"scarce index\")</p> </li> <li>Writers: <code>segment/SegmentFullWriterTx</code> and <code>segment/SegmentFullWriter</code></li> <li>Internals: <code>ChunkEntryFileWriterTx</code> for SST, <code>ScarceIndexWriterTx</code> for the sparse index</li> <li> <p>Bloom filter: <code>BloomFilterWriterTx</code> builds a new filter and commits (rename) before the SST and sparse index are committed. This ordering avoids false negatives on restart.</p> </li> <li> <p>Bloom filter</p> </li> <li> <p>Writes to a temporary file via <code>BloomFilterWriterTx.open()</code> and commits with <code>rename</code>; also updates the in\u2011memory hash snapshot on commit.</p> </li> <li> <p>Key\u2192segment map (<code>index.map</code>)</p> </li> <li>Writer: <code>SortedDataFileWriterTx.execute(\u2026)</code> inside <code>KeyToSegmentMap.optionalyFlush()</code></li> <li>Ensures the map is replaced atomically.</li> </ul>"},{"location":"architecture/general/recovery/#what-is-not-transactional","title":"\ud83d\udeab What Is Not Transactional","text":"<ul> <li>Segment manifest metadata (counts and delta\u2011file numbering) is persisted via an overwrite (<code>Directory.Access.OVERWRITE</code>). It is updated after data files are committed, and is not critical to data correctness. If a crash corrupts or desynchronizes this metadata, the reader logic remains safe (e.g., missing delta file names yield empty reads) and you can re\u2011establish consistency via the checker below.</li> </ul> <p>Code: <code>properties/PropertyStoreimpl</code> and <code>SegmentPropertiesManager</code>.</p>"},{"location":"architecture/general/recovery/#failure-model-examples","title":"\ud83d\udca5 Failure Model (Examples)","text":"<ul> <li>Crash while writing a delta file before commit: only <code>*.tmp</code> exists; it is ignored on boot; prior state remains valid.</li> <li>Crash after committing a Bloom filter but before committing SST/sparse index: Bloom filter is ahead of data, which is safe (may increase positives but never produce false negatives).</li> <li>Crash after committing SST/sparse index but before properties update: data is fully committed; metadata may lag but does not affect correctness.</li> </ul>"},{"location":"architecture/general/recovery/#consistency-check-and-repair","title":"\ud83e\uddf0 Consistency Check and Repair","text":"<ul> <li>Run <code>SegmentIndex.checkAndRepairConsistency()</code> after an unexpected shutdown to verify that segments are well\u2011formed and sorted and that the key\u2192segment map is coherent. This walks all segments, checks ordering and basic invariants, and raises an error if it finds non\u2011recoverable issues.</li> </ul> <p>Key classes: <code>segmentindex/IndexConsistencyChecker</code>, <code>segment/SegmentConsistencyChecker</code>.</p>"},{"location":"architecture/general/recovery/#developer-notes-opencommit-and-tmp","title":"\ud83d\udc69\u200d\ud83d\udcbb Developer Notes: <code>open()</code>/<code>commit()</code> and <code>*.tmp</code>","text":"<ul> <li><code>open()</code> returns a writer bound to a temporary file (typically with a <code>.tmp</code> suffix). You must close the writer before calling <code>commit()</code>.</li> <li><code>commit()</code> performs an atomic <code>rename(temp, final)</code> so either the old file or the new file is visible on disk.</li> <li>Prefer <code>execute(writer -&gt; {\u2026})</code> to ensure the correct order: open \u2192 write \u2192 close \u2192 commit.</li> </ul> <p>Examples in code: - <code>sorteddatafile/SortedDataFileWriterTx#open()</code> \u2192 <code>commit()</code> renames temp to final - <code>unsorteddatafile/UnsortedDataFileWriterTx#open()</code> \u2192 <code>commit()</code> renames temp to final - <code>datablockfile/DataBlockWriterTx#open()</code> \u2192 <code>commit()</code> renames temp to final - <code>bloomfilter/BloomFilterWriterTx#open()</code> \u2192 <code>commit()</code> renames temp to final and swaps hash</p>"},{"location":"architecture/general/recovery/#practical-guidance","title":"\ud83e\udded Practical Guidance","text":"<ul> <li>Call <code>flushAndWait()</code> on periodic boundaries and always before shutdown to persist in\u2011memory writes.</li> <li>After a crash, reopen the index and run <code>checkAndRepairConsistency()</code>; optionally trigger a <code>compact()</code> to collapse delta caches.</li> <li>Remember there is no WAL: durability is guaranteed at the <code>flushAndWait()</code>/close boundaries and via atomic file replacement for all data files.</li> </ul>"},{"location":"architecture/general/recovery/#related-glossary","title":"\ud83d\udd17 Related Glossary","text":"<ul> <li>Flush</li> <li>Write Transaction</li> <li>Compaction</li> <li>Consistency Checker</li> </ul>"},{"location":"architecture/registry/","title":"\ud83d\uddc2\ufe0f Registry Architecture","text":"<p>This section describes the segment registry as the lifecycle/cache orchestration layer between SegmentIndex and Segment instances.</p>"},{"location":"architecture/registry/#topics","title":"Topics","text":"<ul> <li>Segment Registry \u2014 responsibilities, state machine,   per-entry model, and operation semantics.</li> </ul>"},{"location":"architecture/registry/#diagrams","title":"Diagrams","text":"<ul> <li>Registry states</li> <li>Get: cache hit READY</li> <li>Get: cache hit LOADING wait</li> <li>Get: cache miss with <code>putIfAbsent(LOADING)</code></li> <li>Eviction: <code>removeLastRecentUsedSegment(...)</code></li> </ul>"},{"location":"architecture/registry/registry/","title":"Segment Registry","text":"<p>This document describes the segment registry responsibilities and supported operations. </p>"},{"location":"architecture/registry/registry/#scope","title":"Scope","text":"<ul> <li>The registry owns:<ul> <li>safe access to segment resources (load/create/delete)</li> <li>in-memory segment cache (LRU)</li> <li>registry-level state gate (<code>READY</code>, <code>CLOSED</code>, <code>ERROR</code>, <code>FREEZE</code>)</li> <li>segment id allocation for new segments via <code>SegmentIdAllocator</code></li> </ul> </li> <li>The registry does not own protection of \"segment in use\" vs \"segment close/delete\" races.   This responsibility belongs to the Segment package. Segment implementations   must remain safe when one thread uses a segment while another thread closes it.</li> <li>The registry does not own split execution, scheduling, or in-flight   tracking. Those belong to the segment index layer.</li> <li>The registry is about safe access to segment resources; it should not manage   operations on those resources (flush/compact/split remain outside).</li> <li>Segment load/open failures are status-driven:   <code>getSegment()</code> and <code>createSegment()</code> return <code>ERROR</code> when loading/opening   fails (including missing segment files), with no dedicated registry-status exception type.</li> </ul>"},{"location":"architecture/registry/registry/#registry-state-machine","title":"Registry State Machine","text":"<p>Registry starts in <code>FREEZE</code> during bootstrap and transitions to <code>READY</code> when startup completes.</p> <p></p>"},{"location":"architecture/registry/registry/#transitions","title":"Transitions","text":"Original State New State When <code>READY</code> <code>FREEZE</code> is part of <code>close()</code> procedure <code>FREEZE</code> <code>READY</code> happens after starting procedure <code>FREEZE</code> <code>CLOSED</code> index closing while frozen (<code>close()</code>) any <code>ERROR</code> unrecoverable registry failure (<code>fail()</code>)"},{"location":"architecture/registry/registry/#rules","title":"Rules","text":"<ul> <li>Request operations (<code>getSegment()</code>, <code>allocateSegmentId()</code>, <code>createSegment()</code>,   <code>deleteSegment()</code>) are state-gated:   <code>READY</code> -&gt; normal flow, <code>FREEZE</code> -&gt; <code>BUSY</code>, <code>CLOSED</code> -&gt; <code>CLOSED</code>,   <code>ERROR</code> -&gt; <code>ERROR</code>.</li> <li>In <code>READY</code>, <code>getSegment()</code> and <code>deleteSegment()</code> can still return <code>BUSY</code>   on cache entry state conflict.</li> <li><code>close()</code> is idempotent and moves <code>READY</code> to <code>FREEZE</code> and than to  <code>CLOSED</code>.</li> <li><code>ERROR</code> is terminal for the state machine; <code>close()</code> does not transition   <code>ERROR</code> to <code>CLOSED</code>.</li> </ul>"},{"location":"architecture/registry/registry/#registry-operations","title":"Registry Operations","text":"Operation Description <code>getSegment(id)</code> Load or return cached segment by id (<code>SegmentRegistryResult&lt;Segment&gt;</code>). <code>allocateSegmentId()</code> Allocate a new segment id for split or growth (<code>SegmentRegistryResult&lt;SegmentId&gt;</code>). <code>createSegment()</code> Allocate id and create a new segment (<code>SegmentRegistryResult&lt;Segment&gt;</code>). <code>deleteSegment(id)</code> Close and delete a segment, then remove from cache (<code>SegmentRegistryResult&lt;Void&gt;</code>). <code>close()</code> Close cached segments (<code>SegmentRegistryResult&lt;Void&gt;</code>). <p>All registry operations return <code>SegmentRegistryResult&lt;T&gt;</code> (status + optional value). Registry BUSY/CLOSED/ERROR outcomes are propagated by <code>SegmentRegistryResultStatus</code>. The primary safety model is the registry state gate + per-key cache entry state machine, not caller-side pinning.</p>"},{"location":"architecture/registry/registry/#response-codes","title":"Response Codes","text":"<p><code>SegmentRegistryResultStatus</code> is carried by <code>SegmentRegistryResult&lt;T&gt;</code> with semantics:</p> Code Description <code>OK</code> Segment returned or operation accepted. <code>BUSY</code> Temporary refusal (cache entry state conflict, <code>UNLOADING</code>, or registry is <code>FREEZE</code>). <code>CLOSED</code> Registry closed; no further operations. <code>ERROR</code> Unrecoverable registry failure."},{"location":"architecture/registry/registry/#registry-cache-entry","title":"Registry cache Entry","text":"<p>This section describes the target cache-entry model planned for implementation.</p>"},{"location":"architecture/registry/registry/#entry-operations","title":"Entry operations","text":"Operation Entry state precondition Outcome Used by <code>tryStartLoad()</code> <code>MISSING</code> Attempts to start load for the key by transitioning to <code>LOADING</code> for the winning caller. Returns not-started when another entry already exists. <code>SegmentRegistryCache.get()</code> miss path <code>waitWhileLoading(currentAccessCx)</code> Any Waits while <code>LOADING</code> Returns value in <code>READY</code>, any exceptions in propagated. up <code>SegmentRegistryCache.get()</code> <code>finishLoad(value)</code> <code>LOADING</code> Stores value, transitions to <code>READY</code>, signals waiters. cache load winner <code>fail(exception)</code> <code>LOADING</code> Stores failure, marks entry failed/unloading path, signals waiters; map entry is removed by cache loader error path. cache loader error path <code>tryStartUnload()</code> <code>READY</code> with value Attempts atomic transition to <code>UNLOADING</code>. Returns <code>true</code> when unload was started, <code>false</code> otherwise. eviction and <code>invalidate()</code> <code>finishUnload()</code> <code>UNLOADING</code> Clears value and signals waiters; entry is then treated as missing by readers. eviction and <code>invalidate()</code> finalization <code>getEvictionOrder()</code> Any Returns LRU order only for unloadable <code>READY</code> entry; otherwise returns sentinel (<code>Long.MAX_VALUE</code>). LRU candidate selection <p><code>waitWhileLoading(currentAccessCx)</code> blocks only while the entry is in <code>LOADING</code>. It does not wait for <code>UNLOADING</code>; that case must be handled by higher-level caller logic, because an unloaded entry is no longer valid.</p>"},{"location":"architecture/registry/registry/#per-key-entry-state-machine","title":"Per-key <code>Entry</code> state machine","text":"<p>Each cache key owns one <code>Entry</code> object with an independent lock/condition. Only threads touching the same key can block each other.</p>"},{"location":"architecture/registry/registry/#states","title":"States","text":"State Description <code>LOADING</code> Entry was atomically installed with <code>putIfAbsent(key, Entry{LOADING})</code>; winner thread loads value. <code>READY</code> Value is available; <code>get(key)</code> returns the value immediately and updates recency. <code>UNLOADING</code> Value is being closed/unloaded and entry is no longer usable for normal reads. <p><code>MISSING</code> is a virtual state: it means no entry was found in cache for the key yet, but an entry can still be instantiated by the loading path.</p>"},{"location":"architecture/registry/registry/#transitions_1","title":"Transitions","text":"From To Trigger <code>MISSING</code> <code>LOADING</code> Winning caller starts load (<code>tryStartLoad()</code>). <code>LOADING</code> <code>READY</code> Loader completes successfully and signals waiters. <code>LOADING</code> <code>MISSING</code> Loader fails; entry is removed and waiters are signaled with failure. <code>READY</code> <code>UNLOADING</code> Eviction/delete starts unload (<code>tryStartUnload()</code>). <code>UNLOADING</code> <code>MISSING</code> Unloader completes and entry is removed from cache."},{"location":"architecture/registry/registry/#guarantees","title":"Guarantees","text":"<ul> <li>At most one loader runs per key.</li> <li>Wait/notify is per key (<code>Entry</code>), not global.</li> <li>Keys with different <code>Entry</code> instances progress independently.</li> <li>Unload is started by direct per-entry transition (<code>READY -&gt; UNLOADING</code>), not   by reference pin counting.</li> </ul>"},{"location":"architecture/registry/registry/#thread-model","title":"Thread model","text":""},{"location":"architecture/registry/registry/#1-registry-getid-with-cached","title":"1. Registry <code>get(id)</code> with cached","text":""},{"location":"architecture/registry/registry/#2-registry-getid-with-cached-entry-in-loading-state","title":"2. Registry get(id) with cached entry in LOADING state","text":"<p>When the registry entry exists in cache but is still <code>LOADING</code>, the cache waits until loading finishes before returning it. When the entry is <code>UNLOADING</code>, the registry treats it as temporarily unavailable and returns <code>BUSY</code>. The flow is shown below:</p>"},{"location":"architecture/registry/registry/#3-registry-getid-cache-miss-with-putifabsentloading","title":"3. Registry <code>get(id)</code> cache miss with <code>putIfAbsent(LOADING)</code>","text":"<p>Please note that segment is loaded in callers thread.</p>"},{"location":"architecture/registry/registry/#4-cache-method-removelastrecentusedsegment","title":"4. Cache method <code>removeLastRecentUsedSegment()</code>","text":"<p>The diagram shows only the case where <code>segment.close()</code> succeeds. If <code>segment.close()</code> fails with exeception than the entry remains in <code>UNLOADING</code>. Which is fine.</p>"},{"location":"architecture/registry/registry/#deletesegmentid-flow","title":"<code>deleteSegment(id)</code> flow","text":"<ol> <li>Try to transition the cached entry to <code>UNLOADING</code>; return <code>BUSY</code> when the entry is not unloadable.</li> <li>Close the segment with retry/backoff until it is <code>CLOSED</code> or returns <code>OK</code>.</li> <li>Delete the segment directory and files on disk.</li> <li>Remove the unloaded entry from cache memory.</li> </ol> <p>When the segment is not cached, deletion is best\u2011effort and only touches disk.</p>"},{"location":"architecture/segment/","title":"\ud83e\uddf1 Segment Architecture","text":"<p>This is the central place for segment-specific internals.</p>"},{"location":"architecture/segment/#what-a-segment-contains","title":"What a Segment Contains","text":"<ul> <li>Main SST file: <code>vNN-index.sst</code></li> <li>Sparse (scarce) index: <code>vNN-scarce.sst</code></li> <li>Bloom filter: <code>vNN-bloom-filter.bin</code></li> <li>Manifest: <code>manifest.txt</code></li> <li>Lock file: <code>.lock</code></li> <li>Delta cache files: <code>vNN-delta-NNNN.cache</code></li> </ul>"},{"location":"architecture/segment/#core-segment-structures","title":"Core Segment Structures","text":"<ul> <li>Delta cache: in-memory and on-disk overlay for recent updates.</li> <li>Bloom filter: fast negative lookup guard before on-disk scans.</li> <li>Sparse/scarce index: maps sampled keys to chunk positions in main SST.</li> </ul>"},{"location":"architecture/segment/#topics","title":"Topics","text":"<ul> <li>Segment Design \u2014 segment behavior and implementation notes.</li> <li>On-Disk Layout &amp; File Names \u2014 naming, directory   examples, and transactional file writes.</li> <li>Sparse Index \u2014 sparse/scarce index structure and lookup   role.</li> <li>Segment Concurrency \u2014 state machine and   operation gating.</li> </ul>"},{"location":"architecture/segment/#diagrams","title":"Diagrams","text":"<ul> <li>Segment state machine source</li> <li>Segment write sequence source</li> </ul>"},{"location":"architecture/segment/arch-index/","title":"\ud83e\udded Architecture","text":"<p>Here is described basic index concepts. This page explain you how to correctly configure index.</p> <p></p>"},{"location":"architecture/segment/arch-index/#operation-consistency","title":"\ud83e\udde9 Operation consistency","text":"<p>The <code>getStream()</code> method can sometimes return inconsistent results, occasionally omitting some items. This can occur in the following scenarios:</p> <ul> <li>Segment Compaction: If data is being streamed from a segment and new keys are added to that segment during the process, the segment may stop providing additional keys. In this case, the stream operation will either continue with the next segment or terminate if no more segments are available.</li> <li>Adding New Keys: If a completely new key is added to the index and is only present in the main index cache, it will not be returned.</li> </ul> <p>To prevent these issues, you should call <code>compact()</code> before invoking <code>getStream()</code> and ensure no new keys are added during streaming.</p> <p>Updating values in the index using <code>put()</code> or deleting keys using <code>delete()</code> does not cause inconsistencies. Updated values will be returned, and deleted keys will be excluded from the stream.</p> <p>Other operations, like <code>get()</code>, remain consistently reliable.</p>"},{"location":"architecture/segment/arch-index/#states","title":"\ud83d\udcca States","text":"<p>Index could be in following states:</p> <p></p> <p></p> <p>Interruption of process of writing data to index could lead to corruption of entire index.</p>"},{"location":"architecture/segment/on-disk-layout/","title":"\ud83d\udcbe On-Disk Layout &amp; File Names","text":"<p>This page documents the files HestiaStore writes into an index directory, their naming conventions, how they evolve over time, and the atomic commit pattern used to keep them consistent.</p>"},{"location":"architecture/segment/on-disk-layout/#directory-layout-one-segmentindex-per-directory","title":"\ud83d\udcc2 Directory Layout (One SegmentIndex per Directory)","text":"<p>Top-level files:</p> <ul> <li><code>index.map</code> \u2014 Global key\u2192segment map (max key per segment). Sorted key\u2192SegmentId pairs. Updated atomically.</li> </ul> <p>Top-level directories:</p> <ul> <li><code>segment-00000/</code>, <code>segment-00001/</code>, \u2026 \u2014 one directory per segment id.</li> </ul> <p>Per\u2011segment files (inside <code>segment-00000/</code>):</p> <ul> <li><code>manifest.txt</code> \u2014 Segment metadata (counts, active version, delta numbering).</li> <li><code>.lock</code> \u2014 Segment lock file.</li> <li><code>v01-index.sst</code> \u2014 Main SST in chunked format (ChunkStoreFile). Holds sorted key/value entries in chunks.</li> <li><code>v01-scarce.sst</code> \u2014 Sparse index (key\u2192chunk start position) to accelerate probes into the main SST.</li> <li><code>v01-bloom-filter.bin</code> \u2014 Bloom filter backing store for negative lookups.</li> <li><code>v01-delta-0000.cache</code>, <code>v01-delta-0001.cache</code>, \u2026 \u2014 Per\u2011segment delta cache files created between compactions.</li> </ul> <p>Notes:</p> <ul> <li>Segment ids are zero\u2011based and padded: <code>segment-00000</code>, <code>segment-00001</code>, \u2026</li> <li>Versions are zero\u2011padded to 2 digits: <code>v01</code>, <code>v02</code>, \u2026</li> <li>Delta file counters are padded to 4 digits: <code>0000</code>, <code>0001</code>, \u2026</li> </ul>"},{"location":"architecture/segment/on-disk-layout/#naming-and-extensions","title":"\ud83c\udff7\ufe0f Naming and Extensions","text":"<ul> <li>Main data: <code>vNN-index.sst</code> (chunked SST)</li> <li>Sparse index: <code>vNN-scarce.sst</code> (sorted key\u2192int pointer)</li> <li>Bloom: <code>vNN-bloom-filter.bin</code></li> <li>Segment metadata: <code>manifest.txt</code></li> <li>Segment lock: <code>.lock</code></li> <li>Delta/overlay: <code>vNN-delta-NNNN.cache</code></li> <li>Key\u2192segment map: <code>index.map</code></li> </ul> <p>Code: <code>segment/SegmentFiles.java</code>, <code>segmentindex/KeyToSegmentMap.java</code>.</p>"},{"location":"architecture/segment/on-disk-layout/#atomic-commit-pattern-tmp-rename","title":"\ud83e\udde8 Atomic Commit Pattern (<code>*.tmp</code> + rename)","text":"<p>All persistent writers follow the same pattern:</p> <p>1) <code>openWriter()</code> returns a writer bound to a temporary file (usually <code>*.tmp</code>). 2) Close the writer to flush OS buffers. 3) <code>commit()</code> atomically renames the temp file to its final name.</p> <p>Implications:</p> <ul> <li>A crash never exposes a partially written visible file. At restart, either the old file or the new file is present.</li> <li>Readers treat missing files as empty where applicable (e.g., no delta files \u21d2 empty overlay).</li> </ul> <p>Code pointers:</p> <ul> <li>Delta cache: <code>sorteddatafile/SortedDataFileWriterTx</code> (used by <code>SegmentDeltaCacheWriter</code>)</li> <li>Main SST: <code>chunkentryfile/ChunkEntryFileWriterTx</code> \u2192 <code>chunkstore/ChunkStoreWriterTx</code> \u2192 <code>datablockfile/DataBlockWriterTx</code></li> <li>Sparse index: <code>scarceindex/ScarceIndexWriterTx</code></li> <li>Bloom filter: <code>bloomfilter/BloomFilterWriterTx</code></li> </ul>"},{"location":"architecture/segment/on-disk-layout/#segment-lifecycle","title":"\ud83d\udd04 Segment Lifecycle","text":"<p>1) New writes accumulate in the index write buffer; on flush they are routed by key into per\u2011segment delta files <code>vNN-delta-NNNN.cache</code>. 2) Reads consult delta cache first, then <code>vNN-bloom-filter.bin</code> and <code>vNN-scarce.sst</code> to bound the probe into <code>vNN-index.sst</code>. 3) Compaction rewrites <code>vNN-index.sst</code>, <code>vNN-scarce.sst</code>, and <code>vNN-bloom-filter.bin</code> transactionally; on success, delta files are deleted and the in\u2011memory delta cache is cleared. 4) When a segment grows beyond the threshold, it is split: a new <code>segment-xxxxx</code> appears and <code>index.map</code> is updated atomically.</p>"},{"location":"architecture/segment/on-disk-layout/#chunked-sst-anatomy","title":"\ud83e\uddec Chunked SST Anatomy","text":"<p>The <code>vNN-index.sst</code> file is a sequence of fixed\u2011cell chunks stored in a data\u2011block file. Each chunk has:</p> <ul> <li>Header: magic number, version, payload length, CRC32, flags</li> <li>Payload: a batch of sorted entries, optionally transformed by filters</li> </ul> <p>Filters add robustness and optional compression/obfuscation; their flags and order are recorded so the reader can invert them. See \u201cFilters &amp; Integrity\u201d.</p> <p>Code: <code>chunkstore/*</code>, <code>chunkentryfile/*</code>.</p>"},{"location":"architecture/segment/on-disk-layout/#compatibility","title":"\ud83d\udd01 Compatibility","text":"<ul> <li>Header fields (magic, version) allow future readers to validate format.</li> <li>Sparse index and Bloom filter are rebuilt during compaction; no upgrade step is required beyond re\u2011writing segments if formats change in the future.</li> </ul>"},{"location":"architecture/segment/on-disk-layout/#example-directory-minimal","title":"\ud83d\udcc1 Example Directory (minimal)","text":"<pre><code>index.map\nsegment-00000/\n  manifest.txt\n  .lock\n  v01-index.sst\n  v01-scarce.sst\n  v01-bloom-filter.bin\n  v01-delta-0000.cache   # present until compaction\n</code></pre>"},{"location":"architecture/segment/on-disk-layout/#related-glossary","title":"\ud83d\udd17 Related Glossary","text":"<ul> <li>SegmentId</li> <li>Main SST</li> <li>Sparse Index</li> <li>Bloom Filter</li> <li>Key-to-Segment Map</li> <li>Delta Cache</li> <li>Write Transaction</li> </ul>"},{"location":"architecture/segment/segment-concurrency/","title":"Concurrency &amp; Lifecycle","text":""},{"location":"architecture/segment/segment-concurrency/#glossary","title":"Glossary","text":"<ul> <li>Published view: immutable view of the main index, delta cache, bloom filter, and sparse index.</li> <li>Write cache: mutable map of recent writes not yet published.</li> <li>Segment version: monotonic epoch counter used by optimistic iterators.</li> <li>Maintenance thread: dedicated background thread for disk IO.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#segment-operations","title":"Segment operations","text":"<p>Please look at main operations supported by segment. Other operations are from thread-safety not interesting.</p> Operation Description <code>Segment.builder(directory)</code> Create a builder for constructing a segment with the provided directory. <code>compact()</code> Start compaction; returns once accepted. <code>openIterator(isolation)</code> Open an iterator with the requested isolation level. <code>put(key, value)</code> Write into the in-memory write cache. <code>flush()</code> Flush write cache to delta cache; returns once accepted. <code>get(key)</code> Perform a point lookup. <code>getState()</code> Return the current segment   state. <code>close()</code> Start async close; transitions to <code>CLOSED</code> when finished. <ul> <li><code>flush()</code> and <code>compact()</code> return <code>SegmentResult&lt;Void&gt;</code>; completion is observed by <code>getState()</code> returning <code>READY</code>.</li> <li><code>close()</code> returns <code>SegmentResult&lt;Void&gt;</code> only; completion is observed via <code>getState()</code>.</li> <li>Do not wait for <code>READY</code> while running on the segment maintenance   executor thread; this can deadlock the maintenance queue.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#response-codes","title":"Response Codes","text":"<p>All method depending on segment state could replay with on of the follown codes: </p> Code Description <code>OK</code> Processed successfully or operation was accepted and scheduled; completion is observed when the segment returns to <code>READY</code>. <code>BUSY</code> Temporary refusal; retry makes sense. <code>CLOSED</code> Segment permanently unavailable. <code>ERROR</code> Unrecoverable. <ul> <li><code>BUSY</code>/<code>CLOSED</code>/<code>ERROR</code> mean the operation was not started.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#iterator-modes","title":"Iterator Modes","text":"<p>Segmend can open iterator in following modes:</p> <ul> <li><code>INTERRUPT_FAST</code> (default): optimistic read; throws on version change. Reads   a snapshot of the merged view captured at open time.</li> <li><code>STOP_FAST</code>: optimistic read; stops on version change. Reads a snapshot of the   merged view captured at open time.</li> <li><code>EXCLUSIVE_ACCESS</code>: stop-the-world maintenance; blocks other operations and   must be short. Reads a merged snapshot captured while the segment is frozen.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#segment-states","title":"Segment States","text":"<p>Segment could be in one of following states. States:</p> <ul> <li><code>READY</code>: normal operation.</li> <li><code>MAINTENANCE_RUNNING</code>: background <code>flush()</code> or <code>compact()</code> is executing.</li> <li><code>FREEZE</code>: short exclusive phase for snapshot or swap.</li> <li><code>CLOSED</code>: segment closed.</li> <li><code>ERROR</code>: unrecoverable.</li> </ul> <p>If an operation is not allowed in the current state, return <code>BUSY</code> in <code>FREEZE</code> or <code>MAINTENANCE_RUNNING</code>, <code>CLOSED</code> in <code>CLOSED</code>, and <code>ERROR</code> in <code>ERROR</code>.</p> <p></p>"},{"location":"architecture/segment/segment-concurrency/#transitions","title":"Transitions","text":"Original State New State When <code>READY</code> <code>FREEZE</code> start of <code>flush()</code>, <code>compact()</code>, <code>close</code> or <code>openIterator(EXCLUSIVE_ACCESS)</code> <code>FREEZE</code> <code>MAINTENANCE_RUNNING</code> maintenance thread starts <code>flush()</code> or <code>compact()</code> <code>MAINTENANCE_RUNNING</code> <code>FREEZE</code> maintenance IO finished, swap to new files starts <code>FREEZE</code> <code>READY</code> swap complete or <code>openIterator(EXCLUSIVE_ACCESS)</code> closed any <code>ERROR</code> index or IO failure <code>FREEZE</code> <code>CLOSED</code> <code>close()</code> completes"},{"location":"architecture/segment/segment-concurrency/#optimistic-locking","title":"Optimistic locking","text":"<p>Core concept is that all data read via an iterator are immutable to the reader. When <code>flush()</code> is called, iterators must stop to avoid serving a stale snapshot. This is done via optimistic locking: when a segment iterator is opened it reads the segment version, and before each read it verifies the version has not changed. If the version changes, the iterator is interrupted. The version increments when a new immutable view is published (after <code>flush()</code> or <code>compact()</code> swaps in new files) and when <code>EXCLUSIVE_ACCESS</code> is acquired.</p>"},{"location":"architecture/segment/segment-concurrency/#iterator-isolation","title":"Iterator Isolation","text":"<p>optimistic locking allows distinguis following iteration modes:</p> <ul> <li>FAIL_FAST: optimistic read; any mutation can invalidate the iterator and   terminate iteration early.</li> <li>FULL_ISOLATION: exclusive access for the iterator lifetime; blocks writes,   flush/compact, and split on the same segment until closed.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#segment-behavior","title":"Segment Behavior","text":"<p>Segment can be accessd from multiple threads in paraell. Segment </p>"},{"location":"architecture/segment/segment-concurrency/#operation-behavior-matrix","title":"Operation Behavior Matrix","text":"Operation Allowed states Version bump Iterator impact Read write cache Read delta cache Notes <code>put</code> <code>READY</code>; <code>MAINTENANCE_RUNNING</code> (until cache full) No None N/A N/A Writes to write cache; in <code>MAINTENANCE_RUNNING</code>, returns <code>BUSY</code> if cache full. <code>get</code> <code>READY</code>, <code>MAINTENANCE_RUNNING</code> No None Yes Yes No read lock required. <code>flush</code> <code>READY</code> Yes (after publish) Invalidates optimistic iterators N/A N/A Serialized; concurrent request returns <code>BUSY</code>. May be triggered by full write cache. <code>compact</code> <code>READY</code> Yes (after publish) Invalidates optimistic iterators N/A N/A Serialized; concurrent request returns <code>BUSY</code>. May be triggered by full delta cache. <code>openIterator(INTERRUPT_FAST)</code> <code>READY</code> No Throws on version change Yes (snapshot) Yes Default mode. <code>openIterator(STOP_FAST)</code> <code>READY</code> No Stops on version change Yes (snapshot) Yes Snapshot captured at open time. <code>openIterator(EXCLUSIVE_ACCESS)</code> <code>READY</code> Yes (on lock acquisition) Invalidates existing iterators; blocks others Yes Yes Maintenance only; must be short. <code>close</code> <code>READY</code> No Invalidates existing iterators; blocks others N/A N/A Sets <code>FREEZE</code>, optionally flushes the write cache, schedules close on maintenance thread, then transitions to <code>CLOSED</code>."},{"location":"architecture/segment/segment-concurrency/#flushcompact-lifecycle","title":"Flush/Compact Lifecycle","text":"<ol> <li>Caller enters <code>FREEZE</code>, drains in-flight ops, and prepares the maintenance    plan (flush freezes the write cache; compact captures a snapshot).</li> <li>Maintenance thread sets <code>MAINTENANCE_RUNNING</code> and performs IO.</li> <li>When IO completes, state returns to <code>FREEZE</code> and new index/delta files are swapped in.</li> <li>Version increments immediately after the swap (publication).</li> <li>State becomes <code>READY</code>.</li> <li>Concurrent <code>flush()</code>/<code>compact()</code> requests return <code>BUSY</code>.</li> </ol>"},{"location":"architecture/segment/segment-concurrency/#failure-cancellation","title":"Failure &amp; Cancellation","text":"<ul> <li>On <code>flush()</code> or <code>compact()</code> failure, the current maintenance task aborts and   the segment moves to <code>ERROR</code> (unless already <code>CLOSED</code>).</li> <li>If <code>close()</code> is called during maintenance, the segment returns <code>BUSY</code>.   Retry after the segment returns to <code>READY</code>.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#concepts","title":"Concepts","text":""},{"location":"architecture/segment/segment-concurrency/#core-rules","title":"Core Rules","text":"<ul> <li>Concurrent reads/writes are supported; published data is immutable to readers.</li> <li>No disk IO in caller threads; the maintenance thread performs disk IO.</li> <li>Keep locks short; the state machine is the admission control.</li> <li><code>flush()</code> and <code>compact()</code> are commit points and are started asynchronously.</li> <li><code>Segment</code> is thread-safe by contract; callers may access it concurrently.</li> <li><code>FREEZE</code> is a short exclusive phase used for snapshotting, publish swaps, or   close admission.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#contracts-and-guarantees","title":"Contracts and Guarantees","text":""},{"location":"architecture/segment/segment-concurrency/#atomic-publish-invariant","title":"Atomic Publish Invariant","text":"<ul> <li>Publication of a new immutable view is atomic: readers see either the old view or the new view, never a partial mix.</li> <li>The view swap and version increment are linearized; operations after the swap observe the new view.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#freeze-prohibitions","title":"FREEZE Prohibitions","text":"<ul> <li>During <code>FREEZE</code>, all external operations return <code>BUSY</code>.</li> <li>Only internal maintenance steps (snapshot and swap) run in this phase.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#exclusive_access-lifecycle","title":"EXCLUSIVE_ACCESS Lifecycle","text":"<ul> <li><code>openIterator(EXCLUSIVE_ACCESS)</code> is allowed only in <code>READY</code>; otherwise it returns <code>BUSY</code>.</li> <li>On acquisition, the segment enters <code>FREEZE</code> and increments the version.</li> <li>While held, all other operations return <code>BUSY</code>. The iterator must be closed to return to <code>READY</code>.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#busy-reasons","title":"BUSY Reasons","text":"<ul> <li>Write cache full during <code>MAINTENANCE_RUNNING</code> (backpressure).</li> <li>Segment is in <code>FREEZE</code>.</li> <li><code>MAINTENANCE_RUNNING</code> and the requested operation is not allowed.</li> <li><code>flush()</code> / <code>compact()</code> already running.</li> <li><code>EXCLUSIVE_ACCESS</code> held or requested while not <code>READY</code>.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#retry-and-backpressure-guidance","title":"Retry and Backpressure Guidance","text":"<ul> <li>Treat <code>BUSY</code> as transient; retry with backoff and jitter.</li> <li>For write-cache full during <code>MAINTENANCE_RUNNING</code>, retry writes after maintenance publishes a new view.</li> <li>For maintenance or exclusive access, retry after the segment returns to <code>READY</code>.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#automatic-maintenance-triggers","title":"Automatic Maintenance Triggers","text":"<ul> <li>If the delta cache becomes full, the segment schedules <code>compact()</code>.</li> <li>If the write cache becomes full, the segment schedules <code>flush()</code>.</li> <li>These triggers transition the segment out of <code>READY</code> (into <code>FREEZE</code>/<code>MAINTENANCE_RUNNING</code>) before refusing new writes.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#memory-visibility","title":"Memory Visibility","text":"<ul> <li>The published view is swapped under <code>FREEZE</code> (exclusive); new reads see only the old or the new view, never a partial publish.</li> <li>Version increments provide an ordering point for optimistic iterators.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#freshness-vs-consistency","title":"Freshness vs Consistency","text":"<ul> <li><code>get</code> reads from the write cache and published view (freshest data).</li> <li>Iterators read a snapshot of the merged view (published + write cache) taken   at open time; later <code>put()</code> calls are not visible. Iterators are invalidated   only by version changes (publish or <code>EXCLUSIVE_ACCESS</code>).</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#serialized-state-transitions","title":"Serialized State Transitions","text":"<ul> <li>State transitions are serialized; only one transition is in flight at a time.</li> <li><code>flush()</code>, <code>compact()</code>, and <code>EXCLUSIVE_ACCESS</code> are mutually exclusive and linearized.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/segment/segment-concurrency/#thread-safety-mechanisms","title":"Thread Safety Mechanisms","text":"<ul> <li>Segment version is stored in <code>VersionController</code> (an <code>AtomicInteger</code>).</li> <li>The write cache uses a thread-safe map implementation.</li> <li><code>SegmentConcurrencyGate</code> tracks in-flight reads/writes and drains them during <code>FREEZE</code>.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#maintenance-execution","title":"Maintenance Execution","text":"<ul> <li><code>SegmentMaintenanceService.startMaintenance(...)</code> enters <code>FREEZE</code>, drains in-flight ops, builds work, then transitions to <code>MAINTENANCE_RUNNING</code> and schedules the maintenance task.</li> <li><code>runMaintenance(...)</code> executes IO work, returns to <code>FREEZE</code>, runs publish work, then returns to <code>READY</code> and optionally runs the <code>onReady</code> callback.</li> <li>Failures call <code>gate.fail()</code> unless the segment is already <code>CLOSED</code>.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#versioning-and-iterator-invalidation","title":"Versioning and Iterator Invalidation","text":"<ul> <li><code>SegmentWritePath.applyFrozenWriteCacheAfterFlush()</code> merges frozen entries and bumps the version when a snapshot was applied.</li> <li><code>SegmentCompacter.publishCompaction(...)</code> bumps the version after publish.</li> <li><code>SegmentCore.invalidateIterators()</code> bumps the version for explicit invalidation or exclusive access.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#close-workflow","title":"Close Workflow","text":"<ul> <li><code>SegmentImpl.close()</code> enters <code>FREEZE</code>, drains in-flight ops, freezes the write cache, and schedules <code>runClose(...)</code> on the maintenance executor.</li> <li><code>runClose(...)</code> flushes frozen data (if any), closes the core, transitions to <code>CLOSED</code>, and releases the segment lock.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#components","title":"Components","text":"<ul> <li>Segment: user-facing API (<code>put</code>, <code>get</code>, <code>openIterator</code>, <code>flush</code>,   <code>compact</code>) implemented by <code>SegmentImpl</code>.</li> <li>SegmentImpl: owns <code>SegmentStateMachine</code>, checks state, and delegates   to <code>SegmentCore</code>. Schedules maintenance work on the provided executor.</li> <li>SegmentCore: single-threaded core with caches, on-disk access, and   version tracking.</li> <li>SegmentStateMachine: atomic lifecycle transitions (<code>READY</code> \u2192   <code>FREEZE</code> \u2192 <code>MAINTENANCE_RUNNING</code> \u2192 <code>READY</code>).</li> <li>SegmentCompacter: performs full rewrite compaction using   <code>SegmentCore</code>.</li> <li>SegmentMaintenanceCoordinator (segmentindex): decides when to call   <code>flush()</code>/<code>compact()</code> after writes.</li> <li>SegmentAsyncExecutor + executor (segmentindex): maintenance executor   provided to <code>SegmentImpl</code> via <code>SegmentRegistry</code>.</li> <li>SegmentAsyncSplitCoordinator / SegmentSplitCoordinator (segmentindex):   schedule and perform segment splits.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#responsibilities","title":"Responsibilities","text":"<ul> <li>SegmentImpl: gates operations with the state machine, schedules   maintenance on the executor, and reports status via <code>SegmentResult</code>.</li> <li>SegmentCore: executes single-threaded read/write/maintenance steps,   manages caches and version updates, and performs no threading or state   transitions.</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#implementation-mapping","title":"Implementation Mapping","text":"<ul> <li><code>EXCLUSIVE_ACCESS</code> in this document maps to   <code>SegmentIteratorIsolation.FULL_ISOLATION</code> in code.</li> <li><code>INTERRUPT_FAST</code> / <code>STOP_FAST</code> map to   <code>SegmentIteratorIsolation.FAIL_FAST</code>.</li> <li>State transitions are enforced by <code>SegmentStateMachine</code> and executed in   <code>SegmentMaintenanceService.startMaintenance(...)</code> and   <code>SegmentMaintenanceService.runMaintenance(...)</code>.</li> <li>Maintenance scheduling lives in <code>SegmentMaintenanceCoordinator</code> and uses the   executor from <code>SegmentRegistry</code> (<code>SegmentAsyncExecutor</code>).</li> </ul>"},{"location":"architecture/segment/segment-concurrency/#future-mvcc","title":"Future: MVCC","text":"<p>Currently unused. MVCC could support iterators that remain consistent across version changes, balancing deadlocks, performance, and memory.</p>"},{"location":"architecture/segment/segment/","title":"\ud83e\uddf1 Segment implementation","text":"<p>Segment is core part of index. It represents one string sorted table file with:</p> <ul> <li>Partial consistency - iterator stop working or return consistent data</li> <li>Support Writing changes into delta files</li> <li>Bloom filter for faster evaluating if key is in index</li> <li>Scarce index for faster searching for data in main index</li> </ul>"},{"location":"architecture/segment/segment/#segment-putget-and-iterate-consistency","title":"\ud83d\udd04 Segment put/get and iterate consistency","text":"<p>operations like write and get should be always consistent. What is written is read. Iteration behave differently. better than provide old data it stop providing any data.</p> <p>Let's have a followin key value entries in main index:</p> <pre><code>&lt;a, 20 &gt;\n&lt;b, 30 &gt;\n&lt;c, 40 &gt;\n</code></pre> <p>In segment cache are following entries:</p> <pre><code>&lt;a, 25&gt;\n&lt;e, 28&gt;\n&lt;b, tombstone&gt;\n</code></pre> <p>When user will iterate throught segment data, there will be followin cases:</p>"},{"location":"architecture/segment/segment/#case-1-read-data","title":"Case 1 - Read data","text":"<pre><code>iterator.read() --&gt; &lt;a, 25&gt;\niterator.read() --&gt; &lt;c, 40&gt;\niterator.read() --&gt; &lt;e, 28&gt;\n</code></pre>"},{"location":"architecture/segment/segment/#case-2-change-data","title":"Case 2 - Change data","text":"<pre><code>iterator.read() --&gt; &lt;a, 25&gt;\nsegment.write(c, 10)\niterator.read() --&gt; null\n</code></pre> <p>Any segment write operation will break segment iterator. It's easier way to secure segment consistency.  </p>"},{"location":"architecture/segment/segment/#caching-of-segment-data","title":"\ud83d\uddc4\ufe0f Caching of segment data","text":"<p>Segment caching has two parts: in-memory caches for write/delta data and lazy-loaded disk-backed resources.</p> <p>In-memory caches: * <code>SegmentCache</code> keeps three views: write cache (new writes), frozen write   cache (snapshot during flush), and delta cache (in-memory view of on-disk   delta files). * On segment creation, <code>SegmentBuilder#createSegmentCache</code> calls   <code>SegmentDeltaCacheLoader.loadInto</code>, which reads all delta files and populates   the delta cache. This is the only eager load. * During flush, <code>freezeWriteCache</code> moves the current write cache into the   frozen cache, writes it to a delta file, then merges it into the delta cache   (<code>mergeFrozenWriteCacheToDeltaCache</code>). * Reads consult write \u2192 frozen \u2192 delta. Iteration merges the index iterator   with <code>SegmentCache.getAsSortedList()</code>.</p> <p>Lazy-loaded resources: * <code>SegmentResourcesImpl</code> lazily loads and caches the Bloom filter and scarce   index via <code>SegmentDataSupplier</code>. They are created on first access and held in   <code>AtomicReference</code>s. * <code>SegmentDeltaCacheController.clear(...)</code> invalidates these resources when   delta files are cleared (compaction or replacement) to avoid stale lookups. * <code>SegmentReadPath</code> also caches <code>SegmentIndexSearcher</code> for point lookups and   resets it on maintenance.</p>"},{"location":"architecture/segment/segment/#segment-directory-layout","title":"\ud83d\udcc1 Segment directory layout","text":"<p>Segment writes all files into the <code>AsyncDirectory</code> passed to <code>SegmentBuilder</code>. That directory can point to:</p> <ul> <li>Index root (flat layout): segment files live next to <code>index.map</code>.</li> <li>Per-segment directory (segment-root layout): e.g. <code>segment-00001/</code> contains   all files for that segment.</li> </ul> <p>For segment id <code>segment-00001</code> the directory contains:</p> <ul> <li><code>v01-index.sst</code> - main SST file</li> <li><code>v01-scarce.sst</code> - sparse index</li> <li><code>v01-bloom-filter.bin</code> - Bloom filter store</li> <li><code>manifest.txt</code> - segment metadata (active version, delta count)</li> <li><code>.lock</code> - segment lock file</li> <li><code>v01-delta-0000.cache</code>, <code>v01-delta-0001.cache</code>, ... - delta cache files   (4-digit padded counter)</li> </ul> <p>Versioned layouts use the <code>vNN-</code> marker in file names when <code>SegmentPropertiesManager</code> records the active version, e.g. <code>v02-index.sst</code> and <code>v02-delta-0001.cache</code>.</p>"},{"location":"architecture/segment/segment/#writing-to-segment","title":"\u270d\ufe0f Writing to segment","text":"<p>Opening segment writer immediatelly close all segment readers. When writing operation add key that is in index but is not in cache this value will not returned updated. </p> <p>Putting new entry into segment is here:</p> <p></p>"},{"location":"architecture/segmentindex/","title":"\ud83d\udcda SegmentIndex Architecture","text":"<p>This section describes the top-level index orchestration layer: how operations are routed, cached, and executed across segments.</p> <p>Segment internals are intentionally centralized in Segment Architecture to avoid duplication.</p>"},{"location":"architecture/segmentindex/#topics","title":"Topics","text":"<ul> <li>Read Path \u2014 request routing and lookup flow.</li> <li>Write Path \u2014 buffering, flush, compaction, and split   orchestration.</li> <li>Caching Strategy \u2014 index-level cache roles and sizing.</li> <li>Cache LRU \u2014 bounded LRU behavior and trade-offs.</li> <li>Performance Model &amp; Sizing \u2014 throughput/latency model and   tuning knobs.</li> <li>Segment Index Concurrency \u2014 index   thread-safety and lifecycle behavior.</li> </ul>"},{"location":"architecture/segmentindex/cache-lru/","title":"Cache LRU","text":"<p>This page describes the <code>CacheLru</code> API and the <code>CacheLruImpl</code> implementation, including the compromise between throughput and strict LRU ordering. The goal is to keep access hot paths lock-free while still bounding memory and evicting least-recently-used entries approximately.</p>"},{"location":"architecture/segmentindex/cache-lru/#purpose","title":"Purpose","text":"<p><code>CacheLru</code> provides a bounded in-memory cache with an approximate LRU eviction policy. Each cached entry stores a monotonic access counter; the smallest counter is treated as the least recently used entry during eviction.</p> <p>Code: <code>src/main/java/org/hestiastore/index/cache/CacheLru.java</code>, <code>src/main/java/org/hestiastore/index/cache/CacheLruImpl.java</code>, <code>src/main/java/org/hestiastore/index/cache/CacheElement.java</code>.</p>"},{"location":"architecture/segmentindex/cache-lru/#thread-safety-compromise","title":"Thread-safety compromise","text":"<p>The implementation favors throughput over strict LRU accuracy:</p> <ul> <li>Reads and writes are lock-free for normal access.</li> <li>Eviction is serialized by a single lock when the size limit is exceeded.</li> <li>Recency updates use an atomic counter; per-entry counters are unsynchronized.</li> </ul> <p>This keeps common operations fast while still maintaining a bounded cache.</p> <p>The selected model is eviction-only locking: normal access never blocks, and only eviction scans/removals are serialized. This avoids contention on reads and writes at the cost of approximate LRU ordering under high concurrency.</p>"},{"location":"architecture/segmentindex/cache-lru/#supported-operations","title":"Supported operations","text":"Operation Behavior Thread-safety notes <code>put(key, value)</code> Inserts or overwrites a value and may trigger eviction. Lock-free write; eviction takes the eviction lock and scans the map. <code>putNull(key)</code> Inserts a null marker to short-circuit future lookups. Lock-free write; eviction behavior same as <code>put</code>. <code>get(key)</code> Returns an <code>Optional</code> and updates recency on hit. Lock-free read; access counter is atomic and entry counters are best-effort. <code>ivalidate(key)</code> Removes a single entry and notifies the eviction listener. Lock-free remove; callback runs immediately. <code>invalidateAll()</code> Clears all entries and notifies the listener for each value entry. Iterates concurrently; callbacks run during the traversal."},{"location":"architecture/segmentindex/cache-lru/#data-structures","title":"Data structures","text":"<ul> <li><code>ConcurrentHashMap&lt;K, CacheElement&lt;V&gt;&gt;</code> for concurrent access.</li> <li><code>AtomicLong accessCx</code> for a monotonic access counter.</li> <li><code>long cx</code> in each <code>CacheElement</code> as an unsynchronized access marker.</li> <li><code>evictionLock</code> to serialize eviction scans and removals.</li> </ul>"},{"location":"architecture/segmentindex/cache-lru/#access-flow","title":"Access flow","text":"<ul> <li><code>get</code>: read entry, update <code>cx</code> with the next counter.</li> <li><code>put</code> / <code>putNull</code>: write entry, then trigger eviction if needed.</li> </ul>"},{"location":"architecture/segmentindex/cache-lru/#eviction-flow","title":"Eviction flow","text":"<ul> <li>If <code>cache.size() &gt; limit</code>, acquire <code>evictionLock</code>.</li> <li>Scan entries to find the minimum <code>cx</code> (oldest).</li> <li>Remove using <code>remove(key, value)</code> to avoid racing with concurrent updates.</li> <li>Notify the eviction listener outside the lock.</li> </ul> <p>Because eviction is serialized and removal is conditional, a key is evicted at most once per insertion (unless it is reinserted). Eviction callbacks can run concurrently for different keys, but the same key is not evicted twice.</p>"},{"location":"architecture/segmentindex/cache-lru/#trade-offs","title":"Trade-offs","text":"<ul> <li>Approximate LRU: concurrent updates can reorder access counters, and entry   counters are unsynchronized, so eviction is best-effort under contention.</li> <li>Eviction cost: each eviction is O(n) over the current map.</li> <li>Temporary overshoot: concurrent puts can exceed the limit until eviction runs.</li> <li>No pinning: eviction does not skip special entries; use higher-level guards if   some entries must never be evicted while in use.</li> </ul>"},{"location":"architecture/segmentindex/cache-lru/#when-to-use","title":"When to use","text":"<p>Use <code>CacheLru</code> when you need high throughput and can tolerate approximate LRU ordering. If strict LRU or pinned entries are required, prefer a different cache structure or wrap <code>CacheLru</code> with additional coordination.</p>"},{"location":"architecture/segmentindex/caching/","title":"\ud83d\uddc4\ufe0f Caching Strategy","text":"<p>HestiaStore uses a few focused caches to deliver read\u2011after\u2011write visibility and predictable read latency while keeping memory bounded. This page outlines each layer, how it is populated/evicted, and which configuration knobs control sizing.</p> <p>For segment internals (delta, Bloom, sparse/scarce structures), use Segment Architecture. This page describes cache roles at the SegmentIndex integration level.</p>"},{"location":"architecture/segmentindex/caching/#goals","title":"\ud83c\udfaf Goals","text":"<ul> <li>Read\u2011after\u2011write consistency without synchronous disk I/O</li> <li>Bound the working set in memory via LRU at the segment layer</li> <li>Keep read I/O predictable: avoid random seeks with Bloom filter + sparse index</li> <li>Make flush/compact operations deterministic and safe</li> </ul>"},{"location":"architecture/segmentindex/caching/#layers-overview","title":"\ud83e\uddf1 Layers Overview","text":"<ul> <li>SegmentIndex write buffer: in\u2011memory, unique latest value per key</li> <li>Class: <code>cache/UniqueCache</code></li> <li>Owner: <code>segmentindex/SegmentIndexImpl</code> (top\u2011level)</li> <li> <p>Purpose: absorb writes and provide immediate visibility before flush</p> </li> <li> <p>Segment delta cache: per\u2011segment overlay of recent writes</p> </li> <li>Classes: <code>segment/SegmentDeltaCache</code>, <code>segment/SegmentDeltaCacheWriter</code>, <code>segment/SegmentDeltaCacheController</code></li> <li> <p>Purpose: hold sorted updates for a segment between compactions; also backs reads</p> </li> <li> <p>Segment data LRU: cache of heavyweight per\u2011segment objects</p> </li> <li>Classes: <code>segmentindex/SegmentDataCache</code> (LRU), values are <code>segment/SegmentData</code> (lazy container)</li> <li> <p>Contents: delta cache, Bloom filter, sparse index (scarce index)</p> </li> <li> <p>Bloom filter: per\u2011segment probabilistic set for negative checks</p> </li> <li> <p>Classes: <code>bloomfilter/*</code>; created by <code>segment/SegmentDataSupplier</code></p> </li> <li> <p>Sparse index (\"scarce index\"): per\u2011segment in\u2011memory snapshot of pointers</p> </li> <li> <p>Classes: <code>scarceindex/ScarceIndex</code>, <code>ScarceIndexSnapshot</code></p> </li> <li> <p>Key\u2192segment map: max\u2011key to SegmentId mapping</p> </li> <li>Class: <code>segmentindex/KeyToSegmentMap</code> (TreeMap, persisted to <code>index.map</code>)</li> </ul>"},{"location":"architecture/segmentindex/caching/#writetime-caches","title":"\u270d\ufe0f Write\u2011Time Caches","text":""},{"location":"architecture/segmentindex/caching/#segmentindex-write-buffer-uniquecache","title":"SegmentIndex write buffer (UniqueCache)","text":"<ul> <li>On <code>SegmentIndex.put/delete</code>, the write is stored in an index\u2011level <code>UniqueCache</code>.</li> <li>Replaces any prior value for the same key; deletes are represented as a tombstone value.</li> <li>Triggered flush routes sorted writes to target segments and clears the buffer.</li> </ul> <p>Code: <code>segmentindex/SegmentIndexImpl#put</code>, <code>segmentindex/SegmentIndexImpl#delete</code>, <code>segmentindex/SegmentIndexImpl#flushCache</code>, <code>cache/UniqueCache</code>.</p>"},{"location":"architecture/segmentindex/caching/#segment-delta-cache","title":"Segment delta cache","text":"<ul> <li>Flush writes become per\u2011segment delta files via <code>SegmentDeltaCacheWriter</code> (transactional temp file + rename).</li> <li>If the segment\u2019s data is currently loaded in memory, the in\u2011memory delta cache is updated immediately to keep reads fresh.</li> <li>Compaction (<code>SegmentCompacter</code>) rewrites the segment, then <code>SegmentDeltaCacheController.clear()</code> evicts in\u2011memory delta cache and deletes delta files.</li> </ul> <p>Code: <code>segment/SegmentDeltaCacheWriter</code>, <code>segment/SegmentDeltaCacheController</code>, <code>segment/SegmentCompacter</code>, <code>segment/SegmentFullWriterTx#doCommit</code>.</p>"},{"location":"architecture/segmentindex/caching/#readtime-caches","title":"\ud83d\udcd6 Read\u2011Time Caches","text":"<ul> <li>Top\u2011level overlay: <code>SegmentIndex.get(k)</code> checks the index write buffer first. Iterators are also overlaid with <code>EntryIteratorRefreshedFromCache</code> so scans see most recent writes.</li> <li>Per\u2011segment overlay: <code>SegmentDeltaCache</code> is consulted before the Bloom filter + sparse index path. If it returns a tombstone, the key is absent.</li> <li>Heavy objects (Bloom filter, scarce index, delta cache) are obtained via a provider backed by LRU:</li> <li><code>segmentindex/SegmentDataCache</code> holds <code>segment/SegmentData</code> instances with an LRU limit; eviction calls <code>close()</code> on the container.</li> <li>Providers: <code>segment/SegmentDataProvider</code> implementations<ul> <li><code>segment/SegmentDataProviderLazyLoaded</code> \u2014 lazy local holder</li> </ul> </li> </ul> <p>Code: <code>segmentindex/SegmentIndexImpl#get</code>, <code>segment/SegmentImpl#get</code>, <code>segment/SegmentSearcher</code>, <code>segmentindex/EntryIteratorRefreshedFromCache</code>, <code>segmentindex/SegmentDataCache</code>.</p>"},{"location":"architecture/segmentindex/caching/#eviction-and-lifecycle","title":"\u267b\ufe0f Eviction and Lifecycle","text":"<ul> <li>UniqueCache (index write buffer): no incremental eviction; cleared on flush.</li> <li>SegmentDataCache (LRU of SegmentData): evicts least\u2011recently\u2011used segment; eviction closes Bloom filter and clears delta cache via <code>close()</code> hook.</li> <li>SegmentDeltaCache: cleared and files removed after compaction via <code>SegmentDeltaCacheController.clear()</code>; rebuilt on demand from delta files.</li> <li>KeyToSegmentMap: persisted via <code>optionalyFlush()</code> when updated; survives process restarts by reading <code>index.map</code>.</li> </ul>"},{"location":"architecture/segmentindex/caching/#configuration-knobs","title":"\u2699\ufe0f Configuration Knobs","text":"<p>Index\u2011level: - <code>IndexConfiguration.getMaxNumberOfSegmentsInCache()</code> \u2014 LRU size for <code>SegmentDataCache</code></p> <p>Per\u2011segment (via <code>SegmentConf</code>, derived from index configuration): - <code>maxNumberOfKeysInSegmentCache</code> \u2014 target size for a single delta cache - <code>maxNumberOfKeysInSegmentWriteCache</code> \u2014 in-memory write cache size before flush - <code>maxNumberOfKeysInSegmentChunk</code> \u2014 sparse index sampling cadence (affects read scan window)</p> <p>Bloom filter sizing: - <code>bloomFilterIndexSizeInBytes</code> and <code>bloomFilterNumberOfHashFunctions</code> - <code>bloomFilterProbabilityOfFalsePositive</code></p> <p>I/O buffering: - <code>diskIoBufferSize</code> \u2014 affects memory used by readers/writers across files</p> <p>See: <code>segmentindex/IndexConfiguration</code>, <code>segment/SegmentConf</code>.</p>"},{"location":"architecture/segmentindex/caching/#warmup-strategies","title":"\ud83d\udd25 Warm\u2011Up Strategies","text":"<ul> <li>Point warm\u2011up: issue representative <code>get(key)</code> calls; this loads the target segments\u2019 Bloom filter and sparse index into the LRU.</li> <li>Segment warm\u2011up: iterate a small range to prime chunk readers and caches.</li> <li>Global warm\u2011up: a bounded <code>index.getStream(SegmentWindow.limit(N))</code> over initial segments to seed the LRU without scanning the full dataset.</li> </ul>"},{"location":"architecture/segmentindex/caching/#observability","title":"\ud83e\udded Observability","text":"<ul> <li>Bloom filter effectiveness and false\u2011positive rate: <code>bloomfilter/BloomFilterStats</code>, accessible via <code>BloomFilter.getStatistics()</code>.</li> <li>SegmentIndex operation counters (coarse): <code>segmentindex/Stats</code> increments on get/put/delete.</li> </ul>"},{"location":"architecture/segmentindex/caching/#tuning-guidance","title":"\ud83d\udee0\ufe0f Tuning Guidance","text":"<ul> <li>Throughput\u2011oriented writes: tune <code>maxNumberOfKeysInSegmentWriteCache</code> and <code>maxNumberOfKeysInSegmentWriteCacheDuringMaintenance</code>; monitor memory and flush latency.</li> <li>Read\u2011heavy workloads touching few segments: increase <code>maxNumberOfSegmentsInCache</code> so the working set of segments (Bloom + scarce + delta) stays resident.</li> <li>Space\u2011sensitive deployments: reduce Bloom filter size (may increase false positives and extra reads) or disable compression filters to trade CPU for I/O.</li> <li>Latency\u2011sensitive point lookups: ensure Bloom filter is sized adequately; keep segments\u2019 working set in the LRU; consider slightly smaller <code>maxNumberOfKeysInSegmentChunk</code> to narrow the local scan window.</li> </ul>"},{"location":"architecture/segmentindex/caching/#code-pointers","title":"\ud83e\udde9 Code Pointers","text":"<ul> <li>SegmentIndex write buffer: <code>src/main/java/org/hestiastore/index/segmentindex/SegmentIndexImpl.java</code></li> <li>Segment caches and providers: <code>src/main/java/org/hestiastore/index/segmentindex/*SegmentData*</code>, <code>src/main/java/org/hestiastore/index/segment/SegmentData*</code></li> <li>LRU cache: <code>src/main/java/org/hestiastore/index/cache/CacheLru.java</code>,   <code>src/main/java/org/hestiastore/index/cache/CacheLruImpl.java</code></li> <li>Key\u2192segment map: <code>src/main/java/org/hestiastore/index/segmentindex/KeyToSegmentMap.java</code></li> </ul>"},{"location":"architecture/segmentindex/caching/#related-glossary","title":"\ud83d\udd17 Related Glossary","text":"<ul> <li>UniqueCache</li> <li>Delta Cache</li> <li>SegmentData</li> <li>Bloom Filter</li> <li>Sparse Index</li> <li>Key-to-Segment Map</li> </ul>"},{"location":"architecture/segmentindex/performance/","title":"\ud83d\ude80 Performance Model &amp; Sizing","text":"<p>This page summarizes how HestiaStore achieves high throughput and predictable latency, and how to size the main knobs. All claims map to code so you can verify behavior.</p> <p>Segment-specific internals referenced here are centralized in Segment Architecture.</p>"},{"location":"architecture/segmentindex/performance/#mental-model-hot-paths","title":"\ud83e\udde0 Mental Model (Hot Paths)","text":"<ul> <li>Put/Delete:</li> <li>O(1) to update in\u2011memory write buffer (<code>UniqueCache</code>).</li> <li>Batched flush sorts unique keys (parallel sort over entries) and writes per\u2011segment delta files sequentially.</li> <li> <p>Optional compaction merges delta files into the main SST (sequential chunk write).</p> </li> <li> <p>Get (negative):</p> </li> <li> <p>O(k) Bloom probe (k = hash functions), no disk I/O when filter says \u201cabsent\u201d.</p> </li> <li> <p>Get (positive):</p> </li> <li>Locate target segment via key\u2192segment map (in\u2011memory TreeMap ceiling lookup).</li> <li>Seek into <code>vNN-index.sst</code> by sparse index pointer, then bounded local scan of at most <code>maxNumberOfKeysInSegmentChunk</code> entries in ascending order. Typically one chunk read.</li> </ul>"},{"location":"architecture/segmentindex/performance/#io-patterns-and-amplification","title":"\ud83d\udcbd I/O Patterns and Amplification","text":"<ul> <li>Sequential writes: delta files and SST chunks append sequentially via transactional writers (<code>*.tmp</code> + rename).</li> <li>Sequential reads: positive get reads one chunk and scans \u2264 N keys (N = <code>maxNumberOfKeysInSegmentChunk</code>).</li> <li>Negative reads: avoid disk I/O via Bloom filter unless false positive.</li> <li>Alignment and block size:</li> <li>Chunk store uses fixed 16\u2011byte cells with data blocks sized by <code>diskIoBufferSize</code> (divisible by 1024). Payloads are padded to whole cells for easy positioning.</li> <li>Code: <code>chunkstore/CellPosition.java</code>, <code>datablockfile/DataBlockSize.java</code>, <code>Vldtn#requireIoBufferSize</code>.</li> </ul>"},{"location":"architecture/segmentindex/performance/#key-knobs-what-they-do","title":"\u2699\ufe0f Key Knobs (What They Do)","text":"<ul> <li><code>maxNumberOfKeysInSegmentChunk</code> (sparse index cadence)</li> <li> <p>Lower \u21d2 smaller local scan window (read latency) with more sparse\u2011index entries; slightly more write work during compaction.</p> </li> <li> <p>Bloom filter sizing: <code>bloomFilterIndexSizeInBytes</code>, <code>bloomFilterNumberOfHashFunctions</code>, or target probability</p> </li> <li>From <code>BloomFilterBuilder</code>: m = \u2212(n ln p)/(ln2)^2, k \u2248 m/n\u00b7ln2. Larger m lowers false positives and I/O on negative lookups at the cost of RAM and disk for the filter.</li> <li> <p>Code: <code>bloomfilter/BloomFilterBuilder.java</code>.</p> </li> <li> <p><code>maxNumberOfSegmentsInCache</code> (SegmentData LRU)</p> </li> <li> <p>Number of segments whose Bloom + sparse index + delta cache can be resident. Too small \u21d2 thrash; too large \u21d2 memory waste.</p> </li> <li> <p><code>diskIoBufferSize</code></p> </li> <li> <p>Sets data\u2011block size for chunk store and buffers for file readers/writers. Choose 4\u201364 KiB depending on device. Must be divisible by 1024.</p> </li> <li> <p>Encoding/Decoding filters (CRC, magic, Snappy, XOR)</p> </li> <li> <p>Snappy reduces I/O on compressible values at CPU cost. CRC + magic are lightweight integrity guards and on by default.</p> </li> <li> <p>Context logging (<code>isContextLoggingEnabled</code>)</p> </li> <li>Adds MDC setup/teardown per operation so logs can include <code>index.name</code>.</li> </ul>"},{"location":"architecture/segmentindex/performance/#memory-sizing","title":"\ud83e\uddee Memory Sizing","text":"<ul> <li>Per\u2011segment delta overlay (in memory): when a segment is loaded, delta files are folded into a <code>UniqueCache</code>. Upper bound approximates number of unique keys across delta files (see segment properties).</li> <li>Bloom filter: fully memory\u2011mapped in RAM when present; <code>indexSizeInBytes</code> bytes per segment plus metadata. Code: <code>bloomfilter/BloomFilterImpl.java</code>.</li> <li>SegmentData LRU: holds delta cache + Bloom + scarce index for up to <code>maxNumberOfSegmentsInCache</code> segments; evictions call <code>close()</code> to free memory.</li> </ul>"},{"location":"architecture/segmentindex/performance/#cpu-sizing","title":"\ud83e\udde0 CPU Sizing","text":"<ul> <li>Put path: hashing and HashMap work; occasional sort on flush (parallel sort over entries) and CRC/magic/Snappy filters on compaction.</li> <li>Get path: a few compares, at most N key compares during the bounded scan, optional Snappy decompression on read.</li> <li>Enabling context logging adds a small per\u2011operation MDC overhead.</li> </ul>"},{"location":"architecture/segmentindex/performance/#practical-tuning-recipes","title":"\ud83e\uddea Practical Tuning Recipes","text":"<ul> <li>Write\u2011heavy ingestion:</li> <li>Consider enabling Snappy if values are highly compressible and I/O bound.</li> <li> <p>Keep <code>maxNumberOfKeysInSegmentChunk</code> moderate (e.g., 512\u20132048) to keep sparse index size reasonable during compaction.</p> </li> <li> <p>Read\u2011latency sensitive point lookups:</p> </li> <li>Ensure Bloom filters are sized adequately (lower false positive rate with larger <code>indexSizeInBytes</code>).</li> <li>Reduce <code>maxNumberOfKeysInSegmentChunk</code> to shrink the local scan window.</li> <li> <p>Increase <code>maxNumberOfSegmentsInCache</code> so hot segments stay resident.</p> </li> <li> <p>Mixed workloads:</p> </li> <li>Start with defaults; adjust Bloom size and segment LRU to fit your hot set; validate with counters and filter stats.</li> </ul>"},{"location":"architecture/segmentindex/performance/#observability-and-validation","title":"\ud83d\udd0e Observability and Validation","text":"<ul> <li>Bloom stats: <code>BloomFilter.getStatistics()</code> reports avoided disk accesses and false\u2011positive rate. Code: <code>bloomfilter/BloomFilterStats</code>.</li> <li>Operation counters: <code>segmentindex/Stats</code> exposes get/put/delete counts (logged on close in <code>SegmentIndexImpl#doClose</code>).</li> <li>Consistency: after unexpected shutdown, run <code>SegmentIndex.checkAndRepairConsistency()</code>; optionally <code>compact()</code> to reclaim locality.</li> </ul>"},{"location":"architecture/segmentindex/performance/#code-pointers","title":"\ud83e\udde9 Code Pointers","text":"<ul> <li>Write buffer and flush: <code>src/main/java/org/hestiastore/index/segmentindex/SegmentIndexImpl.java</code>, <code>src/main/java/org/hestiastore/index/segmentindex/CompactSupport.java</code></li> <li>Read path bounds: <code>src/main/java/org/hestiastore/index/segment/SegmentSearcher.java</code>, <code>.../SegmentIndexSearcher.java</code></li> <li>Bloom filter: <code>src/main/java/org/hestiastore/index/bloomfilter/*</code></li> <li>Chunked I/O and filters: <code>src/main/java/org/hestiastore/index/chunkstore/*</code></li> <li>Segment sizing/splitting: <code>src/main/java/org/hestiastore/index/segmentindex/SegmentSplitCoordinator.java</code>, <code>src/main/java/org/hestiastore/index/segment/SegmentSplitter*.java</code></li> </ul>"},{"location":"architecture/segmentindex/performance/#related-glossary","title":"\ud83d\udd17 Related Glossary","text":"<ul> <li>Main SST</li> <li>Sparse Index</li> <li>Bloom Filter</li> <li>UniqueCache</li> <li>Delta Cache</li> <li>Compaction</li> <li>Write Transaction</li> </ul>"},{"location":"architecture/segmentindex/read-path/","title":"\ud83d\udcd6 Read Path","text":"<p>This page explains how reads resolve values with low latency and predictable I/O. It walks through point lookups, range iteration, and the interplay of caches, Bloom filter, and the sparse index, mapped to concrete classes in the codebase.</p> <p>Segment-internal details are centralized in Segment Architecture. This page focuses on the SegmentIndex-level orchestration path.</p>"},{"location":"architecture/segmentindex/read-path/#highlevel-flow-point-lookup","title":"\ud83e\udded High\u2011Level Flow (Point Lookup)","text":"<ol> <li>API call: <code>SegmentIndex.get(key)</code></li> <li>Check the index\u2011level unique buffer (latest in\u2011process writes)</li> <li>Locate the target segment using the key\u2192segment map</li> <li>Inside the segment: consult delta cache \u2192 Bloom filter \u2192 sparse index \u2192 local scan</li> </ol> <p>Lookups are read\u2011after\u2011write consistent thanks to the in\u2011memory buffers.</p>"},{"location":"architecture/segmentindex/read-path/#entry-point-and-firstlevel-cache","title":"\ud83d\udeaa Entry Point and First\u2011Level Cache","text":"<ul> <li><code>segmentindex/SegmentIndexImpl#get(K)</code> does:</li> <li>Check the index\u2011level <code>UniqueCache</code> (holds latest writes prior to flush)</li> <li>If miss, find <code>SegmentId</code> via <code>KeyToSegmentMap.findSegmentId(key)</code></li> <li>Delegate to <code>Segment.get(key)</code></li> </ul> <p>Key classes: <code>segmentindex/SegmentIndexImpl.java</code>, <code>segmentindex/KeyToSegmentMap.java</code>, <code>cache/UniqueCache.java</code>.</p>"},{"location":"architecture/segmentindex/read-path/#behavior","title":"Behavior","text":"<ul> <li>Cache hit and non\u2011tombstone \u2192 return value</li> <li>Cache hit and tombstone \u2192 treat as absent</li> <li>Otherwise fall back to the segment path below</li> </ul>"},{"location":"architecture/segmentindex/read-path/#persegment-read-path","title":"\ud83e\udde9 Per\u2011Segment Read Path","text":"<p><code>segment/SegmentImpl#get(key)</code> uses <code>SegmentSearcher</code> with lazily loaded segment data:</p> <ol> <li>Delta cache probe: in\u2011memory map of the segment\u2019s pending updates (merged from delta files). If hit and value not a tombstone \u2192 return; tombstone \u2192 absent.</li> <li>Bloom filter: <code>bloomFilter.isNotStored(key)</code> guards the on\u2011disk path. If \u201cnot stored\u201d \u2192 absent.</li> <li>Sparse index (\"scarce index\"): returns a chunk start position for keys \u2265 query.</li> <li>Local scan: within at most N keys (<code>maxNumberOfKeysInIndexPage</code>) starting at that chunk, compare keys in ascending order and stop as soon as the target is found or passed.</li> <li>If the sparse index pointed us into the file but no exact key was found, mark a false positive on the Bloom filter for metrics and return absent.</li> </ol> <p>Key classes: <code>segment/SegmentSearcher.java</code>, <code>segment/SegmentIndexSearcher.java</code>, <code>scarceindex/ScarceSegmentIndex.java</code>, <code>bloomfilter/BloomFilter.java</code>.</p>"},{"location":"architecture/segmentindex/read-path/#range-scans-and-full-iteration","title":"\ud83d\udd01 Range Scans and Full Iteration","text":"<ul> <li><code>SegmentIndex.getStream()</code> and <code>SegmentIndex.openSegmentIterator(...)</code> produce iterators over all data:</li> <li><code>segmentindex/SegmentsIterator</code> chains <code>Segment.openIterator()</code> across all segments in order.</li> <li><code>segment/SegmentImpl.openIterator()</code> merges the on\u2011disk main SST with the segment\u2019s delta cache via <code>MergeDeltaCacheWithIndexIterator</code>, skipping tombstones.</li> <li>The per\u2011segment iterator is wrapped with <code>EntryIteratorWithLock</code> using an <code>OptimisticLock</code>. If a write changes the segment version mid\u2011scan, the iterator stops gracefully (no partial records).</li> <li>At the top level, <code>EntryIteratorRefreshedFromCache</code> overlays the index\u2011level unique buffer so that the iterator sees the latest writes even before they\u2019re flushed to disk.</li> </ul> <p>Key classes: <code>segmentindex/SegmentsIterator.java</code>, <code>segment/MergeDeltaCacheWithIndexIterator.java</code>, <code>segmentindex/EntryIteratorRefreshedFromCache.java</code>, <code>EntryIteratorWithLock.java</code>, <code>OptimisticLock.java</code>.</p>"},{"location":"architecture/segmentindex/read-path/#readafterwrite-semantics","title":"\ud83d\udd04 Read\u2011After\u2011Write Semantics","text":"<p>Two layers provide immediate visibility of recent writes:</p> <ul> <li>Index\u2011level <code>UniqueCache</code> (pre\u2011flush) is checked first by <code>SegmentIndex.get</code> and overlaid on iterators.</li> <li>Segment delta cache (post\u2011flush) is kept in memory when loaded; writes to a new delta file also update the in\u2011memory delta cache when present.</li> </ul> <p>Deletes are represented as tombstones by the value type descriptor. The read path treats a tombstone as \u201cnot found\u201d.</p>"},{"location":"architecture/segmentindex/read-path/#complexity-and-io-characteristics","title":"\ud83e\uddee Complexity and I/O Characteristics","text":"<ul> <li>Index\u2011level cache probe: O(1) hash map</li> <li>Segment delta cache probe: O(1) hash map</li> <li>Bloom filter probe: O(k) where k is number of hash functions; no I/O</li> <li>Sparse index probe: in\u2011memory list search over a small sample set (fast, cache\u2011friendly)</li> <li>Local scan: sequential read within one chunk window of up to <code>maxNumberOfKeysInIndexPage</code> entries</li> <li>Iterators: sequential over chunks; minimal seeks due to chunked layout</li> </ul> <p>These choices keep random access bounded and predictable, with sequential I/O for scans.</p>"},{"location":"architecture/segmentindex/read-path/#configuration-knobs-affecting-reads","title":"\u2699\ufe0f Configuration Knobs Affecting Reads","text":"<ul> <li><code>maxNumberOfKeysInSegmentChunk</code> \u2014 upper bound of keys per chunk; also the window size for a local scan from the sparse index pointer</li> <li>Bloom filter parameters \u2014 <code>numberOfHashFunctions</code>, <code>indexSizeInBytes</code>, <code>falsePositiveProbability</code></li> <li><code>diskIoBufferSize</code> \u2014 affects chunk and data block I/O buffering</li> <li>Encoding/decoding filters \u2014 enable CRC32, magic number and optional Snappy compression on read/write paths</li> </ul> <p>See: <code>segmentindex/IndexConfiguration</code> and <code>segment/SegmentConf</code>.</p>"},{"location":"architecture/segmentindex/read-path/#integrity-on-the-read-path","title":"\ud83d\udee1\ufe0f Integrity on the Read Path","text":"<p>Decoding applies the inverse of the write pipeline when reading chunks:</p> <ul> <li>Validate magic number</li> <li>Verify CRC32</li> <li>Decompress (if Snappy was enabled)</li> </ul> <p>Errors surface as exceptions; partial reads do not corrupt state.</p> <p>Key classes: <code>chunkstore/ChunkStoreReaderImpl</code>, <code>chunkstore/ChunkFilterMagicNumberValidation</code>, <code>chunkstore/ChunkFilterCrc32Validation</code>, <code>chunkstore/ChunkFilterSnappyDecompress</code>.</p>"},{"location":"architecture/segmentindex/read-path/#where-to-look-in-the-code","title":"\ud83e\udde9 Where to Look in the Code","text":"<ul> <li>Point lookup orchestration: <code>src/main/java/org/hestiastore/index/segmentindex/SegmentIndexImpl.java</code></li> <li>Segment search path: <code>src/main/java/org/hestiastore/index/segment/SegmentSearcher.java</code></li> <li>Sparse index: <code>src/main/java/org/hestiastore/index/scarceindex/*</code></li> <li>Iteration and merging: <code>src/main/java/org/hestiastore/index/segment/MergeDeltaCacheWithIndexIterator.java</code></li> <li>Iterator safety: <code>src/main/java/org/hestiastore/index/EntryIteratorWithLock.java</code></li> </ul>"},{"location":"architecture/segmentindex/read-path/#related-glossary","title":"\ud83d\udd17 Related Glossary","text":"<ul> <li>Segment</li> <li>Delta Cache</li> <li>Bloom Filter</li> <li>Sparse Index</li> <li>UniqueCache</li> <li>EntryIterator</li> <li>SegmentWindow</li> </ul>"},{"location":"architecture/segmentindex/segment-index-concurrency/","title":"Segment Index Concurrency &amp; Lifecycle","text":""},{"location":"architecture/segmentindex/segment-index-concurrency/#glossary","title":"Glossary","text":"<ul> <li>Segment index: top-level API that routes operations to segments.</li> <li>Key-segment mapping: map of max key -&gt; SegmentId (KeyToSegmentMap).</li> <li>Mapping version: monotonically increasing counter for optimistic mapping   checks.</li> <li>Segment registry: cache of Segment instances plus the maintenance executor.</li> <li>Maintenance coordinator: decides compact/split after writes.</li> <li>Split: replace one segment with a new segment (or two) and update the   mapping.</li> </ul>"},{"location":"architecture/segmentindex/segment-index-concurrency/#core-rules","title":"Core Rules","text":"<ul> <li>SegmentIndex is thread-safe by contract; calls may be concurrent.</li> <li>Target: highly concurrent SegmentIndex API; avoid global synchronization and   only protect minimal shared structures (mapping updates, split swaps).</li> <li>Index operations are not globally serialized; concurrency is bounded by   shared caches, mapping updates, and per-segment state machines.</li> <li>Segment maintenance IO runs on the segment maintenance executor.</li> <li>The maintenance executor is always created by SegmentRegistry from   IndexConfiguration.numberOfSegmentIndexMaintenanceThreads (default 10).</li> <li>Automatic post-write flush/compact is optional and enabled by default.</li> <li>Segment BUSY is treated as transient and retried internally; callers do not   see BUSY.</li> <li>Mapping changes are applied atomically and validated by version checks.</li> </ul>"},{"location":"architecture/segmentindex/segment-index-concurrency/#thread-safety-mechanisms","title":"Thread Safety Mechanisms","text":"<ul> <li>IndexInternalConcurrent executes sync operations on caller threads without   a global executor.</li> <li>SegmentRegistrySynchronized serializes access to the segment instance map and   registry mutations.</li> <li>KeyToSegmentMap uses snapshot reads plus a mapping version; updates take a   write lock and increment the version.</li> <li>Segment implementations are thread-safe; read/write operations proceed in   parallel when the segment state allows it.</li> </ul>"},{"location":"architecture/segmentindex/segment-index-concurrency/#api-behavior","title":"API Behavior","text":"<ul> <li>put/get/delete: retry on per-segment BUSY using IndexRetryPolicy   (indexBusyBackoffMillis + indexBusyTimeoutMillis); mapping version mismatch   triggers a retry with a fresh snapshot. Timeouts throw IndexException.</li> <li>putAsync/getAsync/deleteAsync: run the synchronous operation on a background   thread via IndexAsyncAdapter and return a CompletionStage.</li> <li>flush/compact: start maintenance on each segment and return once accepted;   do not wait for IO completion; BUSY retries follow IndexRetryPolicy.</li> <li>flushAndWait/compactAndWait: wait for each segment to return to <code>READY</code>   (or <code>CLOSED</code>); do not call from a segment maintenance executor thread.</li> <li>getStream: captures a snapshot of segment ids and iterates them using the   default segment iterator isolation (FAIL_FAST). An overload allows   FULL_ISOLATION for per-segment exclusivity; the stream must be closed to   release the segment lock.</li> <li>Segment close (async): once close starts, the segment drains in-flight work   and rejects/blocks new operations until CLOSED. The registry should not   reopen a closing segment; attempts should retry until the close completes.   The per-segment <code>.lock</code> file enforces single-open at the directory level.</li> </ul>"},{"location":"architecture/segmentindex/segment-index-concurrency/#maintenance-splits","title":"Maintenance &amp; Splits","text":"<ul> <li>SegmentMaintenanceCoordinator evaluates thresholds after each write and   triggers flush/compact only when segmentMaintenanceAutoEnabled is true.</li> <li>Splits are scheduled by SegmentAsyncSplitCoordinator on the shared   maintenance executor; only one split per segment id can be in flight.</li> <li>SegmentSplitCoordinator retries BUSY using IndexRetryPolicy; timeouts throw.</li> <li>SegmentSplitCoordinator opens a FULL_ISOLATION iterator and keeps it open   until file swap + mapping update completes to prevent partial splits from   leaking to writers.</li> <li>After a split, KeyToSegmentMap updates the mapping and flushes it to disk;   any in-flight write with a stale mapping version retries.</li> </ul>"},{"location":"architecture/segmentindex/segment-index-concurrency/#index-state-machine","title":"Index State Machine","text":"<p>States: - OPENING: index bootstrap/consistency checks (and lock acquisition) in   progress; operations are rejected. - READY: operations allowed. - ERROR: unrecoverable failure; operations are rejected. - CLOSED: operations are rejected.</p> <p>Transitions: - OPENING -&gt; READY: after initialization and consistency checks complete. - READY -&gt; CLOSED: close() completes; file lock released. - any -&gt; ERROR: unrecoverable failure (e.g., OOM, disk full, failed split/file   swap, or consistency check failure).</p> <p>Only one index instance may hold the directory lock at a time.</p>"},{"location":"architecture/segmentindex/segment-index-concurrency/#failure-handling","title":"Failure Handling","text":"<ul> <li>SegmentResultStatus.ERROR from any segment results in IndexException.</li> <li>Maintenance failures move the segment to ERROR; flushAndWait/compactAndWait   propagate as IndexException.</li> <li>Split failures surface through the split future and are rethrown when joined.</li> <li>When entering ERROR, the index stops accepting operations and requires manual   intervention (recovery/repair or restore from backups).</li> </ul>"},{"location":"architecture/segmentindex/segment-index-concurrency/#components","title":"Components","text":"<ul> <li>SegmentIndex (public API): thread-safe entry point.</li> <li>SegmentIndexImpl: retries BUSY, routes operations to segments, and manages   maintenance.</li> <li>IndexAsyncAdapter: provides async operations by running sync calls on a   background thread and waiting for in-flight async calls on close.</li> <li>SegmentIndexCore: single-attempt mapping + segment selection.</li> <li>IndexRetryPolicy: backoff + timeout for BUSY retries.</li> <li>IndexResult/IndexResultStatus: internal OK/BUSY/CLOSED/ERROR wrapper.</li> <li>KeyToSegmentMap: mapping, snapshot versioning, and persistence of segment ids.</li> <li>SegmentRegistry(Synchronized): caches Segment instances and supplies the   maintenance executor.</li> <li>SegmentMaintenanceCoordinator: post-write flush/compact/split decisions.</li> <li>SegmentSplitCoordinator / SegmentAsyncSplitCoordinator: split execution and   scheduling.</li> </ul>"},{"location":"architecture/segmentindex/segment-index-concurrency/#iterator-isolation","title":"Iterator Isolation","text":"<ul> <li>FAIL_FAST: iteration is optimistic; any mutation can invalidate the   iterator and terminate the stream early.</li> <li>FULL_ISOLATION: holds exclusive access per segment while its iterator is   open; writers, flush/compact, and split on that segment block until the   iterator (or stream) is closed.</li> </ul>"},{"location":"architecture/segmentindex/segment-index-concurrency/#implementation-mapping","title":"Implementation Mapping","text":"<ul> <li>Index implementation: IndexInternalConcurrent (caller-thread execution).</li> <li>Mapping version: KeyToSegmentMap.version (AtomicLong).</li> <li>Maintenance executor: SegmentRegistry.getMaintenanceExecutor() backed by   IndexConfiguration.numberOfSegmentIndexMaintenanceThreads (default 10).</li> <li>Split isolation: SegmentIteratorIsolation.FULL_ISOLATION.</li> <li>Retry policy: IndexConfiguration.indexBusyBackoffMillis and   IndexConfiguration.indexBusyTimeoutMillis.</li> </ul>"},{"location":"architecture/segmentindex/write-path/","title":"\u270d\ufe0f Write Path","text":"<p>This page describes how a write travels through HestiaStore from the API call to on\u2011disk structures, highlighting buffering, compaction, and atomicity. It maps directly to the code so you can cross\u2011check behavior and tune configuration.</p> <p>Segment-internal structures are centralized in Segment Architecture. This page focuses on SegmentIndex orchestration and operation flow.</p>"},{"location":"architecture/segmentindex/write-path/#highlevel-flow","title":"\ud83e\udded High\u2011Level Flow","text":"<ol> <li>API call: <code>SegmentIndex.put(key, value)</code> or <code>SegmentIndex.delete(key)</code></li> <li>In\u2011memory unique write buffer accepts the latest value per key</li> <li>Threshold\u2011based flush routes buffered writes to target segments</li> <li>Segment delta caches persist sorted updates as transactional files</li> <li>Segment compaction merges delta caches into the main SST + sparse index + bloom filter</li> <li>Optional segment split when size thresholds are exceeded</li> </ol> <p>Writes become durable when flushed to segment files. Closing the index performs a flush.</p>"},{"location":"architecture/segmentindex/write-path/#entry-points","title":"\ud83d\udeaa Entry Points","text":"<ul> <li><code>SegmentIndex.put(K,V)</code> and <code>SegmentIndex.delete(K)</code> validate input, update counters, and delegate to the internal implementation.</li> <li>Internal implementation: <code>IndexInternalConcurrent</code> (caller-thread execution,   thread-safe without global serialization).</li> <li>Async operations are provided by <code>IndexAsyncAdapter</code>; logging context by   <code>IndexContextLoggingAdapter</code>.</li> </ul> <p>Key classes: <code>segmentindex/SegmentIndex.java</code>, <code>segmentindex/IndexInternalConcurrent.java</code>, <code>segmentindex/IndexAsyncAdapter.java</code>, <code>segmentindex/IndexContextLoggingAdapter.java</code>.</p>"},{"location":"architecture/segmentindex/write-path/#optional-logging-context","title":"\ud83d\uddd2\ufe0f Optional Logging Context","text":"<p>If <code>IndexConfiguration.isContextLoggingEnabled()</code> is true, index operations populate the <code>index.name</code> MDC key so downstream logs can include the index identifier. This is purely for log correlation and does not write any additional files or provide durability.</p>"},{"location":"architecture/segmentindex/write-path/#unique-write-buffer-indexlevel","title":"\ud83e\uddf0 Unique Write Buffer (Index\u2011Level)","text":"<p>Every <code>put</code>/<code>delete</code> is first stored in an in\u2011memory unique cache that holds only the latest value per key and is flushed by internal policy.</p> <ul> <li>Structure: <code>UniqueCache</code> keyed by K with comparator ordering.</li> <li>Behavior:</li> <li>New write replaces any previous value for the same key.</li> <li>Reads consult this buffer first (read\u2011after\u2011write visibility without disk I/O).</li> <li>Deletes are represented as a tombstone value from the value type descriptor.</li> </ul> <p>Key classes: <code>cache/UniqueCache</code>, <code>segmentindex/SegmentIndexImpl#put</code>, <code>segmentindex/SegmentIndexImpl#delete</code>.</p>"},{"location":"architecture/segmentindex/write-path/#flush-and-routing-to-segments","title":"\ud83d\ude9a Flush and Routing to Segments","text":"<p>On flush, buffered entries are sorted and routed to target segments based on the key\u2011to\u2011segment map. Routing is incremental and batched per target segment for locality.</p> <p>Flow:</p> <p>1) Sort unique cache entries by key. 2) For each key, find the target segment id via <code>KeyToSegmentMap.insertKeyToSegment</code>. 3) Buffer entries to the current segment; when switching segments, write the batch to that segment\u2019s delta cache and continue. 4) After all entries are written, optionally split segments that exceed size thresholds. 5) Clear the unique buffer and flush the key\u2011segment map (if changed).</p> <p>Key classes: <code>segmentindex/CompactSupport</code>, <code>segmentindex/KeyToSegmentMap</code>, <code>segmentindex/SegmentSplitCoordinator</code>.</p>"},{"location":"architecture/segmentindex/write-path/#segment-delta-cache-files-transactional","title":"\ud83d\uddc2\ufe0f Segment Delta Cache Files (Transactional)","text":"<p>Writes land in a segment\u2019s delta cache as sorted key/value files. Each delta file is written transactionally:</p> <ul> <li>Data is written to <code>vNN-delta-NNNN.cache.tmp</code> and atomically renamed on commit.</li> <li>Segment properties track counts and delta file numbering.</li> <li>If the segment data is currently cached in memory, the delta cache is also updated in\u2011memory to keep reads fresh.</li> </ul> <p>Key classes: <code>segment/SegmentDeltaCacheWriter</code>, <code>segment/SegmentPropertiesManager</code>, <code>sorteddatafile/SortedDataFileWriterTx</code>.</p>"},{"location":"architecture/segmentindex/write-path/#ondisk-merge-compaction","title":"\ud83e\uddf9 On\u2011Disk Merge (Compaction)","text":"<p>Compaction merges the main SST with all delta cache files into a new consistent state and rebuilds auxiliary structures:</p> <ul> <li>Main SST (chunked file) written via <code>ChunkEntryFileWriter</code> and <code>ChunkStoreWriterTx</code>.</li> <li>Sparse index (\"scarce index\") updated every Nth key to accelerate seeks.</li> <li>Bloom filter rebuilt from keys to accelerate negative lookups.</li> <li>Delta cache is cleared on successful commit.</li> </ul> <p>Triggers:</p> <ul> <li>Opportunistic: after delta writes, if policy advises compaction.</li> <li>Forced: explicitly via <code>compact()</code> or before certain operations like splitting.</li> </ul> <p>Atomicity:</p> <ul> <li>All writers use temp files (<code>.tmp</code>) and <code>rename</code> to commit.</li> <li>Bloom filter writes inside a dedicated transaction (<code>BloomFilterWriterTx</code>).</li> </ul> <p>Key classes: <code>segment/SegmentCompacter</code>, <code>segment/SegmentFullWriterTx</code>, <code>segment/SegmentFullWriter</code>, <code>bloomfilter/BloomFilterWriterTx</code>, <code>scarceindex/*</code>.</p>"},{"location":"architecture/segmentindex/write-path/#segment-splitting","title":"\u2702\ufe0f Segment Splitting","text":"<p>When a segment grows beyond <code>maxNumberOfKeysInSegment</code>, the split coordinator computes a plan, optionally compacts first, and then splits into two segments. The key\u2011to\u2011segment map is updated with the new segment\u2019s max key.</p> <p>Key classes: <code>segmentindex/SegmentSplitCoordinator</code>, <code>segment/SegmentSplitter</code>, <code>segment/SegmentSplitterPlan</code>, <code>segmentindex/KeyToSegmentMap</code>.</p>"},{"location":"architecture/segmentindex/write-path/#delete-semantics-tombstones","title":"\ud83e\udea6 Delete Semantics (Tombstones)","text":"<p>Deletes write a tombstone value:</p> <ul> <li>Buffered in the unique cache and delta cache like any other update.</li> <li>During compaction, tombstones suppress older values and may be dropped if safe.</li> <li>Reads treat tombstones as absent.</li> </ul> <p>Key classes: <code>segmentindex/SegmentIndexImpl#delete</code>, <code>datatype/TypeDescriptor#getTombstone</code>, <code>segment/SegmentSearcher</code>.</p>"},{"location":"architecture/segmentindex/write-path/#durability-and-atomicity","title":"\ud83d\udcbe Durability and Atomicity","text":"<ul> <li>Transactional writers use a temp file + atomic rename to ensure either the old state or the new state is visible after a crash.</li> <li>SegmentIndex <code>close()</code> and explicit <code>flushAndWait()</code> drive persistence of buffered writes.</li> <li>Context logging is not a durability mechanism.</li> </ul>"},{"location":"architecture/segmentindex/write-path/#configuration-knobs-affecting-writes","title":"\u2699\ufe0f Configuration Knobs Affecting Writes","text":"<ul> <li><code>maxNumberOfKeysInSegmentWriteCache</code> \u2013 bounds in\u2011segment write cache size before flushing to delta files.</li> <li><code>maxNumberOfKeysInSegmentCache</code> \u2013 bounds total in\u2011segment cache size before compaction/split decisions.</li> <li><code>maxNumberOfKeysInSegmentChunk</code> \u2013 controls sparse index sampling cadence.</li> <li><code>maxNumberOfKeysInSegment</code> \u2013 split threshold per segment.</li> <li><code>bloomFilter*</code> \u2013 Bloom filter size/hash tuning.</li> <li><code>diskIoBufferSize</code> \u2013 I/O buffer sizing for on\u2011disk writers.</li> <li><code>encoding/decodingChunkFilters</code> \u2013 write/read pipelines (e.g., Snappy, CRC32, magic number).</li> </ul> <p>See: <code>segmentindex/IndexConfiguration</code> and <code>segmentindex/IndexConfigurationBuilder</code>.</p>"},{"location":"architecture/segmentindex/write-path/#integrity-filters-on-the-write-path","title":"\ud83d\udee1\ufe0f Integrity Filters on the Write Path","text":"<p>The chunk writer applies a filter pipeline when persisting chunk payloads:</p> <ul> <li>Magic number writing</li> <li>CRC32 computation</li> <li>Optional Snappy compression</li> </ul> <p>These produce a self\u2011describing chunk header and robust payload handling.</p> <p>Key classes: <code>chunkstore/ChunkProcessor</code>, <code>chunkstore/ChunkFilterMagicNumberWriting</code>, <code>chunkstore/ChunkFilterCrc32Writing</code>, <code>chunkstore/ChunkFilterSnappyCompress</code>.</p>"},{"location":"architecture/segmentindex/write-path/#sequence-put","title":"\ud83d\udd22 Sequence (Put)","text":"<p>1) <code>SegmentIndex.put(k,v)</code> \u2192 validate inputs; forbid direct tombstone values 2) Buffer latest <code>(k,v)</code> into unique cache (replaces any prior value for k) 3) If buffer over threshold \u2192 flushCache:    - Route sorted entries by key to segments    - For each target segment: write a new delta cache file (transactional)    - Optionally compact the segment and optionally split if too large    - Clear unique cache, flush key\u2011segment map</p>"},{"location":"architecture/segmentindex/write-path/#where-to-look-in-the-code","title":"\ud83e\udde9 Where to Look in the Code","text":"<ul> <li>SegmentIndex entry points and buffering: <code>src/main/java/org/hestiastore/index/segmentindex/SegmentIndexImpl.java</code></li> <li>Segment write/merge path: <code>src/main/java/org/hestiastore/index/segment/*</code></li> <li>Chunk store and filters: <code>src/main/java/org/hestiastore/index/chunkstore/*</code></li> <li>Delta and sorted file writers: <code>src/main/java/org/hestiastore/index/sorteddatafile/*</code></li> </ul> <p>For the read path and on\u2011disk layout, see the related pages:</p> <ul> <li>Read Path: <code>architecture/segmentindex/read-path.md</code></li> <li>On\u2011Disk Layout &amp; File Names: <code>architecture/segment/on-disk-layout.md</code></li> <li>Filters &amp; Integrity: <code>architecture/general/filters.md</code></li> </ul>"},{"location":"architecture/segmentindex/write-path/#related-glossary","title":"Related Glossary","text":"<ul> <li>Segment</li> <li>UniqueCache</li> <li>Delta Cache</li> <li>Flush</li> <li>Compaction</li> <li>Split</li> <li>Write Transaction</li> <li>Filters</li> <li>Tombstone</li> </ul>"},{"location":"configuration/","title":"\u2699\ufe0f Configuration","text":"<p>Don\u2019t be afraid to experiment\u2014if a configuration is missing or invalid, the SegmentIndex will fail fast, helping you catch issues early.</p> <p>The index is configured using the <code>IndexConfiguration</code> class. All essential index properties are configurable through the builder. See the example below:</p> <pre><code>IndexConfiguration&lt;Integer, Integer&gt; conf = IndexConfiguration\n    .&lt;Integer, Integer&gt;builder()//\n    .withKeyClass(Integer.class)//\n    .withValueClass(Integer.class)//\n    .withKeyTypeDescriptor(tdi) //\n    .withValueTypeDescriptor(tdi) //\n    .withMaxNumberOfKeysInSegment(4) //\n    .withMaxNumberOfKeysInSegmentCache(10L) //\n    .withMaxNumberOfKeysInSegmentIndexPage(2) //\n    .withBloomFilterIndexSizeInBytes(0) //\n    .withBloomFilterNumberOfHashFunctions(4) //\n    .withContextLoggingEnabled(false) //\n    .withName(\"test_index\") //\n    .build();\n\nSegmentIndex&lt;Integer, Integer&gt; index = SegmentIndex.&lt;Integer, Integer&gt;create(directory, conf);\n</code></pre> <p>Now let's look at particular parameters.</p>"},{"location":"configuration/#segmentindex-directory","title":"\ud83d\udcc1 SegmentIndex Directory","text":"<p>Place where all data are stored. There are two already prepared types:</p>"},{"location":"configuration/#in-memory","title":"\ud83e\udde0 In Memory","text":"<p>All data are stored in memory. It's created like this:</p> <pre><code>Directory directory = new MemDirectory();\n</code></pre> <p>It's usefull for testing purposes.</p>"},{"location":"configuration/#file-system","title":"\ud83d\udcbe File system","text":"<p>Its main purpose is to store index data in the file system. Create a file-system-based directory like this:</p> <pre><code>Directory directory = new FsDirectory(new File('my directory'));\n</code></pre>"},{"location":"configuration/#properties-of-indexconfiguration-class","title":"\ud83e\uddfe Properties of <code>IndexConfiguration</code> class","text":"<p>All properties are required and have the following meanings:</p>"},{"location":"configuration/#segmentindex-related-configuration","title":"\ud83e\uddf1 SegmentIndex related configuration","text":""},{"location":"configuration/#key-class-withkeyclass","title":"\ud83d\udd11 Key class - <code>withKeyClass()</code>","text":"<p>A <code>Class</code> object that represents the type of keys used in the index. Only instances of this class may be inserted. While any Java class is technically supported, it's recommended to use simple, compact types for performance reasons. Predefined classes are:</p> <ul> <li>Integer</li> <li>Long</li> <li>String</li> <li>Byte</li> </ul> <p>If a different class is used, the key type descriptor must be set using the <code>withKeyTypeDescriptor()</code> method from the builder. If you use a custom class, you must implement the <code>com.hestiastore.index.datatype.TypeDescriptor</code> interface to describe how the type is serialized and compared.</p>"},{"location":"configuration/#value-class-withvalueclass","title":"\ud83e\uddf2 Value class - <code>withValueClass()</code>","text":"<p>Required. Specifies the Java class used for values. The same rules that apply to the key class also apply to the value class.</p>"},{"location":"configuration/#segmentindex-name-withname","title":"\ud83c\udff7\ufe0f SegmentIndex name - <code>withName()</code>","text":"<p>Required. Assigns a logical name to the index. This can be useful in diagnostics and logging.</p>"},{"location":"configuration/#key-type-descriptor-withkeytypedescriptor","title":"\ud83e\udde9 Key type descriptor - <code>withKeyTypeDescriptor()</code>","text":"<p>Type descriptor for the key class. Required for non-default types.</p>"},{"location":"configuration/#value-type-descriptor-withvaluetypedescriptor","title":"\ud83e\udde9 Value type descriptor - <code>withValueTypeDescriptor()</code>","text":"<p>Type descriptor for the value class. Required for non-default types.</p>"},{"location":"configuration/#max-number-of-segments-in-cache-withmaxnumberofsegmentsincache","title":"\ud83e\uddf1 Max number of segments in cache - <code>withMaxNumberOfSegmentsInCache()</code>","text":"<p>Limits the number of segments stored in memory. Useful for controlling memory usage.</p>"},{"location":"configuration/#context-logging-enabled-withcontextloggingenabled","title":"\ud83d\uddd2\ufe0f Context logging enabled - <code>withContextLoggingEnabled()</code>","text":"<p>Controls whether the index wraps operations with MDC context propagation so log statements include the index name. When it's set on 'true' following loog message will contain set 'index' property:</p> <pre><code>&lt;Console name=\"indexAppender\" target=\"SYSTEM_OUT\"&gt;\n    &lt;PatternLayout\n        pattern=\"%d{ISO8601} %-5level [%t] index='%X{index.name}' %-C{1.mv}: %msg%n%throwable\" /&gt;\n&lt;/Console&gt;\n</code></pre> <p>Default value is 'true'.</p> <p>Please note, that in highly intensive applications enabling this option could eat up to 40% of CPU time.</p>"},{"location":"configuration/#segment-related-configuration","title":"\ud83e\udde9 Segment related configuration","text":""},{"location":"configuration/#max-number-of-keys-in-segment-withmaxnumberofkeysinsegment","title":"\ud83d\udccf Max number of keys in segment - <code>withMaxNumberOfKeysInSegment()</code>","text":"<p>Sets the maximum number of keys allowed in a single segment. Exceeding this splits the segment.</p>"},{"location":"configuration/#max-number-of-keys-in-segment-cache-withmaxnumberofkeysinsegmentcache","title":"\ud83d\uddc3\ufe0f Max number of keys in segment cache - <code>withMaxNumberOfKeysInSegmentCache()</code>","text":"<p>Defines how many keys can be cached from a segment during regular operation.</p>"},{"location":"configuration/#max-number-of-keys-in-segment-index-page-withmaxnumberofkeysinsegmentindexpage","title":"\ud83d\udcd1 Max number of keys in segment index page - <code>withMaxNumberOfKeysInSegmentIndexPage()</code>","text":"<p>Defines the number of keys in the index page for a segment. This impacts lookup efficiency.</p>"},{"location":"configuration/#bloom-filter-configuration","title":"\ud83c\udf38 Bloom filter configuration","text":"<p>A Bloom filter is a probabilistic data structure that efficiently tests whether an element is part of a set. You can find a detailed explanation on Wikipedia. In this context, each segment has its own Bloom filter.</p> <p>To disable bloom filter completle set:</p> <pre><code> .withBloomFilterIndexSizeInBytes(0)\n</code></pre> <p>The settings for the Bloom filter can be adjusted using the following methods:</p>"},{"location":"configuration/#bloom-filter-size-withbloomfilterindexsizeinbytes","title":"\ud83d\udce6 Bloom filter size - <code>withBloomFilterIndexSizeInBytes()</code>","text":"<p>Sets the size of the Bloom filter in bytes. A value of 0 disables the use of the Bloom filter.</p>"},{"location":"configuration/#number-of-hash-functions-withbloomfilternumberofhashfunctions","title":"\ud83d\udd22 Number of hash functions - <code>withBloomFilterNumberOfHashFunctions()</code>","text":"<p>Sets the number of hash functions used in the Bloom filter.</p>"},{"location":"configuration/#probability-of-false-positive-withbloomfilterprobabilityoffalsepositive","title":"\ud83d\udcc8 Probability of false positive - <code>withBloomFilterProbabilityOfFalsePositive()</code>","text":"<p>Sets the probability of false positives. When <code>get(someKey)</code> is called on a segment, the Bloom filter is checked to determine if the value is not in the segment. It can return <code>true</code>, indicating that the key could be in the segment. If the Bloom filter indicates the key is in the segment but it's not found, that's a false positive. The probability of this occurring is a value between 0 and 1.</p> <p>Usually, it's not necessary to adjust the Bloom filter settings.</p>"},{"location":"configuration/#changing-segmentindex-propertise","title":"\u270f\ufe0f Changing SegmentIndex propertise","text":"<p>Some parameters can be redefined when the index is opened.</p> <pre><code>SegmentIndex&lt;String, String&gt; index = SegmentIndex.&lt;String, String&gt;open(directory, conf);\n</code></pre> <p>At allows to pass <code>IndexConfiguration</code> object and this way change configuration parameters. Fllowing table shou parameters that can be changed.  </p> Name Meaning Can be changed Applies to indexName Logical name of the index \ud83d\udfe9 index keyClass Key class \ud83d\udfe5 index valueClass Value class \ud83d\udfe5 index keyTypeDescriptor Key class type descriptor \ud83d\udfe5 index valueTypeDescriptor Value class type descriptor \ud83d\udfe5 index maxNumberOfKeysInSegmentIndexPage Maximum keys in segment index page \ud83d\udfe5 segment maxNumberOfKeysInSegmentCache Maximum number of keys in segment cache \ud83d\udfe9 segment maxNumberOfKeysInSegmentWriteCache Maximum number of keys in segment write cache \ud83d\udfe9 segment maxNumberOfKeysInSegment Maximum keys in a segment \ud83d\udfe5 segment maxNumberOfSegmentsInCache Maximum number of segments in cache \ud83d\udfe9 index bloomFilterNumberOfHashFunctions Bloom filter - number of hash functions used \ud83d\udfe5 segment bloom filter bloomFilterIndexSizeInBytes Bloom filter - index size in bytes \ud83d\udfe5 segment bloom filter bloomFilterProbabilityOfFalsePositive Bloom filter - probability of false positives \ud83d\udfe5 segment bloom filter diskIoBufferSize Size of the disk I/O buffer \ud83d\udfe9 Disk IO contextLoggingEnabled If MDC-based context logging is enabled \ud83d\udfe9 index"},{"location":"configuration/#add-custom-data-type","title":"\u2795 Add custom data type","text":"<p>HestiaStore have to know how to work with new data type. So first is create implementatio of <code>com.hestiastore.index.datatype.TypeDescriptor</code>. Than during index creation set let index know about your implementation by <code>withKeyTypeDescriptor</code>. And it's done.</p>"},{"location":"configuration/data-types/","title":"\ud83e\udde9 Data types","text":"<p>HestiaStore supports a variety of data types for storing keys and values in a binary-efficient and consistent manner. Each data type is associated with a <code>TypeDescriptor</code>, which handles serialization, deserialization, comparison, and hashing logic.</p> <p>Below is a list of the supported data types and their characteristics.</p> Java Class TypeDescriptor Class Max Length (Bytes) Notes <code>java.lang.Byte</code> <code>TypeDescriptorByte</code> 1 Two's complement representation <code>java.lang.Integer</code> <code>TypeDescriptorInteger</code> 4 Big-endian encoding <code>java.lang.Long</code> <code>TypeDescriptorLong</code> 8 Big-endian encoding <code>java.lang.Float</code> <code>TypeDescriptorFloat</code> 4 IEEE 754 format <code>java.lang.Double</code> <code>TypeDescriptorDouble</code> 8 IEEE 754 format <code>java.lang.String</code> <code>TypeDescriptorShortString</code> 128 UTF-8 encoding, prefixed with 1-byte length, it's default string type descriptor <code>java.lang.String</code> <code>TypeDescriptorString</code> 2 GB UTF-8 encoding, prefixed with 4-byte length <code>org.hestiastore.index.datatype.ByteArray</code> <code>TypeDescriptorByteArray</code> n Raw bytes, length determined by actual data <code>org.hestiastore.index.datatype.NullValue</code> <code>TypeDescriptorNullValue</code> 0 Usefulll when value is not needed. Doesn't occupy any space. <code>org.hestiastore.index.datatype.CompositeValue</code> <code>TypeDescriptorCompositeValue</code> n Represents multiple values."},{"location":"configuration/data-types/#custom-data-types","title":"\ud83e\uddf0 Custom Data Types","text":"<p>HestiaStore allows advanced users to define custom <code>TypeDescriptor</code> implementations for handling specialized serialization strategies or complex types. Main usecases:</p> <ul> <li>Allows to store new data type.</li> <li>Introduce some specific encoding. it could lead to space saving.</li> <li>Limit data to some exact size</li> </ul> <p>To create a new data type:</p> <ol> <li>Implement the <code>TypeDescriptor&lt;T&gt;</code> interface. It's just a collection of simple interfaces which allows store data type to bytes and restore it from byte array</li> <li>Optionallly register it using <code>org.hestiastore.index.segmentindex.DataTypeDescriptorRegistry.addTypeDescriptor(Class, descriptor)</code>.</li> </ol>"},{"location":"configuration/data-types/#why-register-your-custom-type-descriptor","title":"\ud83d\udca1 Why Register Your Custom Type Descriptor","text":"<p>Registering your <code>TypeDescriptor</code> with <code>DataTypeDescriptorRegistry</code> is not mandatory, but it brings benefits:</p> <ul> <li>Simpler configuration defaults: When building an <code>IndexConfiguration</code> you can specify only <code>keyClass</code>/<code>valueClass</code>. The manager auto-fills <code>keyTypeDescriptor</code>/<code>valueTypeDescriptor</code> from the registry, so you don\u2019t have to wire descriptors everywhere.</li> <li>Consistency and validation: A single registry reduces mismatches between classes and descriptors. The configuration manager can validate and prevent accidental overriding of fixed properties because the descriptor identity is explicit.</li> <li>Future-proofing: Sharing an index between services or running offline maintenance works seamlessly as long as your descriptor type is on the classpath.</li> </ul> <p>Important: If you register using a <code>TypeDescriptor</code> instance, its class will be saved. That class must have a public no-args constructor (the system instantiates it reflectively). If this is not possible, you can register using the explicit class name overload <code>addTypeDescriptor(Class, String)</code>.</p>"},{"location":"configuration/data-types/#how-to-use-new-data-type","title":"\ud83e\uddea How to use new Data Type","text":"<p>During SegmentIndex configuration new data type descriptor can by directly used:</p> <pre><code>IndexConfiguration&lt;Integer, Integer&gt; conf = IndexConfiguration\n    .&lt;Integer, Integer&gt;builder()//\n    .withKeyClass(Integer.class)//\n    .withValueClass(MySuperDataType.class)//\n        ...\n    .withValueTypeDescriptor(new TypeDescriptorMySuperDataType()) //\n        ...\n    .build();\n\nSegmentIndex&lt;Integer, Integer&gt; index = SegmentIndex.&lt;Integer, Integer&gt;create(directory, conf);\n</code></pre>"},{"location":"configuration/data-types/#notes","title":"\ud83d\udcdd Notes","text":"<ul> <li>All numeric types use big-endian byte order for consistent sorting and comparison.</li> <li><code>ByteArray</code> is a wrapper class designed to support <code>equals()</code>, <code>hashCode()</code>, and lexicographic comparison. It can be used for binary blobs or hash digests.</li> </ul>"},{"location":"configuration/data-types/#equality-hashing-and-sorting-contracts","title":"\u2696\ufe0f Equality, Hashing, and Sorting Contracts","text":"<p>When you define custom key types (or use low-level binary types), it is critical that equality, hashing, and ordering behave consistently. HestiaStore relies on these properties to deduplicate updates in memory, to merge data from multiple sources, and to safely write strictly-increasing keys to disk.</p> <p>Why this matters</p> <ul> <li>In-memory caches coalesce multiple updates to the same key before flush. If the cache cannot recognize \u201cthe same key\u201d, duplicate keys can slip through.</li> <li>During flushing and compaction, keys are iterated in sorted order and written by <code>SortedDataFileWriter</code>. That writer requires strictly increasing keys. Two adjacent keys that are equal under the comparator will cause a hard failure.</li> <li>Merging/lookup structures assume a total and stable order; a non-transitive or unstable comparator leads to undefined behavior.</li> </ul> <p>Typical failure mode when the contract is broken</p> <ul> <li>Using raw <code>byte[]</code> as a key: <code>byte[]</code> uses reference equality and identity-based <code>hashCode()</code>. If your comparator compares by content (e.g., lexicographically), then two different <code>byte[]</code> instances with the same content are NOT equal to <code>HashMap</code>, but ARE equal to the comparator. The cache keeps both, sorts them, and the writer sees two adjacent equal keys and throws an exception like:</li> <li>\"Attempt to insert the same key as previous. Key(Base64)='\u2026'\"</li> </ul> <p>Contracts to follow</p> <ul> <li>equals/hashCode consistency: If <code>a.equals(b)</code> is <code>true</code>, then <code>a.hashCode() == b.hashCode()</code> must also be <code>true</code>.</li> <li>Comparator total order: The comparator returned by your <code>TypeDescriptor#getComparator()</code> must be total (anti-symmetric, transitive, and consistent), and must define the uniqueness of keys.</li> <li>Comparator consistency with equality: If the system uses hash-based maps anywhere for keys of type <code>K</code>, ensure that <code>compare(a, b) == 0</code> implies <code>a.equals(b)</code>. This avoids having one part of the system see two objects as the same key while another part sees them as different. If you cannot satisfy this for a given JVM type (e.g., raw <code>byte[]</code>), wrap it in a type that does (e.g., <code>ByteArray</code>).</li> <li>Stable order vs. encoding: Keep the comparator consistent with the intended semantics of your keys and their encoding. For numbers encoded big-endian, natural numeric order is the correct comparator.</li> </ul> <p>Practical guidance</p> <ul> <li>Do NOT use raw <code>byte[]</code> as keys. Use <code>org.hestiastore.index.datatype.ByteArray</code> or another wrapper that implements content-based <code>equals()</code>, <code>hashCode()</code>, and a lexicographic <code>compareTo</code>.</li> <li>For fixed-length binary keys, prefer a wrapper or a fixed-length <code>String</code> (<code>TypeDescriptorFixedLengthString</code>) if your domain allows it.</li> <li>Avoid lossy comparators (e.g., comparing only a prefix) unless that prefix truly defines key identity. Otherwise, distinct keys will be treated as equal under the comparator and collapse unintentionally.</li> <li>Ensure your comparator never depends on mutable or external state; it must be stable for the lifetime of the index.</li> </ul> <p>How to self-check</p> <ul> <li>Property checks you can add to tests when introducing a new key type <code>K</code> and comparator <code>cmp</code>:</li> <li>For many random pairs <code>(a, b)</code>: if <code>cmp.compare(a, b) == 0</code>, then assert <code>a.equals(b)</code> and <code>a.hashCode() == b.hashCode()</code>.</li> <li>For many triples <code>(a, b, c)</code>: assert transitivity: if <code>cmp.compare(a, b) &lt;= 0</code> and <code>cmp.compare(b, c) &lt;= 0</code> then <code>cmp.compare(a, c) &lt;= 0</code>.</li> <li>Round-trip through caches: insert duplicates under the comparator, ensure only the latest value is kept, and iteration returns strictly increasing unique keys.</li> </ul> <p>Example: byte[] vs. ByteArray</p> <ul> <li>Bad (will fail): using <code>byte[]</code> as keys with a content comparator. Two different arrays with the same content will be considered different by a hash-based map but equal by the comparator, leading to duplicate adjacent keys during write.</li> <li>Good: using <code>ByteArray</code> as keys. <code>ByteArray</code> implements content-based <code>equals()</code>, <code>hashCode()</code>, and lexicographic <code>compareTo</code>, so all parts of the system agree on key identity and ordering.</li> </ul> <p>By following these contracts, you ensure that updates are correctly deduplicated, merges are deterministic, and on-disk files maintain the strict key ordering required for safe reads and compactions.</p>"},{"location":"configuration/logging/","title":"\ud83d\uddd2\ufe0f Logging","text":"<p>HestiaStore uses slf4j for internal logging. So you should include your preferred logging library like logback, log4j, or another with a bridge to slf4j. In case you use log4j, look at the example configuration:</p>"},{"location":"configuration/logging/#example-configuration-file-for-log4j","title":"\ud83d\udcdc Example configuration File for log4j","text":"<p>Bellow is the example Log4j2 configuration used in HestiaStore:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;Configuration status=\"INFO\"&gt;\n  &lt;Appenders&gt;\n\n    &lt;Console name=\"Console\" target=\"SYSTEM_OUT\"&gt;\n      &lt;PatternLayout\n          pattern=\"%style{%d{ISO8601}}{white} %highlight{%-5level }[%style{%t}{bright,blue}] %style{%-C{1.mv}}{bright,yellow}: %msg%n%throwable\" /&gt;\n    &lt;/Console&gt;\n\n    &lt;Console name=\"indexAppender\" target=\"SYSTEM_OUT\"&gt;\n      &lt;PatternLayout\n          pattern=\"%style{%d{ISO8601}}{white} %highlight{%-5level }[%style{%t}{bright,blue}] index='%style{%X{index.name}}{magenta}' %style{%-C{1.mv}}{bright,yellow}: %msg%n%throwable\" /&gt;\n    &lt;/Console&gt;\n\n  &lt;/Appenders&gt;\n  &lt;Loggers&gt;\n    &lt;logger name=\"org.hestiastore.index\" level=\"DEBUG\" additivity=\"false\"&gt;\n      &lt;appender-ref ref=\"indexAppender\" /&gt;\n    &lt;/logger&gt;\n    &lt;Root level=\"DEBUG\"&gt;\n      &lt;AppenderRef ref=\"Console\"/&gt;\n    &lt;/Root&gt;\n  &lt;/Loggers&gt;\n&lt;/Configuration&gt;\n</code></pre> <p>this example will produce logs look like this:</p> <p></p>"},{"location":"configuration/logging/#log-appenders","title":"\ud83d\udd0c Log Appenders","text":"<ul> <li>Console (default): Used by all components not explicitly assigned a logger. Outputs time, level, thread, and class name.</li> <li>indexAppender: Specifically configured for <code>org.hestiastore.index</code>, outputs additional context (<code>index.name</code>) for disambiguating messages from different index instances.</li> </ul>"},{"location":"configuration/logging/#customizing-logging-levels","title":"\ud83c\udf9a\ufe0f Customizing Logging Levels","text":"<p>You can control verbosity by modifying the <code>&lt;logger&gt;</code> or <code>&lt;Root&gt;</code> levels:</p> <ul> <li><code>ERROR</code>, <code>WARN</code>, <code>INFO</code>, <code>DEBUG</code>, or <code>TRACE</code></li> <li>For example, to suppress general debug logs:</li> </ul> <pre><code>&lt;Root level=\"INFO\"&gt;\n</code></pre>"},{"location":"configuration/logging/#disabling-segmentindex-logs","title":"\ud83d\udeab Disabling SegmentIndex Logs","text":"<p>If you want to disable index-specific logging entirely, remove or comment out the <code>org.hestiastore.index</code> logger section. Alternatively log level for package could be set to \"ERROR\".</p>"},{"location":"configuration/logging/#recommendations","title":"\ud83d\udca1 Recommendations","text":"<ul> <li>Use <code>DEBUG</code> during development or troubleshooting.</li> <li>Switch to <code>INFO</code> or <code>WARN</code> in production to reduce log noise.</li> <li>Ensure you clear MDC values (<code>ThreadContext.clearAll()</code>) in thread pools to prevent memory leaks or incorrect context reuse.</li> </ul>"},{"location":"configuration/logging/#logging-implementation","title":"\ud83e\udde9 Logging Implementation","text":"<p>The <code>indexAppender</code> uses a mapped diagnostic context (MDC) value <code>index.name</code>, which should be set programmatically:</p> <pre><code>ThreadContext.put(\"index.name\", \"userIndex\");\n</code></pre> <p>This allows the log output to include which index instance the message is referring to, aiding in debugging concurrent access or behavior across multiple indexes.</p>"},{"location":"development/","title":"\ud83d\udee0\ufe0f Development","text":"<p>This page links development-focused documentation.</p>"},{"location":"development/#topics","title":"Topics","text":"<ul> <li>Documentation</li> <li>Guides</li> <li>Segment API</li> <li>JVM Profiling</li> <li>Release Process</li> <li>Refactor Backlog</li> </ul>"},{"location":"development/documentation/","title":"\ud83d\udcda Documentation","text":"<p>Documentation is available in the following locations:</p> <ul> <li><code>mvn site</code> - Includes detailed project reports from PMC, checkstyle, JUnit test line coverage, and Javadocs. It's not published.</li> <li>GitHub project - Simple technical development oriented site</li> <li>HestiaStore.org site - Detailed, user-focused information</li> </ul> <p>Following text is about HestiaStore.org site documentation.</p>"},{"location":"development/documentation/#refactor-backlog","title":"Refactor backlog","text":"<p>The project-wide refactor backlog lives in <code>docs/refactor-backlog.md</code>. Use it for planned refactors and refactor-related defects, and keep items in the defined status sections.</p>"},{"location":"development/documentation/#how-to-make-changes-to-hestiastoreorg","title":"How to make changes to HestiaStore.org","text":"<p>HestiaStore.org site documentation is served from the main project branch <code>gh-pages</code>. Publishing a new site version involves generating HTML from Markdown and pushing it to <code>gh-pages</code>.</p> <p>Prerequisites:</p> <ul> <li>Installed git</li> <li>Site generating tool mkdocs-material - as described at https://squidfunk.github.io/mkdocs-material/. It is user-friendly and easy to work with. In case of MacOS install it with:</li> </ul> <p><pre><code>brew install mkdocs-material\n</code></pre> * Some Markdown editor of your choice * GitHub personal access token with permission to read and write project pages.</p>"},{"location":"development/documentation/#page-editing-and-viewing-documentation-locally","title":"Page editing and viewing documentation locally","text":"<p>From project checkout branch <code>docs</code>, there are all source files for main site. Markdown files for documentation are located in the directory <code>docs</code>. To preview documentation changes locally, run:</p> <pre><code>mkdocs serve\n</code></pre> <p>Now at http://127.0.0.1:8080/HestiaStore/ should display the documentation.</p> <p>The <code>mkdocs.yml</code> file in the root directory controls site structure, navigation, and theme. For more information see mkdocs-material documentation.</p>"},{"location":"development/documentation/#how-to-publish-changes-at-hestiastoreorg","title":"How to publish changes at hestiastore.org","text":"<ul> <li>From github.com/jajir/HestiaStore checkout branch <code>docs</code>. </li> <li>Make changes</li> <li>Commit changes to <code>docs</code></li> <li>Pull again to be sure, that latest chnages from branch <code>gh-pages</code> is at local.</li> <li>Then, run the following command locally:</li> </ul> <p><pre><code>mkdocs gh-deploy\n</code></pre>   In a few minutes (could be 15 minutes) new documentation will be published.</p>"},{"location":"development/guides/","title":"\ud83d\udee0\ufe0f Development","text":"<p>Here are some development related topiscs.</p>"},{"location":"development/guides/#how-to-run-jmh-benchmarks","title":"\ud83e\uddea How to run JMH benchmarks","text":"<p>Follow this steps: * Compile whole project and create pacakge containing all benchmarks data * Go to <code>jmh-benchmarks</code> * Execute it, with temp directory in <code>target</code> directory</p> <pre><code>mvn clean install\ncd jmh-benchmarks\njava -Ddir=./target/ -jar target/jmh-benchmarks.jar\n</code></pre> <p>Specific JMH benchmark class could be run:</p> <pre><code>java -Ddir=./target/ -jar target/jmh-benchmarks.jar SegmentSearchBenchmark\n</code></pre> <p>result could look like:</p> <pre><code>Benchmark                                             Mode  Cnt    Score      Error  Units\nSequentialFileReadingBenchmark.test_with_buffer_01KB  avgt    4   70.747 \u00b1   42.480  ms/op\nSequentialFileReadingBenchmark.test_with_buffer_02KB  avgt    4   60.009 \u00b1   52.899  ms/op\nSequentialFileReadingBenchmark.test_with_buffer_04KB  avgt    4   51.254 \u00b1   30.112  ms/op\nSequentialFileReadingBenchmark.test_with_buffer_08KB  avgt    4   48.600 \u00b1   28.892  ms/op\nSequentialFileReadingBenchmark.test_with_buffer_16KB  avgt    4   48.471 \u00b1   25.665  ms/op\nSequentialFileReadingBenchmark.test_with_buffer_32KB  avgt    4   45.256 \u00b1   24.986  ms/op\nSequentialFileReadingBenchmark.test_with_buffer_64KB  avgt    4   45.204 \u00b1   24.867  ms/op\nSequentialFileWritingBenchmark.test_with_buffer_01KB  avgt    4  238.075 \u00b1   75.311  ms/op\nSequentialFileWritingBenchmark.test_with_buffer_02KB  avgt    4  271.272 \u00b1   64.747  ms/op\nSequentialFileWritingBenchmark.test_with_buffer_04KB  avgt    4  276.001 \u00b1   45.815  ms/op\nSequentialFileWritingBenchmark.test_with_buffer_08KB  avgt    4  352.189 \u00b1 1140.814  ms/op\nSequentialFileWritingBenchmark.test_with_buffer_16KB  avgt    4  258.806 \u00b1   44.693  ms/op\nSequentialFileWritingBenchmark.test_with_buffer_32KB  avgt    4  276.246 \u00b1  135.019  ms/op\nSequentialFileWritingBenchmark.test_with_buffer_64KB  avgt    4  275.944 \u00b1  128.835  ms/op\n</code></pre> <p>When some JMH benchmark class is changed command <code>mvn package</code> have to be run.</p>"},{"location":"development/guides/#possible-problems","title":"\u26a0\ufe0f Possible problems","text":"<p>Generally JMH is quite fragile. Small changes broke JMH benchmark execution. Usually helps rebuild project and start again as described above.</p>"},{"location":"development/guides/#load-test","title":"\ud83e\udded Load test","text":"<p>Runnign JVM should be inspected with some profiller. For profilling is usefull to hae long running task to watch it. Go to project <code>load-test</code>. Following command show all optional parameters:</p> <pre><code>java -jar target/load-test.jar com.coroptis.index.loadtest.Main --help\n</code></pre> <p>Theer are two main supported operations. First is data generating. It's could be usefull to place in java profilling agent. It could look like:</p> <pre><code>java \\\n    -agentpath:/Applications/YourKit-Java-Profiler-2023.9.app/Contents/Resources/bin/mac/libyjpagent.dylib=exceptions=disable,delay=10000,listen=all \\\n    -jar target/load-test.jar com.coroptis.index.loadtest.Main \\\n    --write \\\n    --directory /Volumes/LaCie/test/  \\\n    --count 5_000_000_000 \\\n    --max-number-of-keys-in-cache 5_000_000 \\\n    --max-number-of-keys-in-segment 10_000_000 \\\n    --max-number-of-keys-in-segment-cache 500_000 \\\n    --max-number-of-keys-in-segment-cache-during-flushing 2_000_000 \\\n    --max-number-of-keys-in-segment-index-page 1_000 \\\n    --bloom-filter-index-size-in-bytes 10_000_000 \\\n    --bloom-filter-number-of-hash-functions 2\n</code></pre> <p>It will generate 210 GB of testing data. Furst search test can be performed like this:</p> <pre><code>java \\\n    -agentpath:/Applications/YourKit-Java-Profiler-2023.9.app/Contents/Resources/bin/mac/libyjpagent.dylib=exceptions=disable,delay=10000,listen=all \\\n    -jar target/load-test.jar com.coroptis.index.loadtest.Main \\\n    --search \\\n    --directory /Volumes/LaCie/test/  \\\n    --count 5_000_000_000 \\\n    --max-key 5_000_000_000 \\\n    --max-number-of-keys-in-cache 5_000_000 \\\n    --max-number-of-keys-in-segment 10_000_000 \\\n    --max-number-of-keys-in-segment-cache 500_000 \\\n    --max-number-of-keys-in-segment-cache-during-flushing 2_000_000 \\\n    --max-number-of-keys-in-segment-index-page 1_000 \\\n    --bloom-filter-index-size-in-bytes 10_000_000 \\\n    --bloom-filter-number-of-hash-functions 2\n</code></pre>"},{"location":"development/guides/#development_1","title":"Development","text":"<p>Mockito requires reflective access to non-public parts in a Java module. It could be manually open by passing following parameter as jvm parameter:</p> <pre><code>--add-opens=java.base/java.lang=ALL-UNNAMED\n</code></pre>"},{"location":"development/guides/#how-to-get-segment-disk-size","title":"How to get segment disk size","text":"<p>On apple try:</p> <pre><code>diskutil  info /Volumes/LaCie\n</code></pre>"},{"location":"development/profiler-stacktrace/","title":"\ud83d\udd2c JVM profiler results (YourKit)","text":"<p>This page summarizes a profiling session focused on read and write performance. The goal was to identify where CPU time is spent and highlight concrete improvements.</p> <p>The workload: a separate generator produced roughly 100,000,000 key\u2013value entries and then executed read-heavy operations against HestiaStore 0.0.5 while YourKit captured CPU samples.</p> <p>Test environment: run on a Mac mini on 24.10.2025.</p> <p></p>"},{"location":"development/profiler-stacktrace/#how-to-read-the-numbers","title":"\ud83e\udded How to read the numbers","text":"<p>Percentages shown below approximate the share of total CPU time across the whole run spent in each operation/stack. For example, \u201c40% byte array manipulations\u201d means about 40% of all CPU cycles were consumed in copying/transforming byte arrays end-to-end.</p>"},{"location":"development/profiler-stacktrace/#key-findings-high-level","title":"\ud83d\udcc8 Key findings (high level)","text":"<ul> <li>~40% in byte array manipulation (ultimately <code>System.arraycopy</code>) across several layers while moving data between buffers and chunks.</li> <li>~21% in sequential reads through stacked streams and iterators, indicating many small reads and buffer boundaries.</li> <li>~18% during stream/channel closing (clean-up cascades), suggesting repeated finalization work per access.</li> <li>~3% in file open syscalls while creating new channels for short-lived reads.</li> </ul> <p>These four areas dominate the observed CPU budget for the scenario above.</p>"},{"location":"development/profiler-stacktrace/#detailed-stacks-and-context","title":"\ud83e\uddf5 Detailed stacks and context","text":""},{"location":"development/profiler-stacktrace/#40-byte-array-manipulation","title":"~40%: byte array manipulation","text":"<p>The hot paths converge on <code>System.arraycopy</code>, coming from multiple places in the chunk/data-block pipeline. This usually indicates extra copying between intermediate buffers.</p>"},{"location":"development/profiler-stacktrace/#3-file-open-overhead","title":"~3%: file open overhead","text":"<p>Opening files repeatedly costs ~3% CPU. Consider reusing channels across related reads to reduce syscalls and JNI transitions.</p> <pre><code>sun.nio.fs.UnixNativeDispatcher.open0(Native Method)\nsun.nio.fs.UnixNativeDispatcher.open(UnixNativeDispatcher.java:72)\nsun.nio.fs.UnixChannelFactory.open(UnixChannelFactory.java:258)\nsun.nio.fs.UnixChannelFactory.newFileChannel(UnixChannelFactory.java:133)\nsun.nio.fs.UnixChannelFactory.newFileChannel(UnixChannelFactory.java:146)\nsun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:259)\njava.nio.file.Files.newByteChannel(Files.java:380)\njava.nio.file.Files.newByteChannel(Files.java:432)\njava.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:420)\njava.nio.file.Files.newInputStream(Files.java:160)\norg.hestiastore.index.directory.FsFileReaderStream.&lt;init&gt;(FsFileReaderStream.java:22)\norg.hestiastore.index.directory.FsDirectory.getFileReader(FsDirectory.java:29)\norg.hestiastore.index.datablockfile.DataBlockFile.getFileReader(DataBlockFile.java:69)\norg.hestiastore.index.datablockfile.DataBlockFile.openReader(DataBlockFile.java:61)\norg.hestiastore.index.chunkstore.ChunkStoreFile.openReader(ChunkStoreFile.java:52)\norg.hestiastore.index.chunkentryfile.ChunkEntryFile.openIteratorAtPosition(ChunkEntryFile.java:42)\norg.hestiastore.index.segment.SegmentIndexSearcher.search(SegmentIndexSearcher.java:43)\norg.hestiastore.index.segment.SegmentSearcher.get(SegmentSearcher.java:62)\norg.hestiastore.index.segment.SegmentImpl.get(SegmentImpl.java:166)\norg.hestiastore.index.segmentindex.SegmentIndexImpl.get(SegmentIndexImpl.java:163)\norg.hestiastore.index.segmentindex.SegmentIndexContextLoggingAdapter.get(IndexContextLoggingAdapter.java:46)\ncom.coroptis.counting.CommandCount.lambda$countBoard$0(CommandCount.java:102)\ncom.coroptis.counting.CommandCount$$Lambda.0x000000080023f0b0.accept()\njava.util.ArrayList.forEach(ArrayList.java:1596)\ncom.coroptis.counting.CommandCount.countBoard(CommandCount.java:99)\ncom.coroptis.counting.CommandCount.lambda$computeNewStates$0(CommandCount.java:82)\ncom.coroptis.counting.CommandCount$$Lambda.0x000000080023e3c0.accept()\njava.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)\njava.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)\norg.hestiastore.index.segmentindex.EntryIteratorToSpliterator.tryAdvance(EntryIteratorToSpliterator.java:31)\njava.util.Spliterator.forEachRemaining(Spliterator.java:332)\njava.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)\njava.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)\njava.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)\njava.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)\njava.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\njava.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)\ncom.coroptis.counting.CommandCount.computeNewStates(CommandCount.java:81)\ncom.coroptis.counting.CommandCount.countRound(CommandCount.java:48)\ncom.coroptis.counting.Main.main(Main.java:132)\n</code></pre>"},{"location":"development/profiler-stacktrace/#21-sequential-reads-through-layered-streams","title":"~21%: sequential reads through layered streams","text":"<p>Reading through <code>BufferedInputStream</code> and custom readers accumulates overhead from many small reads and object boundaries.</p> <pre><code>sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java)\nsun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)\njava.io.BufferedInputStream.read1(BufferedInputStream.java:345)\njava.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\njava.io.BufferedInputStream.read(BufferedInputStream.java:399)\njava.io.FilterInputStream.read(FilterInputStream.java:95)\norg.hestiastore.index.directory.FsFileReaderStream.read(FsFileReaderStream.java:51)\norg.hestiastore.index.datablockfile.DataBlockReaderImpl.read(DataBlockReaderImpl.java:35)\norg.hestiastore.index.datablockfile.DataBlockReaderImpl.read(DataBlockReaderImpl.java:12)\norg.hestiastore.index.datablockfile.DataBlockByteReaderImpl.moveToNextDataBlock(DataBlockByteReaderImpl.java:84)\norg.hestiastore.index.datablockfile.DataBlockByteReaderImpl.optionalyMoveToNextDataBlock(DataBlockByteReaderImpl.java:79)\norg.hestiastore.index.datablockfile.DataBlockByteReaderImpl.readExactly(DataBlockByteReaderImpl.java:71)\norg.hestiastore.index.chunkstore.ChunkData.read(ChunkData.java:85)\norg.hestiastore.index.chunkstore.ChunkStoreReaderImpl.read(ChunkStoreReaderImpl.java:42)\norg.hestiastore.index.chunkstore.ChunkStoreReaderImpl.read(ChunkStoreReaderImpl.java:13)\norg.hestiastore.index.chunkentryfile.ChunkEntryFileIterator.moveToNextChunk(ChunkEntryFileIterator.java:81)\norg.hestiastore.index.chunkentryfile.ChunkEntryFileIterator.&lt;init&gt;(ChunkEntryFileIterator.java:42)\norg.hestiastore.index.chunkentryfile.ChunkEntryFile.openIteratorAtPosition(ChunkEntryFile.java:42)\norg.hestiastore.index.segment.SegmentIndexSearcher.search(SegmentIndexSearcher.java:43)\norg.hestiastore.index.segment.SegmentSearcher.get(SegmentSearcher.java:62)\norg.hestiastore.index.segment.SegmentImpl.get(SegmentImpl.java:166)\norg.hestiastore.index.segmentindex.SegmentIndexImpl.get(SegmentIndexImpl.java:163)\norg.hestiastore.index.segmentindex.SegmentIndexContextLoggingAdapter.get(IndexContextLoggingAdapter.java:46)\ncom.coroptis.counting.CommandCount.lambda$countBoard$0(CommandCount.java:102)\ncom.coroptis.counting.CommandCount$$Lambda.0x000000080023f0b0.accept()\njava.util.ArrayList.forEach(ArrayList.java:1596)\ncom.coroptis.counting.CommandCount.countBoard(CommandCount.java:99)\ncom.coroptis.counting.CommandCount.lambda$computeNewStates$0(CommandCount.java:82)\ncom.coroptis.counting.CommandCount$$Lambda.0x000000080023e3c0.accept()\njava.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)\njava.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)\norg.hestiastore.index.segmentindex.EntryIteratorToSpliterator.tryAdvance(EntryIteratorToSpliterator.java:31)\njava.util.Spliterator.forEachRemaining(Spliterator.java:332)\njava.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)\njava.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)\njava.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)\njava.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)\njava.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\njava.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)\ncom.coroptis.counting.CommandCount.computeNewStates(CommandCount.java:81)\ncom.coroptis.counting.CommandCount.countRound(CommandCount.java:48)\ncom.coroptis.counting.Main.main(Main.java:132)\n</code></pre>"},{"location":"development/profiler-stacktrace/#18-closecleanup-cascades","title":"~18%: close/cleanup cascades","text":"<p>Closing nested readers triggers multiple cleaner/finalization steps. Reusing readers or collapsing the close hierarchy could help.</p> <pre><code>java.io.FileDescriptor.close0(Native Method)\njava.io.FileDescriptor.close(FileDescriptor.java:304)\njava.io.FileDescriptor$1.close(FileDescriptor.java:89)\nsun.nio.ch.FileChannelImpl$Closer.run(FileChannelImpl.java:116)\njdk.internal.ref.CleanerImpl$PhantomCleanableRef.performCleanup(CleanerImpl.java:178)\njdk.internal.ref.PhantomCleanable.clean(PhantomCleanable.java:133)\nsun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:210)\njava.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:113)\nsun.nio.ch.ChannelInputStream.close(ChannelInputStream.java:312)\njava.io.BufferedInputStream.close(BufferedInputStream.java:618)\norg.hestiastore.index.directory.FsFileReaderStream.doClose(FsFileReaderStream.java:33)\norg.hestiastore.index.AbstractCloseableResource.close(AbstractCloseableResource.java:23)\norg.hestiastore.index.datablockfile.DataBlockReaderImpl.doClose(DataBlockReaderImpl.java:29)\norg.hestiastore.index.AbstractCloseableResource.close(AbstractCloseableResource.java:23)\norg.hestiastore.index.datablockfile.DataBlockByteReaderImpl.doClose(DataBlockByteReaderImpl.java:48)\norg.hestiastore.index.AbstractCloseableResource.close(AbstractCloseableResource.java:23)\norg.hestiastore.index.chunkstore.ChunkStoreReaderImpl.doClose(ChunkStoreReaderImpl.java:36)\norg.hestiastore.index.AbstractCloseableResource.close(AbstractCloseableResource.java:23)\norg.hestiastore.index.chunkentryfile.ChunkEntryFileIterator.doClose(ChunkEntryFileIterator.java:96)\norg.hestiastore.index.AbstractCloseableResource.close(AbstractCloseableResource.java:23)\norg.hestiastore.index.segment.SegmentIndexSearcher.search(SegmentIndexSearcher.java:59)\norg.hestiastore.index.segment.SegmentSearcher.get(SegmentSearcher.java:62)\norg.hestiastore.index.segment.SegmentImpl.get(SegmentImpl.java:166)\norg.hestiastore.index.segmentindex.SegmentIndexImpl.get(SegmentIndexImpl.java:163)\norg.hestiastore.index.segmentindex.SegmentIndexContextLoggingAdapter.get(IndexContextLoggingAdapter.java:46)\ncom.coroptis.counting.CommandCount.lambda$countBoard$0(CommandCount.java:102)\ncom.coroptis.counting.CommandCount$$Lambda.0x000000080023f0b0.accept()\njava.util.ArrayList.forEach(ArrayList.java:1596)\ncom.coroptis.counting.CommandCount.countBoard(CommandCount.java:99)\ncom.coroptis.counting.CommandCount.lambda$computeNewStates$0(CommandCount.java:82)\ncom.coroptis.counting.CommandCount$$Lambda.0x000000080023e3c0.accept()\njava.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)\njava.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)\norg.hestiastore.index.segmentindex.EntryIteratorToSpliterator.tryAdvance(EntryIteratorToSpliterator.java:31)\njava.util.Spliterator.forEachRemaining(Spliterator.java:332)\njava.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)\njava.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)\njava.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)\njava.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)\njava.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\njava.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)\ncom.coroptis.counting.CommandCount.computeNewStates(CommandCount.java:81)\ncom.coroptis.counting.CommandCount.countRound(CommandCount.java:48)\ncom.coroptis.counting.Main.main(Main.java:132)\n</code></pre>"},{"location":"development/profiler-stacktrace/#what-to-improve-next-actionable","title":"\ud83d\udee0\ufe0f What to improve next (actionable)","text":"<ul> <li>Reduce copies in the read path<ul> <li>Pool and reuse byte buffers across <code>ChunkStoreReader</code>/<code>ChunkEntryFileIterator</code>.</li> <li>Where feasible, write directly into the final consumer\u2019s buffer instead of staging arrays.</li> </ul> </li> <li>Fewer, larger IO operations<ul> <li>Increase internal buffer sizes; align chunk/page boundaries to reduce partial reads.</li> <li>Consider a shared channel with a single buffering layer to avoid stacking multiple <code>BufferedInputStream</code>s.</li> </ul> </li> <li>Tame cleanup overhead<ul> <li>Scope readers over a batch of gets to amortize <code>close()</code> and cleaner activity.</li> <li>Ensure try-with-resources closes only once at the highest level; avoid redundant closes in nested layers.</li> </ul> </li> <li>Cut file-open churn<ul> <li>Cache open channels per data file with reference counting; close when idle for a period.</li> <li>If feasible, pre-open frequently accessed files at segment initialization.</li> </ul> </li> </ul> <p>These changes should collectively reclaim the majority of the observed CPU time in this profile.</p>"},{"location":"development/release/","title":"\ud83d\ude80 Releasing a New Version","text":"<p>This is a step-by-step guide for making a new HestiaStore release.</p>"},{"location":"development/release/#versioning-of-the-project","title":"\u261d\ufe0f Versioning of the project","text":"<p>The project uses the traditional versioning pattern known as Semantic Versioning, detailed at https://semver.org. The version number consists of three components separated by dots:</p> <pre><code>0.3.6\n</code></pre> <p>Each number has the following meaning:</p> <ul> <li><code>0</code> - Major project version. Project API could be incompatible between two major versions.</li> <li><code>3</code> - Minor project version. Contains changes in features, performance optimizations, and small improvements. Minor versions should be compatible.</li> <li><code>6</code> - Patch version. Bug fixing project release.</li> </ul> <p>There are also snapshot versions with version number <code>0.3.6-SNAPSHOT</code>. Snapshot versions should not be stored in the Maven repository.</p>"},{"location":"development/release/#branching-strategy","title":"\ud83d\udd4a\ufe0f Branching strategy","text":"<p>We use a simplified GitHub Flow:</p> <ul> <li><code>main</code>: the primary development and release branch. Small changes may be committed directly to <code>main</code>, while larger or experimental features must be developed in a separate branch and merged via pull request.</li> <li>Feature branches are created from <code>main</code> for larger or isolated changes. Use descriptive names like <code>feature/compression</code>, <code>fix/index-scan</code>, etc.</li> </ul> <p>The deprecated <code>devel</code> branch has been removed and is no longer used.</p>"},{"location":"development/release/#release-prerequisites","title":"\ud83e\uddd1\u200d\ud83d\udcbb Release prerequisites","text":"<p>The release will be published to Maven Central. Release configuration secrets are placed at the Maven settings file <code>~/.m2/settings.xml</code>. Adjust <code>settings.xml</code> in <code>~/.m2/settings.xml</code> as described in GitHub's official documentation on how to work with the GitHub Maven repository. Generate a valid token and you are done.</p>"},{"location":"development/release/#provide-correct-package-signature","title":"Provide correct package signature","text":"<p>In your <code>~/.m2/settings.xml</code> file, add the following section:</p> <pre><code>&lt;settings&gt;\n    ...\n   &lt;profile&gt;\n     &lt;id&gt;release&lt;/id&gt;\n       &lt;properties&gt;\n       &lt;gpg.executable&gt;gpg&lt;/gpg.executable&gt;\n       &lt;gpg.passphrase&gt;--pgp-password--&lt;/gpg.passphrase&gt;\n     &lt;/properties&gt;      \n   &lt;/profile&gt;\n    ...\n&lt;/settings&gt;\n</code></pre>"},{"location":"development/release/#setup-maven-central-accout-secrets","title":"Setup maven central accout secrets","text":"<p>This provides <code>org.sonatype.central:central-publishing-maven-plugin</code> plugin secrets to enable login to the Maven Central account where release data will be placed. You must have an account with a verified namespace <code>org.hestiastore</code> at central.sonatype.com. From the <code>Account</code> section, generate a key and password. These should be added to:</p> <pre><code>&lt;settings&gt;\n    ...\n    &lt;servers&gt;\n        &lt;server&gt;\n            &lt;id&gt;central&lt;/id&gt;\n           &lt;username&gt;------&lt;/username&gt;\n           &lt;password&gt;---------------token---------------&lt;/password&gt;\n       &lt;/server&gt;\n    &lt;/servers&gt;\n    ...\n&lt;/settings&gt;\n</code></pre>"},{"location":"development/release/#perform-release","title":"Perform release","text":"<p>Perform the following steps to create a new release:</p>"},{"location":"development/release/#1-checkout-the-main-branch","title":"1. \ud83c\udff7\ufe0f Checkout the <code>main</code> branch","text":"<pre><code>git checkout main\n</code></pre>"},{"location":"development/release/#2-set-the-release-version","title":"2. \ud83d\udd22 Set the Release Version","text":"<pre><code>mvn versions:set -DnewVersion=0.0.12\ngit commit -am \"release: version 0.0.12\"\n</code></pre>"},{"location":"development/release/#3-tag-and-push-the-release","title":"3. \ud83c\udff7\ufe0f Tag and Push the Release","text":"<pre><code>git tag v0.0.12\ngit push --follow-tags\n</code></pre>"},{"location":"development/release/#4-deploy-the-release","title":"4. \ud83d\ude80 Deploy the Release","text":"<p>Deploy the release (can be later automated via GitHub Actions or done manually):</p> <pre><code>mvn deploy -P release\n</code></pre>"},{"location":"development/release/#5-bump-to-the-next-snapshot-version","title":"5. \ud83d\udcc8 Bump to the Next Snapshot Version","text":"<pre><code>mvn versions:set -DnewVersion=0.0.13-SNAPSHOT\ngit commit -am \"post-release: bumped to 0.0.13-SNAPSHOT\"\ngit push\n</code></pre>"},{"location":"development/release/#6-publish-the-release-on-github","title":"6. \ud83d\udcdd Publish the Release on GitHub","text":"<ol> <li>Go to https://github.com/jajir/HestiaStore/releases and choose <code>Draft a new release</code>.</li> <li>From the drop-down box <code>target: main</code>, select <code>recent commits</code> and select the correct one with name <code>release: version 0.0.12</code>.</li> <li>From the drop-down box <code>Choose a tag</code> enter <code>release-0.0.12</code> and click <code>Create new tag: release ...</code>. Now in the repo, the tag clearly signals the new release.</li> <li>Release title should be <code>Release 0.0.3</code> and in the <code>Write</code> field, use the text generated from the template below:</li> <li>Press <code>Publish release</code>.</li> </ol> <p>Text template:</p> <pre><code>Release to maven central:\n\n```xml\n&lt;dependencies&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;org.hestiastore.index&lt;/groupId&gt;\n    &lt;artifactId&gt;core&lt;/artifactId&gt;\n    &lt;version&gt;0.0.3&lt;/version&gt; &lt;!-- Replace with the actual version --&gt;\n  &lt;/dependency&gt;\n&lt;/dependencies&gt;\n```\n</code></pre>"},{"location":"development/release/#7-celebrate","title":"7. \ud83c\udf89 Celebrate","text":"<p>That's it \u2014 the release is live and development can continue.</p>"},{"location":"development/release/#helpful-commands","title":"\ud83e\uddf0 Helpful Commands","text":"<p>At the beginning there may be problems. Here are a few tricks that help to gather more information.</p>"},{"location":"development/release/#how-to-use-a-custom-settingsxml-file","title":"How to Use a Custom settings.xml File","text":"<pre><code>mvn --settings ./src/main/settings.xml clean deploy\n</code></pre>"},{"location":"development/release/#how-to-set-the-maven-project-version","title":"How to Set the Maven Project Version","text":"<pre><code>mvn versions:set -DnewVersion=1.0.1-SNAPSHOT\n</code></pre>"},{"location":"development/release/#check-dependencies","title":"Check dependencies","text":"<p>Try to update dependencies. Check them with:</p> <pre><code>mvn versions:display-dependency-updates\n</code></pre>"},{"location":"development/segment-api/","title":"Segment API: Flush, Compact, and Split","text":""},{"location":"development/segment-api/#scope-and-assumptions","title":"Scope and assumptions","text":"<p>This document describes how to use the Segment API for maintenance operations. The public Segment interface exposes <code>flush()</code> and <code>compact()</code>. Split is orchestrated by the segment-index layer (<code>SegmentSplitCoordinator</code> + <code>SegmentSplitter</code>) using <code>Segment.openIterator(SegmentIteratorIsolation.FULL_ISOLATION)</code>. Split behavior is documented here because it affects locking and iterator semantics.</p> <p><code>Segment</code> is thread-safe by contract. The legacy lock-based adapter was removed; historical notes remain below for reference. For the lock-free <code>SegmentImpl</code> behavior and state machine rules, rely on <code>docs/architecture/segment/segment-concurrency.md</code>.</p> <p>This document keeps legacy notes for context and describes the lock-free model used today.</p>"},{"location":"development/segment-api/#legacy-removed-lock-based-adapter","title":"Legacy (removed lock-based adapter)","text":""},{"location":"development/segment-api/#lock-model-historical","title":"Lock model (historical)","text":"<p>This section describes the removed adapter for context only.</p> <p>Per-segment <code>ReentrantReadWriteLock</code> (via <code>SegmentImplSynchronizationAdapter</code>):</p> <ul> <li>Read lock: <code>get()</code>, <code>openIterator(FAIL_FAST)</code> per <code>hasNext()</code>/<code>next()</code> call.</li> <li>Write lock: <code>put()</code>, <code>flush()</code>, <code>compact()</code>, <code>invalidateIterators()</code>, <code>close()</code>.</li> <li><code>openIterator(FULL_ISOLATION)</code> holds the write lock for its entire lifetime.</li> </ul> <p>Optimistic iterator invalidation is driven by <code>VersionController</code> and used by <code>openIterator(FAIL_FAST)</code>.</p>"},{"location":"development/segment-api/#a-flush","title":"A) Flush","text":"<p>What it does today (SegmentImpl):</p> <ul> <li>Snapshot write cache via <code>SegmentCache.freezeWriteCache()</code>.</li> <li>Write entries to a delta cache file (<code>SegmentDeltaCacheWriter</code>).</li> <li>Merge frozen write cache into delta cache and clear the frozen snapshot.</li> <li>Bump version (via <code>openDeltaCacheWriter()</code>), invalidating fail-fast iterators.</li> <li>No-op when the write cache is empty.</li> </ul> <p>When a new write arrives while <code>flush()</code> is running:</p> <ul> <li>The new write blocks on the write lock.</li> </ul>"},{"location":"development/segment-api/#b-compact","title":"B) Compact","text":"<p>What it does today (SegmentCompacter):</p> <ul> <li>Resets searchers and bumps the version (invalidates fail-fast iterators).</li> <li>Rewrites the full segment using a merged view (index + delta + write cache).</li> <li><code>SegmentFullWriterTx</code> clears delta cache and write cache on commit.</li> </ul> <p>When a new write arrives while <code>compact()</code> is running:</p> <ul> <li>The new write blocks on the write lock.</li> </ul>"},{"location":"development/segment-api/#c-split-segmentindex","title":"C) Split (SegmentIndex)","text":"<p>What it does today (SegmentSplitCoordinator + SegmentSplitter):</p> <ul> <li>Triggered when <code>getNumberOfKeysInCache()</code> exceeds   <code>maxNumberOfKeysInSegmentCache</code> (or the coordinator threshold).</li> <li>May run <code>segment.compact()</code> first when the policy says the segment is small   or likely full of tombstones, then re-evaluates the plan.</li> <li>Acquires the segment write lock for the entire split when the segment is a   <code>SegmentImplSynchronizationAdapter</code>.</li> <li>Calls <code>segment.invalidateIterators()</code> to terminate fail-fast iterators.</li> <li>Opens <code>openIterator(FULL_ISOLATION)</code> and streams a merged view   (index + delta cache + write cache). Tombstones are filtered out by the   iterator (<code>MergeDeltaCacheWithIndexIterator</code>).</li> <li>Writes the first half into a new lower segment. Remaining entries go into a   new upper segment. If there are no remaining entries, the result is a   compaction: the lower segment replaces the current segment.</li> <li>Replaces on-disk files and updates <code>KeyToSegmentMap</code>, then closes the old   segment instance.</li> </ul> <p>When a new write arrives while split is running:</p> <ul> <li>The new write blocks on the write lock for the full split duration.</li> </ul> <p>When <code>get()</code> or iterators run during split:</p> <ul> <li><code>get()</code> and fail-fast iterators block on the write lock while split holds it.</li> <li>Fail-fast iterators are explicitly invalidated at split start.</li> <li>FULL_ISOLATION iterators hold the write lock, so split waits until they   close.</li> </ul>"},{"location":"development/segment-api/#backpressure-overload","title":"Backpressure / overload","text":"<p>The segment itself does not enforce a hard size cap. <code>UniqueCache</code> is unbounded, so if writes outpace maintenance, memory can grow.</p> <p>Current index-level handling (SegmentIndexImpl):</p> <ul> <li>When <code>getNumberOfKeysInWriteCache()</code> reaches   <code>maxNumberOfKeysInSegmentWriteCache</code>, the index triggers <code>flush()</code>.</li> <li>When <code>getNumberOfKeysInCache()</code> exceeds   <code>maxNumberOfKeysInSegmentCache</code>, the index tries <code>split()</code>, and falls back   to <code>flush()</code> if splitting is not possible.</li> </ul> <p>If write load still exceeds flush/compact throughput, pick one:</p> <ul> <li>Apply backpressure at the index level (for example, wrap SegmentIndex with a   bounded executor/queue).</li> <li>Lower thresholds to flush/split earlier.</li> <li>Add explicit write-cache caps or throttling around <code>Segment.put()</code>.</li> </ul>"},{"location":"development/segment-api/#parallel-calls-flush-vs-compact-vs-split","title":"Parallel calls (flush vs compact vs split)","text":"<p>Same segment:</p> <ul> <li><code>flush()</code>, <code>compact()</code>, and split are exclusive. The write lock serializes   them, so parallel calls run one after another.</li> <li>Split uses <code>FULL_ISOLATION</code> iterators and is wrapped in the write lock by   <code>SegmentSplitCoordinator</code> when the segment is a   <code>SegmentImplSynchronizationAdapter</code>.</li> </ul> <p>Different segments:</p> <ul> <li>Maintenance can run in parallel (each segment has its own lock).</li> <li>Split takes a registry lock only during file rename; it does not block   other segments otherwise.</li> </ul>"},{"location":"development/segment-api/#reads-get-and-iterators","title":"Reads, get(), and iterators","text":"<ul> <li>Multiple <code>get()</code> calls can run concurrently.</li> <li><code>openIterator(FAIL_FAST)</code> can run concurrently with <code>get()</code> and other   fail-fast iterators, but any mutation (<code>put</code>, <code>flush</code>, <code>compact</code>, <code>split</code>)   invalidates it via the version counter.</li> <li><code>openIterator(FULL_ISOLATION)</code> holds the write lock until closed, blocking   writes, flush, compact, and split on that segment.</li> </ul>"},{"location":"development/segment-api/#corner-cases-to-call-out","title":"Corner cases to call out","text":"<ul> <li>Fail-fast iterators must be expected to end early after any mutation.</li> <li>FULL_ISOLATION iterators must always be closed; otherwise writers can stall.</li> <li><code>flush()</code> is a no-op with an empty write cache.</li> <li><code>compact()</code> clears both delta cache and write cache; callers should not   expect pending writes to remain after compaction.</li> <li>Split can degrade to compaction when no entries remain for the upper segment.</li> <li>Split throws if the plan is not feasible (estimated keys too low).</li> <li>Split can be skipped when <code>hasLiveEntries()</code> finds no live entries (empty or   tombstone-only segment).</li> <li>Split closes and replaces the current segment instance; stale references to   the old segment must not be reused.</li> <li>Calls on a closed segment return <code>SegmentResultStatus.CLOSED</code>; callers can   check <code>getState()</code> or handle the status directly.</li> <li>Version overflow throws in <code>VersionController</code>; long-running services should   monitor for it.</li> </ul>"},{"location":"development/segment-api/#legacy-lock-summary-removed","title":"Legacy lock summary (removed)","text":"<ul> <li>Removed: per-segment read/write lock for API calls.</li> <li>One optimistic version counter for fail-fast iterator invalidation   (<code>VersionController</code>).</li> <li>Segment registry exposes <code>executeWithRegistryLock()</code> for split file   replacement, but the current <code>SegmentRegistry</code> does not implement a real   registry lock.</li> </ul>"},{"location":"development/segment-api/#lock-free-model-current","title":"Lock-free model (current)","text":""},{"location":"development/segment-api/#goals","title":"Goals","text":"<ul> <li>Keep <code>FREEZE</code> exclusive phases short and deterministic (swap + version bump).</li> <li>Run IO-heavy work (sorting, building files) outside <code>FREEZE</code>.</li> <li>Allow writes and most reads to continue during <code>MAINTENANCE_RUNNING</code>.</li> <li>Bound memory growth under sustained write load.</li> </ul>"},{"location":"development/segment-api/#coordination-model","title":"Coordination model","text":"<ul> <li>Short <code>FREEZE</code> phase for cache rotation, version capture, and file/reference   swap.</li> <li>Background maintenance uses frozen snapshots; no long-held   <code>FULL_ISOLATION</code> holds for heavy work.</li> <li>A per-segment maintenance state serializes flush/compact/split without   blocking normal writes.</li> </ul>"},{"location":"development/segment-api/#a-flush_1","title":"A) Flush","text":"<ul> <li>Under <code>FREEZE</code>: swap (freeze) the write cache and capture a version.</li> <li>Release to <code>MAINTENANCE_RUNNING</code>: sort and write the frozen snapshot to the   delta cache file.</li> <li>Under <code>FREEZE</code>: merge metadata and drop the frozen snapshot handle.</li> <li>Writes continue into the new write cache throughout the flush.</li> </ul>"},{"location":"development/segment-api/#b-compact_1","title":"B) Compact","text":"<ul> <li>Under <code>FREEZE</code>: freeze the merged in-memory view (delta + write cache),   capture a version, and redirect writes to a fresh cache.</li> <li>Release to <code>MAINTENANCE_RUNNING</code>: rebuild new on-disk files from the snapshot   plus current index.</li> <li>Optionally replay post-freeze writes before swap (or keep them in the active   write cache) to avoid data loss.</li> <li>Under <code>FREEZE</code>: validate version and swap files; clear obsolete delta files   and reset searchers.</li> </ul>"},{"location":"development/segment-api/#c-split","title":"C) Split","text":"<ul> <li>Use the freeze + redirect + replay pattern (see   <code>docs/development/segment-splitting-lock-minimization.md</code>).</li> <li>Under <code>FREEZE</code>: freeze the write cache, record a split version, and   redirect writes to a fresh cache.</li> <li>Release to <code>MAINTENANCE_RUNNING</code>: run the split pipeline against the frozen   snapshot.</li> <li>Replay post-freeze writes into the new lower/upper segments.</li> <li>Under <code>FREEZE</code>: verify version and swap references; on conflict, retry.</li> </ul>"},{"location":"development/segment-api/#backpressure-overload_1","title":"Backpressure / overload","text":"<ul> <li>Bound the number/size of frozen snapshots per segment.</li> <li>If backlog exceeds limits, apply throttling or spill to disk.</li> <li>Run maintenance on a background executor and prioritize older snapshots.</li> </ul>"},{"location":"development/segment-api/#parallel-calls","title":"Parallel calls","text":"<ul> <li>One maintenance task per segment at a time; other requests coalesce or wait.</li> <li>Flush requests during split should attach to replay or run after split.</li> <li>Maintenance can still run in parallel across different segments.</li> </ul>"},{"location":"development/segment-api/#reads-get-and-iterators_1","title":"Reads, get(), and iterators","text":"<ul> <li><code>get()</code> should read from current caches and files without blocking on long   maintenance tasks.</li> <li>Fail-fast iterators continue to invalidate on version changes.</li> <li>Prefer snapshot-based iterators for long scans instead of holding write   holds for <code>FULL_ISOLATION</code>.</li> </ul>"},{"location":"development/segment-api/#corner-cases","title":"Corner cases","text":"<ul> <li>Version mismatch at swap time should trigger retry or fallback.</li> <li>Large post-freeze write backlog should trigger backpressure or replay caps.</li> <li>Segment replacement must be atomic in the registry to avoid stale reads.</li> <li>Iterators opened before a swap should either complete on their snapshot or   fail fast.</li> </ul>"},{"location":"development/segment-api/#coordination-summary","title":"Coordination summary","text":"<ul> <li>Per-segment <code>FREEZE</code> for short cache/file swaps.</li> <li>Per-segment maintenance state to serialize flush/compact/split work.</li> <li>Optimistic version counter for iterator invalidation.</li> <li>Registry lock for file replacement if added in the future.</li> </ul>"},{"location":"how-to-use/","title":"\ud83d\ude80 Getting Started","text":"<p>Note: HestiaStore is a library, not a standalone application. It is designed to be integrated into a larger system to provide efficient storage and retrieval of large volumes of key\u2011value entries.</p> <p>HestiaStore is a Java library distributed as a JAR. To get started quickly:</p> <ul> <li>Install \u2014 add the dependency via Maven/Gradle.</li> <li>Quick Start \u2014 minimal in\u2011memory and filesystem examples.</li> <li>Troubleshooting \u2014 common issues, .lock files, and how to get help.</li> </ul>"},{"location":"how-to-use/#use-cases","title":"\ud83d\udca1 Use Cases","text":"<p>HestiaStore is especially effective when you need to:</p> <ul> <li>Store billions of key\u2011value entries on local disks</li> <li>Perform efficient point lookups with bounded memory</li> <li>Persist values to disk without external databases</li> <li>Avoid cloud storage or network\u2011attached stores</li> </ul> <p>When not to use HestiaStore:</p> <ul> <li>If all key\u2011value entries fit in RAM, prefer an in\u2011memory map (e.g., <code>HashMap</code> or <code>ConcurrentHashMap</code>) for speed and simplicity.</li> <li>For small datasets with relational queries, a traditional RDBMS may be simpler to operate.</li> </ul>"},{"location":"how-to-use/install/","title":"\ud83d\udce6 Installation Guide","text":""},{"location":"how-to-use/install/#prerequisites","title":"\u2699\ufe0f Prerequisites","text":"<ul> <li>Java 11 or higher (Java 17 recommended)</li> <li>Maven 3.6+ or Gradle 6+</li> </ul> <p>HestiaStore is distributed via Maven Central: https://central.sonatype.com/artifact/org.hestiastore.index/core</p>"},{"location":"how-to-use/install/#maven","title":"\ud83d\udee0\ufe0f Maven","text":"<p>Add the dependency to your pom.xml (use the latest version from Maven Central):</p> <pre><code>&lt;dependencies&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;org.hestiastore.index&lt;/groupId&gt;\n    &lt;artifactId&gt;core&lt;/artifactId&gt;\n    &lt;version&gt;&lt;!-- latest --&gt;&lt;/version&gt;\n  &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>"},{"location":"how-to-use/install/#verify-installation-maven","title":"\u2705 Verify Installation (Maven)","text":"<pre><code>mvn dependency:tree\n</code></pre> <p>Confirm org.hestiastore.index:core is present in the dependency tree.</p>"},{"location":"how-to-use/install/#gradle","title":"\ud83d\udee0\ufe0f Gradle","text":"<p>Add the dependency to your Gradle build (use the latest version from Maven Central). Ensure mavenCentral() is in repositories.</p> <p>Groovy DSL (build.gradle):</p> <pre><code>repositories {\n  mavenCentral()\n}\n\ndependencies {\n  implementation \"org.hestiastore.index:core:&lt;latest&gt;\"\n}\n</code></pre>"},{"location":"how-to-use/install/#verify-installation-gradle","title":"\u2705 Verify Installation (Gradle)","text":"<pre><code>./gradlew dependencyInsight --dependency org.hestiastore.index:core\n</code></pre>"},{"location":"how-to-use/install/#kotlin","title":"\ud83d\udee0\ufe0f Kotlin","text":"<p>Kotlin DSL (build.gradle.kts):</p> <pre><code>repositories {\n  mavenCentral()\n}\n\ndependencies {\n  implementation(\"org.hestiastore.index:core:&lt;latest&gt;\")\n}\n</code></pre>"},{"location":"how-to-use/install/#build-from-sources","title":"\ud83e\uddf1 Build from Sources","text":"<p>Source code for each release can be downloaded from GitHub releases: https://github.com/jajir/HestiaStore/releases</p> <p>Build the desired version:</p> <pre><code>mvn install\n</code></pre>"},{"location":"how-to-use/quick-start/","title":"\u26a1 Quick Start","text":"<p>This page shows common usage patterns with correct imports and short explanations.</p>"},{"location":"how-to-use/quick-start/#hello-world-inmemory","title":"\ud83d\udc4b Hello World (In\u2011Memory)","text":"<p>Simple example that creates in\u2011memory storage, then opens an index in this storage, writes one key\u2011value pair, and closes the index.</p> <pre><code>import org.hestiastore.index.segmentindex.SegmentIndex;\nimport org.hestiastore.index.segmentindex.IndexConfiguration;\nimport org.hestiastore.index.directory.Directory;\nimport org.hestiastore.index.directory.MemDirectory;\n\npublic class Example {\n  public static void main(String[] args) {\n    // Create an in\u2011memory directory implementation\n    Directory directory = new MemDirectory();\n\n    // Prepare index configuration\n    IndexConfiguration&lt;String, String&gt; conf = IndexConfiguration\n        .&lt;String, String&gt;builder()\n        .withKeyClass(String.class)\n        .withValueClass(String.class)\n        .withName(\"test_index\")\n        .build();\n\n    // SegmentIndex is AutoCloseable \u2014 prefer try\u2011with\u2011resources\n    try (SegmentIndex&lt;String, String&gt; index = SegmentIndex.create(directory, conf)) {\n      index.put(\"Hello\", \"World\");\n      String value = index.get(\"Hello\");\n      System.out.println(\"Value for Hello: \" + value);\n    }\n  }\n}\n</code></pre> <p>Once this works, explore the advanced configuration for directory types and custom key/value classes.</p>"},{"location":"how-to-use/quick-start/#filesystem-usage","title":"\ud83d\udcbe Filesystem Usage","text":"<p>Storing data to the file system is the main function of the library. A file system directory can be used like this:</p> <pre><code>import org.hestiastore.index.directory.Directory;\nimport org.hestiastore.index.directory.FsDirectory;\nimport java.io.File;\n\n// Create a file system directory\nDirectory directory = new FsDirectory(new File(\"../some/directory/\"));\n</code></pre> <p>This immediately creates the initial index files and makes it ready to use.</p> <p>Note: When an index works with a directory, it locks it with a <code>.lock</code> file. When the index is closed, the lock is removed.</p>"},{"location":"how-to-use/quick-start/#opening-an-existing-index","title":"\ud83d\udcc2 Opening an Existing Index","text":"<p>Use a dedicated open method for existing indexes:</p> <pre><code>import org.hestiastore.index.segmentindex.SegmentIndex;\nimport org.hestiastore.index.segmentindex.IndexConfiguration;\n\nIndexConfiguration&lt;String, String&gt; conf = IndexConfiguration\n    .&lt;String, String&gt;builder()\n    .withKeyClass(String.class)\n    .withValueClass(String.class)\n    .withName(\"test_index\")\n    .build();\n\nSegmentIndex&lt;String, String&gt; index = SegmentIndex.open(directory, conf);\n</code></pre>"},{"location":"how-to-use/quick-start/#data-manipulation","title":"\u270d\ufe0f Data Manipulation","text":"<p>Put and get are straightforward:</p> <pre><code>index.put(\"Hello\", \"World\");\nString value = index.get(\"Hello\");\n</code></pre> <p>Stored values are immediately available.</p>"},{"location":"how-to-use/quick-start/#sequential-data-reading","title":"\ud83d\udcc8 Sequential Data Reading","text":"<p>Read all entries in ascending key order:</p> <pre><code>index.getStream().forEach(entry -&gt; {\n  System.out.println(\"Entry: \" + entry);\n});\n</code></pre> <p>Select a subset of entries by segment window (offset, limit):</p> <pre><code>import org.hestiastore.index.segmentindex.SegmentWindow;\n\n// Only data from selected segments will be returned\nSegmentWindow window = SegmentWindow.of(1000, 10);\nindex.getStream(window).forEach(entry -&gt; System.out.println(entry));\n</code></pre>"},{"location":"how-to-use/quick-start/#data-maintenance","title":"\ud83e\uddf9 Data Maintenance","text":"<p>Maintenance operations available on Index:</p> <ul> <li><code>flush()</code> Flushes in\u2011memory data to disk. Useful before iterating or when you need to ensure data is durable.</li> <li><code>checkAndRepairConsistency()</code> Verifies metadata and segment consistency and attempts repairs; fails if beyond repair.</li> <li><code>compact()</code> Compacts segments and can reduce disk usage.</li> </ul> <pre><code>// After flush, all data changes are persisted. It is similar to a transaction commit.\nindex.flush();\n\n// Verify consistency or try to repair\nindex.checkAndRepairConsistency();\n\n// Data may be fragmented; this recalculates all segments\nindex.compact();\n</code></pre>"},{"location":"how-to-use/quick-start/#limitations","title":"\u26a0\ufe0f Limitations","text":""},{"location":"how-to-use/quick-start/#streaming-consistency","title":"\ud83c\udf00 Streaming Consistency","text":"<p>Streaming uses a snapshot at iteration time and does not use the index cache to avoid mid\u2011iteration mutations breaking iteration. To reduce stale results:</p> <ul> <li>Call <code>flush()</code> before streaming if recent writes must be included.</li> <li>Avoid calling <code>put()</code> or <code>delete()</code> while iterating a stream.</li> <li>For full snapshot reads, consider <code>compact()</code> beforehand to simplify segments.</li> </ul>"},{"location":"how-to-use/quick-start/#thread-safety","title":"\ud83d\udd12 Thread Safety","text":"<p>Index operations are synchronized internally to support concurrent access. Expect some locking overhead under heavy contention.</p>"},{"location":"how-to-use/quick-start/#exception-handling","title":"\ud83e\udde8 Exception Handling","text":"<p>Common runtime exceptions you may encounter:</p> <ul> <li><code>NullPointerException</code> \u2013 Severe I/O or unexpected nulls (e.g., corrupted files).</li> <li><code>IndexException</code> \u2013 Internal consistency issues detected by the index.</li> <li><code>IllegalArgumentException</code> \u2013 Validation errors (e.g., missing key type).</li> <li><code>IllegalStateException</code> \u2013 Inconsistent state preventing recovery.</li> </ul> <p>All exceptions are runtime exceptions and do not need explicit catching.</p>"},{"location":"how-to-use/troubleshooting/","title":"\ud83d\udee0\ufe0f Troubleshooting","text":"<p>Common issues and quick fixes.</p>"},{"location":"how-to-use/troubleshooting/#lock-file-prevents-opening","title":"\ud83d\udd12 \".lock\" File Prevents Opening","text":"<ul> <li>Cause: Another process holds the index open, or a previous run did not close cleanly.</li> <li>Fix: Ensure the process using the index has terminated and closed the index. If safe, remove the stale .lock file after verifying no process uses the directory.</li> </ul>"},{"location":"how-to-use/troubleshooting/#consistency-errors-indexexception","title":"\ud83e\udde9 Consistency Errors (IndexException)","text":"<ul> <li>Symptom: Consistency checks fail or reads behave unexpectedly after a crash.</li> <li>Fix: Run checkAndRepairConsistency(). If it still fails, restore from a backup.</li> </ul>"},{"location":"how-to-use/troubleshooting/#exception-attempt-to-insert-the-same-key-as-previous","title":"\u26a0\ufe0f Exception 'Attempt to insert the same key as previous'","text":"<p>When followin exception appears in logs. Than it's probably problem with inconsistent implementaion of custom data type. Please look at custom type configuration</p> <pre><code>xception in thread \"main\" java.lang.IllegalArgumentException: Attempt to insert the same key as previous. Key(Base64)='QUFHR0JHQkFBVERTU1BTUw==', comparator='DtFixedLengthByteArray$$Lambda/0x0000000800214238'\n    at org.hestiastore.index.sorteddatafile.SortedDataFileWriter.verifyKeyOrder(SortedDataFileWriter.java:76)\n    at org.hestiastore.index.sorteddatafile.SortedDataFileWriter.write(SortedDataFileWriter.java:104)\n    at org.hestiastore.index.GuardedEntryWriter.write(GuardedEntryWriter.java:18)\n    at org.hestiastore.index.segment.SegmentDeltaCacheWriter.lambda$doClose$0(SegmentDeltaCacheWriter.java:87)\n    at org.hestiastore.index.WriteTransaction.execute(WriteTransaction.java:41)\n    at org.hestiastore.index.segment.SegmentDeltaCacheWriter.doClose(SegmentDeltaCacheWriter.java:84)\n    at org.hestiastore.index.AbstractCloseableResource.close(AbstractCloseableResource.java:23)\n    at org.hestiastore.index.segment.SegmentDeltaCacheCompactingWriter.doClose(SegmentDeltaCacheCompactingWriter.java:47)\n    at org.hestiastore.index.AbstractCloseableResource.close(AbstractCloseableResource.java:23)\n    at org.hestiastore.index.segmentindex.CompactSupport.flushToCurrentSegment(CompactSupport.java:84)\n    at org.hestiastore.index.segmentindex.CompactSupport.compact(CompactSupport.java:59)\n    at java.base/java.util.ArrayList.forEach(ArrayList.java:1596)\n    at org.hestiastore.index.segmentindex.SegmentIndexImpl.flushCache(SegmentIndexImpl.java:126)\n    at org.hestiastore.index.segmentindex.SegmentIndexImpl.put(SegmentIndexImpl.java:83)\n    at org.hestiastore.index.segmentindex.SegmentIndex.put(SegmentIndex.java:93)\n</code></pre>"},{"location":"how-to-use/troubleshooting/#dependency-resolution-fails","title":"\ud83d\udce6 Dependency Resolution Fails","text":"<ul> <li> <p>Maven: run the command below and confirm org.hestiastore.index:core is present.</p> <p>mvn dependency:tree</p> </li> <li> <p>Gradle: run the command below for dependency insight.</p> <p>./gradlew dependencyInsight --dependency org.hestiastore.index:core</p> </li> <li> <p>Also verify you used the latest version from Maven Central.</p> </li> </ul>"},{"location":"how-to-use/troubleshooting/#java-version-mismatch","title":"\u2615 Java Version Mismatch","text":"<ul> <li>Ensure Java 11+ is used (Java 17 recommended). Check with java -version and align your IDE or CI JDK.</li> </ul>"},{"location":"how-to-use/troubleshooting/#permission-or-path-errors","title":"\ud83d\udcc1 Permission or Path Errors","text":"<ul> <li>Ensure your process has read/write permissions to the target directory.</li> <li>Use absolute paths for clarity in services or containers.</li> </ul>"},{"location":"how-to-use/troubleshooting/#stale-streaming-results","title":"\ud83d\udd04 Stale Streaming Results","text":"<ul> <li>Call flush() before streaming if you require latest writes.</li> <li>Avoid put() or delete() while iterating a stream.</li> </ul>"},{"location":"how-to-use/troubleshooting/#need-more-help","title":"\u2753 Need More Help?","text":"<ul> <li>Search existing tickets: https://github.com/jajir/HestiaStore/issues?q=is%3Aissue</li> <li>Open a new ticket: https://github.com/jajir/HestiaStore/issues/new/choose</li> </ul> <p>When opening a ticket, please include:</p> <ul> <li>Your Java version (run <code>java -version</code>)</li> <li>HestiaStore version and build tool (Maven/Gradle)</li> <li>Minimal code snippet or steps to reproduce</li> <li>Relevant logs/stack traces and OS info</li> </ul>"},{"location":"operations/","title":"\ud83c\udfed Production Guide","text":"<p>Operational guidance for running HestiaStore in production.</p> <ul> <li>Deployment considerations and sizing</li> <li>File system and storage layout</li> <li>Backup and restore strategy</li> <li>Observability and logging</li> </ul>"},{"location":"operations/#after-unexpected-shutdown","title":"\u26a0\ufe0f After Unexpected Shutdown","text":"<p>Recommended steps to verify integrity and reclaim optimal layout:</p> <ul> <li>Reopen the index and run a consistency check to validate segments and the key\u2192segment map.</li> <li>Optionally run a compaction to merge delta caches into main SST files and rebuild auxiliary structures (sparse index, Bloom filter).</li> <li>Take a fresh backup after compaction completes.</li> </ul> <p>Example (Java):</p> <pre><code>// Open existing index (types omitted for brevity)\nSegmentIndex&lt;Integer, String&gt; index = SegmentIndex.open(directory);\n\n// 1) Verify internal consistency (throws IndexException on irreparable issues)\nindex.checkAndRepairConsistency();\n\n// 2) Optionally compact to fold delta caches into main SST files\nindex.compact();\n\n// 3) Create a new backup snapshot\n//   (use your backup process; see operations/backup-restore.md)\n\nindex.close();\n</code></pre> <p>Notes: - HestiaStore has no WAL; durability comes from flush/close boundaries and atomic file replacement on commit. - If you keep running without compaction, reads remain correct; compaction improves locality and space usage.</p>"},{"location":"operations/backup-restore/","title":"\ud83d\udcbe Backup &amp; Restore","text":"<p>Approaches for backing up and restoring HestiaStore data.</p> <ul> <li>Snapshotting strategies</li> <li>Consistency considerations</li> <li>Restore and validation steps</li> </ul> <p>Content to be expanded.</p>"},{"location":"operations/monitoring/","title":"\ud83d\udcc8 Observability","text":"<p>Monitoring and logging guidance for HestiaStore.</p> <ul> <li>Metrics and health checks (to be defined)</li> <li>Logging configuration: see configuration/logging.md</li> <li>Common alerts and thresholds</li> </ul> <p>Content to be expanded.</p>"},{"location":"operations/tuning/","title":"\ud83d\udd27 Performance Tuning","text":"<p>Guidance for tuning HestiaStore performance.</p> <ul> <li>Memory and buffer settings</li> <li>Compaction/merge policies</li> <li>I/O characteristics and caching</li> </ul> <p>Content to be expanded.</p>"},{"location":"why-hestiastore/","title":"\u2753 Why HestiaStore","text":"<p>Choosing a storage engine is about fit. HestiaStore shines when you need a fast, embeddable key\u2013value store with predictable file I/O, simple operations, and very large datasets. Understanding where it fits (and where it does not) helps you make confident decisions about performance, durability, and operational effort.</p> <p>What to consider when assessing fit:</p> <ul> <li>Workload shape: write/read ratios, point lookups vs. scans, negative\u2011lookup rate</li> <li>Data volume: per\u2011segment size, total keys, growth expectations</li> <li>Durability model: flush/close boundaries, transactional file commits</li> <li>Concurrency: single\u2011writer vs. multi\u2011threaded access, iteration under mutation</li> <li>Operations: compaction windows, monitoring, backups, and recovery steps</li> <li>Platform constraints: pure\u2011Java, filesystem characteristics, containerization</li> </ul> <p>Key resources to decide quickly:</p> <ul> <li>Alternatives \u2014 quick comparison and when to prefer other engines.</li> <li>Benchmarks (write) \u2014 throughput while writing simple key\u2013value pairs after warm\u2011up.</li> <li>Benchmarks (read) \u2014 random lookup throughput on a pre\u2011populated dataset.</li> <li>Benchmarks (sequential read) \u2014 forward scan throughput across keys in order.</li> <li>Security \u2014 security policy, reporting, and handling guidance.</li> <li>Quality &amp; Testing \u2014 CI status, coverage, static analysis, and dependency checks.</li> </ul>"},{"location":"why-hestiastore/alternatives/","title":"\ud83d\udd00 Alternatives","text":"<p>HestiaStore is one of many available solutions for key-value storage. When selecting the right tool, it's important to consider which one best fits your needs. Here are some key evaluation criteria:</p> <ul> <li>\ud83d\udd01 Transactional Support</li> <li>\ud83e\uddea ACID Compliance</li> <li>\u2601\ufe0f Cloud Availability</li> <li>\u26a1 Performance</li> <li>\ud83d\udee0\ufe0f Error Handling</li> <li>\ud83d\udcda API Completeness ...</li> </ul> <p>Below are a few notable alternatives (not an exhaustive list):</p>"},{"location":"why-hestiastore/alternatives/#mapdb","title":"\ud83d\uddfa\ufe0f MapDB","text":"<p>Homepage / GitHub</p> <p>MapDB focuses on replacing <code>java.util.Map</code> with a disk-backed map structure. While powerful, its recent versions have limited disk persistence support and performance may be slower for some use cases.</p>"},{"location":"why-hestiastore/alternatives/#h2-mvstore","title":"\ud83d\uddc3\ufe0f H2 MVStore","text":"<p>Homepage</p> <p>MVStore is the underlying storage engine for the H2 database. It features a friendly API, support for transactions, and generally good performance. It is well-suited for embedded systems and relational data scenarios.</p>"},{"location":"why-hestiastore/alternatives/#chronicle-map","title":"\ud83d\udcd8 Chronicle Map","text":"<p>Homepage</p> <p>Chronicle Map offers low-latency, off-heap key-value storage with support for huge datasets. It is especially suitable for high-performance and low-GC scenarios. Disk persistence is supported, though the primary target is memory-mapped data sharing.</p>"},{"location":"why-hestiastore/alternatives/#rocksdb","title":"\ud83e\udea8 RocksDB","text":"<p>Homepage / GitHub</p> <p>RocksDB is a mature, high-performance embedded key-value store developed by Facebook. While it is written in C++, a Java binding is available. It supports compression, compaction, snapshots, and many tuning options.</p>"},{"location":"why-hestiastore/alternatives/#babudb","title":"\ud83d\udc18 BabuDB","text":"<p>Homepage</p> <p>BabuDB is a log-structured, non-relational key-value store optimized for write performance and reliability. It's less widely used today but offers interesting architectural choices like write-ahead logging and on-disk persistence.</p>"},{"location":"why-hestiastore/out-read/","title":"\ud83d\udcca HestiaStore Benchmark Results","text":""},{"location":"why-hestiastore/out-read/#test-conditions-read-benchmarks","title":"\ud83e\uddea Test Conditions - Read Benchmarks","text":"<ul> <li>Read-focused runs reuse the same controlled JVM, hardware, and JVM flag configuration as the write suite. Each trial prepares a clean directory pointed to by the <code>dir</code> system property before preloading the dataset.</li> <li>Setup inserts 10\u202f000\u202f000 deterministic key/value pairs (seed <code>324432L</code>) so every engine serves identical data. Keys come from <code>HashDataProvider</code>, while values remain the constant string <code>\"opice skace po stromech\"</code>.</li> <li>Warm-up iterations issue random lookups (80\u202f% hits, 20\u202f% misses) to trigger JIT compilation, cache population, and to ensure index structures have settled before measurements start.</li> <li>Measurement iterations continue the same single-threaded read loop, sampling throughput over 20-second windows to capture sustained lookup performance rather than momentary bursts.</li> <li>Each benchmark keeps a consistent random sequence per iteration, ensuring engines experience the same access pattern and allowing apples-to-apples comparisons.</li> <li>After measurements finish, readers close their resources but the populated directories remain on disk so sizes can be captured by the reporting scripts.</li> <li>Tests executed on Mac mini 2024, 16\u202fGB RAM, macOS 15.6.1 (24G90).</li> </ul>"},{"location":"why-hestiastore/out-read/#benchmark-results","title":"\ud83c\udfc1 Benchmark Results","text":"Engine Score [ops/s] ScoreError Confidence Interval [ops/s] Occupied space CPU Usage ChronicleMap 1 682 003 292 058 1 389 944 .. 1 974 061 2.03 GB 13% H2 506 792 14 165 492 627 .. 520 957 8 KB 11% HestiaStoreBasic 669 41 628 .. 710 507.94 MB 11% HestiaStoreCompress 749 14 735 .. 763 283.94 MB 11% LevelDB 195 086 6 474 188 612 .. 201 560 363.32 MB 10% MapDB 1 264 70 1 194 .. 1 334 1.3 GB 4% RocksDB 106 795 7 214 99 580 .. 114 009 324.22 MB 11% <p>meaning of columns:</p> <ul> <li>Engine: name of the benchmarked engine (as derived from the JSON filename)</li> <li>Score [ops/s]: number of operations per second (higher is better)</li> <li>ScoreError: error margin of the score (lower is better). It's computed as <code>z * (stdev / sqrt(n)) where</code></li> <li><code>z</code> is the z-score for the desired confidence level (1.96 for 95%)</li> <li><code>stdev</code> is the standard deviation of the measurements</li> <li><code>n</code> is the number of measurements</li> <li>Confidence Interval: 95% confidence interval of the score (lower and upper bound). This means that the true mean is likely between this interval of ops/sec. Negative values are possible if the error margin is larger than the score itself.</li> <li>Occupied space : amount of disk space occupied by the engine's data structures (lower is better). It is measured after flushing last data to disk.</li> <li>CPU Usage: average CPU usage during the benchmark (lower is better). Please note, that it includes all system processes, not only the benchmarked engine.</li> </ul>"},{"location":"why-hestiastore/out-read/#raw-json-files","title":"\ud83d\udcc4 Raw JSON Files","text":""},{"location":"why-hestiastore/out-read/#results-read-chroniclemap-myjson","title":"results-read-ChronicleMap-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 2177908736,\n  \"fileCount\" : 1,\n  \"usedMemoryBytes\" : 32256120,\n  \"cpuBefore\" : 671175000,\n  \"cpuAfter\" : 1606379000,\n  \"startTime\" : 1648961732540458,\n  \"endTime\" : 1649675242128625,\n  \"cpuUsage\" : 0.13107097865391423\n}\n</code></pre>"},{"location":"why-hestiastore/out-read/#results-read-chroniclemapjson","title":"results-read-ChronicleMap.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestChronicleMapRead.read\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"--add-opens=java.base/java.lang=ALL-UNNAMED\",\n            \"--add-opens=java.base/java.lang.reflect=ALL-UNNAMED\",\n            \"--add-opens=java.base/java.io=ALL-UNNAMED\",\n            \"--add-opens=java.base/java.nio=ALL-UNNAMED\",\n            \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.code=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.comp=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.main=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.model=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.processing=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED\",\n            \"-Dengine=ChronicleMapRead\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 1682002.8268765218,\n            \"scoreError\" : 292058.3572608181,\n            \"scoreConfidence\" : [\n                1389944.4696157037,\n                1974061.18413734\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 197142.9403720747,\n                \"50.0\" : 1806856.0068332641,\n                \"90.0\" : 1890222.1247093529,\n                \"95.0\" : 1897851.6723723745,\n                \"99.0\" : 1901078.5621515624,\n                \"99.9\" : 1901078.5621515624,\n                \"99.99\" : 1901078.5621515624,\n                \"99.999\" : 1901078.5621515624,\n                \"99.9999\" : 1901078.5621515624,\n                \"100.0\" : 1901078.5621515624\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    1820119.3457355907,\n                    1831918.3065460657,\n                    1806856.0068332641,\n                    1828114.833905765,\n                    1808762.2744857376,\n                    1663180.9878049963,\n                    1803128.4114218033,\n                    1825953.4242856558,\n                    1800829.3752878718,\n                    1786706.336570534,\n                    1776599.6624524684,\n                    1805268.0635949115,\n                    1824415.302102971,\n                    1703394.6484768006,\n                    1536544.8969034227,\n                    1668598.8202788664,\n                    1810899.5998044407,\n                    1730516.169779019,\n                    1809892.6676277258,\n                    1858056.285219788,\n                    671616.1214602407,\n                    197142.9403720747,\n                    1890322.262887603,\n                    1901078.5621515624,\n                    1890155.3659238527\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-read/#results-read-h2-myjson","title":"results-read-H2-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 8192,\n  \"fileCount\" : 1,\n  \"usedMemoryBytes\" : 32616872,\n  \"cpuBefore\" : 635134000,\n  \"cpuAfter\" : 1463415000,\n  \"startTime\" : 1641426355284833,\n  \"endTime\" : 1642149918493208,\n  \"cpuUsage\" : 0.1144725146902063\n}\n</code></pre>"},{"location":"why-hestiastore/out-read/#results-read-h2json","title":"results-read-H2.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestH2Read.read\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=H2Read\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 506791.9285244222,\n            \"scoreError\" : 14165.03183608771,\n            \"scoreConfidence\" : [\n                492626.8966883345,\n                520956.9603605099\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 471237.70242300315,\n                \"50.0\" : 511293.0890450197,\n                \"90.0\" : 527105.9329954404,\n                \"95.0\" : 534035.4136123367,\n                \"99.0\" : 536639.0343340511,\n                \"99.9\" : 536639.0343340511,\n                \"99.99\" : 536639.0343340511,\n                \"99.999\" : 536639.0343340511,\n                \"99.9999\" : 536639.0343340511,\n                \"100.0\" : 536639.0343340511\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    525337.6218759385,\n                    522892.2798470988,\n                    501995.8040768004,\n                    492388.675720105,\n                    513950.5431193879,\n                    517392.852333133,\n                    500875.32160563004,\n                    519771.3696283302,\n                    527960.2985950032,\n                    511293.0890450197,\n                    521623.04335682123,\n                    473836.62781834585,\n                    520727.6426309597,\n                    509364.4466342562,\n                    480486.5968320747,\n                    471853.824905614,\n                    536639.0343340511,\n                    526536.3559290653,\n                    520603.49371563253,\n                    500415.8560454402,\n                    485553.5750647521,\n                    513741.18706217426,\n                    492570.738784536,\n                    471237.70242300315,\n                    510750.2317273797\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-read/#results-read-hestiastorebasic-myjson","title":"results-read-HestiaStoreBasic-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 532617108,\n  \"fileCount\" : 10,\n  \"usedMemoryBytes\" : 31454032,\n  \"cpuBefore\" : 845976000,\n  \"cpuAfter\" : 1743708000,\n  \"startTime\" : 1637158023991291,\n  \"endTime\" : 1637952134906833,\n  \"cpuUsage\" : 0.11304869161599121\n}\n</code></pre>"},{"location":"why-hestiastore/out-read/#results-read-hestiastorebasicjson","title":"results-read-HestiaStoreBasic.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestHestiaStoreBasicRead.read\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=HestiaStoreBasicRead\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 668.9071662663633,\n            \"scoreError\" : 41.11066158931372,\n            \"scoreConfidence\" : [\n                627.7965046770496,\n                710.0178278556771\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 552.8465486331146,\n                \"50.0\" : 696.9593337062771,\n                \"90.0\" : 719.9011508373728,\n                \"95.0\" : 723.7053404419349,\n                \"99.0\" : 724.921148360428,\n                \"99.9\" : 724.921148360428,\n                \"99.99\" : 724.921148360428,\n                \"99.999\" : 724.921148360428,\n                \"99.9999\" : 724.921148360428,\n                \"100.0\" : 724.921148360428\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    631.661806861714,\n                    552.8465486331146,\n                    580.3341018344279,\n                    603.5705256292723,\n                    663.1782915635563,\n                    657.8367832665854,\n                    572.9926808774409,\n                    609.5723072807286,\n                    614.5076053968321,\n                    715.6433196022614,\n                    612.6223043203357,\n                    671.9937661854882,\n                    696.9593337062771,\n                    713.9905792909677,\n                    724.921148360428,\n                    714.8873508239324,\n                    712.9606361578192,\n                    711.6062081606229,\n                    711.3822590511212,\n                    719.2562811964317,\n                    720.8684552987846,\n                    714.7264748111861,\n                    712.8586920887575,\n                    679.3504187306171,\n                    702.1512775303793\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-read/#results-read-hestiastorecompress-myjson","title":"results-read-HestiaStoreCompress-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 297736213,\n  \"fileCount\" : 10,\n  \"usedMemoryBytes\" : 31401000,\n  \"cpuBefore\" : 675596000,\n  \"cpuAfter\" : 1511264000,\n  \"startTime\" : 1647385203175708,\n  \"endTime\" : 1648172315402208,\n  \"cpuUsage\" : 0.1061688501163182\n}\n</code></pre>"},{"location":"why-hestiastore/out-read/#results-read-hestiastorecompressjson","title":"results-read-HestiaStoreCompress.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestHestiaStoreCompressRead.read\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=HestiaStoreCompressRead\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 748.9454404780861,\n            \"scoreError\" : 14.319140539693919,\n            \"scoreConfidence\" : [\n                734.6262999383922,\n                763.26458101778\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 712.2614748019505,\n                \"50.0\" : 746.2646649030198,\n                \"90.0\" : 777.2308012996768,\n                \"95.0\" : 799.7850705472416,\n                \"99.0\" : 808.1926849055343,\n                \"99.9\" : 808.1926849055343,\n                \"99.99\" : 808.1926849055343,\n                \"99.999\" : 808.1926849055343,\n                \"99.9999\" : 808.1926849055343,\n                \"100.0\" : 808.1926849055343\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    755.0830702817915,\n                    770.9991349511698,\n                    780.1673037112255,\n                    808.1926849055343,\n                    775.273133025311,\n                    748.5834089755748,\n                    753.5309075672122,\n                    755.7055670343427,\n                    751.3107849622255,\n                    743.607636318027,\n                    712.2614748019505,\n                    739.7914230972248,\n                    737.4071939539258,\n                    743.4076301567964,\n                    739.086250200293,\n                    746.2646649030198,\n                    751.0728302113451,\n                    738.7756052965466,\n                    752.090197032154,\n                    746.3458132218094,\n                    737.4341890578321,\n                    737.5698580440608,\n                    742.2366252285157,\n                    725.6202029295067,\n                    731.8184220847556\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-read/#results-read-leveldb-myjson","title":"results-read-LevelDB-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 380967678,\n  \"fileCount\" : 195,\n  \"usedMemoryBytes\" : 31850312,\n  \"cpuBefore\" : 667283000,\n  \"cpuAfter\" : 1430939000,\n  \"startTime\" : 1649739319381333,\n  \"endTime\" : 1650485889725958,\n  \"cpuUsage\" : 0.10228855264584373\n}\n</code></pre>"},{"location":"why-hestiastore/out-read/#results-read-leveldbjson","title":"results-read-LevelDB.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestLevelDBRead.read\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=LevelDBRead\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 195085.88678208127,\n            \"scoreError\" : 6474.309123288522,\n            \"scoreConfidence\" : [\n                188611.57765879275,\n                201560.1959053698\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 176190.39792615132,\n                \"50.0\" : 195023.7638549175,\n                \"90.0\" : 210717.30776032349,\n                \"95.0\" : 213571.77150810737,\n                \"99.0\" : 214735.5148025333,\n                \"99.9\" : 214735.5148025333,\n                \"99.99\" : 214735.5148025333,\n                \"99.999\" : 214735.5148025333,\n                \"99.9999\" : 214735.5148025333,\n                \"100.0\" : 214735.5148025333\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    194176.15643904477,\n                    198333.2787851288,\n                    199667.4447046021,\n                    193265.10825709114,\n                    195407.03049141855,\n                    197508.21639159892,\n                    190410.5317169661,\n                    210856.3704877802,\n                    210624.59927535235,\n                    214735.5148025333,\n                    205583.4105362529,\n                    196543.66525143213,\n                    195023.7638549175,\n                    197512.81264500812,\n                    195650.01538755512,\n                    196441.2073694981,\n                    182790.2485559269,\n                    189268.08888627027,\n                    176190.39792615132,\n                    190895.46316005214,\n                    189254.47328615063,\n                    187982.90400167697,\n                    189466.4188215005,\n                    189178.2307349407,\n                    190381.81778318202\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-read/#results-read-mapdb-myjson","title":"results-read-MapDB-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 1399848960,\n  \"fileCount\" : 1,\n  \"usedMemoryBytes\" : 31893256,\n  \"cpuBefore\" : 678430000,\n  \"cpuAfter\" : 2660434000,\n  \"startTime\" : 1642150324938458,\n  \"endTime\" : 1647143083053041,\n  \"cpuUsage\" : 0.039697577060881485\n}\n</code></pre>"},{"location":"why-hestiastore/out-read/#results-read-mapdbjson","title":"results-read-MapDB.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestMapDBRead.read\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=MapDBRead\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 1263.8516542832706,\n            \"scoreError\" : 69.8775734806934,\n            \"scoreConfidence\" : [\n                1193.9740808025772,\n                1333.729227763964\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 947.2129667728096,\n                \"50.0\" : 1284.2584789937207,\n                \"90.0\" : 1339.9479385369061,\n                \"95.0\" : 1385.106205372084,\n                \"99.0\" : 1403.359442120203,\n                \"99.9\" : 1403.359442120203,\n                \"99.99\" : 1403.359442120203,\n                \"99.999\" : 1403.359442120203,\n                \"99.9999\" : 1403.359442120203,\n                \"100.0\" : 1403.359442120203\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    1338.2363511438614,\n                    1332.0758755031911,\n                    1178.2256174857496,\n                    1267.3858818618855,\n                    1190.4857155709674,\n                    1322.550695069825,\n                    1403.359442120203,\n                    1342.5153196264732,\n                    1299.7816558558081,\n                    1330.4724950604327,\n                    1319.8257554819095,\n                    1324.862598467169,\n                    1255.09142594631,\n                    1270.262668537638,\n                    1081.917561263905,\n                    947.2129667728096,\n                    1186.4752568233419,\n                    1246.3827804523664,\n                    1282.7167070422734,\n                    1247.307074200635,\n                    1309.8857185213003,\n                    1293.051064762047,\n                    1284.2584789937207,\n                    1257.494168092977,\n                    1284.458082424962\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-read/#results-read-rocksdb-myjson","title":"results-read-RocksDB-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 339967184,\n  \"fileCount\" : 12,\n  \"usedMemoryBytes\" : 31535552,\n  \"cpuBefore\" : 640065000,\n  \"cpuAfter\" : 1471724000,\n  \"startTime\" : 1648174332510833,\n  \"endTime\" : 1648907521328250,\n  \"cpuUsage\" : 0.11343039886095196\n}\n</code></pre>"},{"location":"why-hestiastore/out-read/#results-read-rocksdbjson","title":"results-read-RocksDB.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestRocksDBRead.read\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=RocksDBRead\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 106794.51976258818,\n            \"scoreError\" : 7214.086135112554,\n            \"scoreConfidence\" : [\n                99580.43362747564,\n                114008.60589770073\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 92533.94948825406,\n                \"50.0\" : 104900.51756255777,\n                \"90.0\" : 124022.53235463434,\n                \"95.0\" : 124734.48481796458,\n                \"99.0\" : 124851.37397051069,\n                \"99.9\" : 124851.37397051069,\n                \"99.99\" : 124851.37397051069,\n                \"99.999\" : 124851.37397051069,\n                \"99.9999\" : 124851.37397051069,\n                \"100.0\" : 124851.37397051069\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    124851.37397051069,\n                    118189.18577054547,\n                    122150.12526452326,\n                    106745.63460278472,\n                    99350.89071646037,\n                    95796.16201799439,\n                    105270.19129176499,\n                    124461.74346202366,\n                    123729.72494970812,\n                    118383.7650817294,\n                    106757.22189820537,\n                    106836.36188189387,\n                    107846.08853177314,\n                    103224.71792558744,\n                    102076.18953051057,\n                    101304.38590978077,\n                    104900.51756255777,\n                    108961.56433403351,\n                    102582.48822762424,\n                    99535.33113220322,\n                    100242.10124263776,\n                    98587.79501187788,\n                    98454.26223849891,\n                    97091.2220212208,\n                    92533.94948825406\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-sequential/","title":"HestiaStore Benchmark Results","text":""},{"location":"why-hestiastore/out-sequential/#test-conditions-sequential-read-benchmarks","title":"Test Conditions - Sequential Read Benchmarks","text":"<ul> <li>Each sequential scenario uses the same JVM flags, hardware, and scratch directory handling as the write/read suites. The <code>dir</code> property is cleaned before every run to guarantee a fresh start.</li> <li>Setup writes 10\u202f000\u202f000 deterministic key/value pairs (seed <code>324432L</code>) into the engine. Keys are generated via <code>HashDataProvider</code> so that the exact ordering is reproducible across runs.</li> <li>After preloading, the benchmark resets its sequential cursor. Warm-up iterations walk the keyspace from the first key to the last key so caches and OS I/O buffers reflect streaming access.</li> <li>Measurement iterations continue sequential scans, looping back to the first key whenever the end is reached. This focuses on sustained read throughput when data is consumed in order.</li> <li>The read workload remains single-threaded; each invocation issues exactly one lookup to keep measurements comparable with the other suites.</li> <li>Directories remain on disk after the run so disk usage and auxiliary metrics can be collected by reporting scripts.</li> <li>Tests for HestiaStoreStream use dedicated stream API. Without using Stream API is performance visible in line HestiaStoreBasic.</li> <li>Tests executed on Mac mini 2024, 16\u202fGB RAM, macOS 15.6.1 (24G90).</li> </ul>"},{"location":"why-hestiastore/out-sequential/#benchmark-results","title":"Benchmark Results","text":"Engine Score [ops/s] ScoreError Confidence Interval [ops/s] Occupied space CPU Usage ChronicleMap 1 707 702 82 988 1 624 713 .. 1 790 690 2.03 GB 13% H2 364 687 43 577 321 110 .. 408 264 8 KB 14% HestiaStoreBasic 592 68 524 .. 660 507.94 MB 15% HestiaStoreStream 4 792 777 144 132 4 648 646 .. 4 936 909 283.94 MB 12% LevelDB 190 698 6 694 184 004 .. 197 391 363.32 MB 10% MapDB 1 528 228 1 300 .. 1 756 1.3 GB 5% RocksDB 109 551 10 513 99 038 .. 120 064 324.23 MB 10% <p>meaning of columns:</p> <ul> <li>Engine: name of the benchmarked engine (as derived from the JSON filename)</li> <li>Score [ops/s]: number of operations per second (higher is better)</li> <li>ScoreError: error margin of the score (lower is better). It's computed as <code>z * (stdev / sqrt(n)) where</code></li> <li><code>z</code> is the z-score for the desired confidence level (1.96 for 95%)</li> <li><code>stdev</code> is the standard deviation of the measurements</li> <li><code>n</code> is the number of measurements</li> <li>Confidence Interval: 95% confidence interval of the score (lower and upper bound). This means that the true mean is likely between this interval of ops/sec. Negative values are possible if the error margin is larger than the score itself.</li> <li>Occupied space : amount of disk space occupied by the engine's data structures (lower is better). It is measured after flushing last data to disk.</li> <li>CPU Usage: average CPU usage during the benchmark (lower is better). Please note, that it includes all system processes, not only the benchmarked engine.</li> </ul>"},{"location":"why-hestiastore/out-sequential/#raw-json-files","title":"Raw JSON Files","text":""},{"location":"why-hestiastore/out-sequential/#results-sequential-chroniclemap-myjson","title":"results-sequential-ChronicleMap-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 2177908736,\n  \"fileCount\" : 1,\n  \"usedMemoryBytes\" : 32422400,\n  \"cpuBefore\" : 801274000,\n  \"cpuAfter\" : 1755750000,\n  \"startTime\" : 1703675458276666,\n  \"endTime\" : 1704389298092541,\n  \"cpuUsage\" : 0.13371010957549861\n}\n</code></pre>"},{"location":"why-hestiastore/out-sequential/#results-sequential-chroniclemapjson","title":"results-sequential-ChronicleMap.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestChronicleMapSequential.readSequential\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"--add-opens=java.base/java.lang=ALL-UNNAMED\",\n            \"--add-opens=java.base/java.lang.reflect=ALL-UNNAMED\",\n            \"--add-opens=java.base/java.io=ALL-UNNAMED\",\n            \"--add-opens=java.base/java.nio=ALL-UNNAMED\",\n            \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.code=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.comp=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.main=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.model=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.processing=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED\",\n            \"-Dengine=ChronicleMapSequential\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 1707701.5393647857,\n            \"scoreError\" : 82988.49124709374,\n            \"scoreConfidence\" : [\n                1624713.048117692,\n                1790690.0306118794\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 1493645.6848657392,\n                \"50.0\" : 1697235.2876620279,\n                \"90.0\" : 1866914.805183605,\n                \"95.0\" : 1949117.6912285273,\n                \"99.0\" : 1982288.7646156966,\n                \"99.9\" : 1982288.7646156966,\n                \"99.99\" : 1982288.7646156966,\n                \"99.999\" : 1982288.7646156966,\n                \"99.9999\" : 1982288.7646156966,\n                \"100.0\" : 1982288.7646156966\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    1696219.9125454635,\n                    1729623.9236061238,\n                    1610710.1801204153,\n                    1769414.6429731457,\n                    1667978.9665652176,\n                    1697235.2876620279,\n                    1615269.9628077352,\n                    1645186.2274296894,\n                    1609251.5083761679,\n                    1532777.2358683166,\n                    1656807.1692482952,\n                    1714353.3714364078,\n                    1632212.814084742,\n                    1602141.1933655317,\n                    1493645.6848657392,\n                    1728442.407270448,\n                    1663535.869794149,\n                    1871718.5199917993,\n                    1750647.1939896245,\n                    1782203.5382766959,\n                    1748345.4161347644,\n                    1863712.3286448088,\n                    1804973.0103171149,\n                    1823843.3541295172,\n                    1982288.7646156966\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-sequential/#results-sequential-h2-myjson","title":"results-sequential-H2-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 8192,\n  \"fileCount\" : 1,\n  \"usedMemoryBytes\" : 31557248,\n  \"cpuBefore\" : 894304000,\n  \"cpuAfter\" : 1925876000,\n  \"startTime\" : 1688895819370416,\n  \"endTime\" : 1689623148230958,\n  \"cpuUsage\" : 0.14183020308465147\n}\n</code></pre>"},{"location":"why-hestiastore/out-sequential/#results-sequential-h2json","title":"results-sequential-H2.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestH2Sequential.readSequential\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=H2Sequential\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 364686.8795874021,\n            \"scoreError\" : 43576.97512676437,\n            \"scoreConfidence\" : [\n                321109.9044606377,\n                408263.8547141665\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 241753.277608701,\n                \"50.0\" : 370385.7523692228,\n                \"90.0\" : 446029.59125580837,\n                \"95.0\" : 448741.1111258906,\n                \"99.0\" : 449399.69289311557,\n                \"99.9\" : 449399.69289311557,\n                \"99.99\" : 449399.69289311557,\n                \"99.999\" : 449399.69289311557,\n                \"99.9999\" : 449399.69289311557,\n                \"100.0\" : 449399.69289311557\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    370385.7523692228,\n                    408171.4525145759,\n                    447204.42033569893,\n                    364287.3952151101,\n                    449399.69289311557,\n                    398253.93659871473,\n                    442632.17906150565,\n                    426731.13119361637,\n                    402697.67651350296,\n                    376196.5657093395,\n                    362036.4249909497,\n                    403956.13641543523,\n                    297643.9869127228,\n                    342528.82342547516,\n                    309758.55097084236,\n                    270084.8363388006,\n                    328622.4460196818,\n                    241753.277608701,\n                    310612.8076248173,\n                    347948.04101550026,\n                    326277.84435612155,\n                    385128.5393856402,\n                    286516.83631126373,\n                    373096.8640354852,\n                    445246.37186921464\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-sequential/#results-sequential-hestiastorebasic-myjson","title":"results-sequential-HestiaStoreBasic-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 532617107,\n  \"fileCount\" : 10,\n  \"usedMemoryBytes\" : 31775872,\n  \"cpuBefore\" : 995124000,\n  \"cpuAfter\" : 2351232000,\n  \"startTime\" : 1694852769687083,\n  \"endTime\" : 1695740649510708,\n  \"cpuUsage\" : 0.15273553513845342\n}\n</code></pre>"},{"location":"why-hestiastore/out-sequential/#results-sequential-hestiastorebasicjson","title":"results-sequential-HestiaStoreBasic.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestHestiaStoreBasicSequential.readSequential\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=HestiaStoreBasicSequential\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 591.9892599976968,\n            \"scoreError\" : 68.32781871990737,\n            \"scoreConfidence\" : [\n                523.6614412777894,\n                660.3170787176041\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 217.07036397533005,\n                \"50.0\" : 614.7650530363439,\n                \"90.0\" : 665.1517330489332,\n                \"95.0\" : 670.7843310676988,\n                \"99.0\" : 672.2055421288844,\n                \"99.9\" : 672.2055421288844,\n                \"99.99\" : 672.2055421288844,\n                \"99.999\" : 672.2055421288844,\n                \"99.9999\" : 672.2055421288844,\n                \"100.0\" : 672.2055421288844\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    651.9451860971615,\n                    530.8595105637474,\n                    634.8872162131643,\n                    558.0429463121978,\n                    553.4575737726775,\n                    554.2370536905249,\n                    488.09095369510084,\n                    614.7650530363439,\n                    552.3371818406346,\n                    217.07036397533005,\n                    610.9353019515233,\n                    591.8184880269033,\n                    650.6265428969912,\n                    628.1819004580707,\n                    633.6255856581995,\n                    616.504191610874,\n                    587.2887115484536,\n                    663.6074404649339,\n                    639.37017713768,\n                    642.0884344099126,\n                    672.2055421288844,\n                    667.4681719249323,\n                    648.9143712953814,\n                    607.0026495079342,\n                    584.4009517248626\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-sequential/#results-sequential-hestiastorestream-myjson","title":"results-sequential-HestiaStoreStream-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 297736210,\n  \"fileCount\" : 10,\n  \"usedMemoryBytes\" : 31879856,\n  \"cpuBefore\" : 705656000,\n  \"cpuAfter\" : 1654368000,\n  \"startTime\" : 1738846165288208,\n  \"endTime\" : 1739625462973208,\n  \"cpuUsage\" : 0.12173935817607363\n}\n</code></pre>"},{"location":"why-hestiastore/out-sequential/#results-sequential-hestiastorestreamjson","title":"results-sequential-HestiaStoreStream.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestHestiaStoreCompressSequential2.readSequentialStream\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=HestiaStoreCompressSequential2\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 4792777.446329955,\n            \"scoreError\" : 144131.89712030266,\n            \"scoreConfidence\" : [\n                4648645.5492096525,\n                4936909.343450258\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 4253292.20242851,\n                \"50.0\" : 4793888.0753746815,\n                \"90.0\" : 4990279.774572669,\n                \"95.0\" : 5013138.2306711,\n                \"99.0\" : 5014745.693444231,\n                \"99.9\" : 5014745.693444231,\n                \"99.99\" : 5014745.693444231,\n                \"99.999\" : 5014745.693444231,\n                \"99.9999\" : 5014745.693444231,\n                \"100.0\" : 5014745.693444231\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    4940897.010906559,\n                    4811267.514070776,\n                    4793888.0753746815,\n                    4253292.20242851,\n                    4445228.221274129,\n                    4374010.689731631,\n                    4708902.625330204,\n                    4950020.989303642,\n                    4977541.301487473,\n                    4870900.688786804,\n                    5009387.484200462,\n                    4930047.971332142,\n                    4927043.182530843,\n                    4728505.515544281,\n                    5014745.693444231,\n                    4908131.309611777,\n                    4971694.40708881,\n                    4747109.83595018,\n                    4782049.903774503,\n                    4891229.719922479,\n                    4793055.605468624,\n                    4765577.03327685,\n                    4763952.21497315,\n                    4771201.630097632,\n                    4689755.332338488\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-sequential/#results-sequential-leveldb-myjson","title":"results-sequential-LevelDB-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 380967678,\n  \"fileCount\" : 195,\n  \"usedMemoryBytes\" : 31418048,\n  \"cpuBefore\" : 654382000,\n  \"cpuAfter\" : 1410956000,\n  \"startTime\" : 1697277383262041,\n  \"endTime\" : 1698020913951000,\n  \"cpuUsage\" : 0.10175423976907554\n}\n</code></pre>"},{"location":"why-hestiastore/out-sequential/#results-sequential-leveldbjson","title":"results-sequential-LevelDB.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestLevelDBSequential.readSequential\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=LevelDBSequential\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 190697.53218919376,\n            \"scoreError\" : 6693.958699915936,\n            \"scoreConfidence\" : [\n                184003.57348927783,\n                197391.4908891097\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 159796.8509345367,\n                \"50.0\" : 191873.73089029174,\n                \"90.0\" : 199452.94601770226,\n                \"95.0\" : 203332.36724773305,\n                \"99.0\" : 204914.9378197743,\n                \"99.9\" : 204914.9378197743,\n                \"99.99\" : 204914.9378197743,\n                \"99.999\" : 204914.9378197743,\n                \"99.9999\" : 204914.9378197743,\n                \"100.0\" : 204914.9378197743\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    196891.7955830414,\n                    204914.9378197743,\n                    197630.38717919897,\n                    199328.44164307916,\n                    199639.70257963688,\n                    159796.8509345367,\n                    179592.90318773626,\n                    177867.3502066716,\n                    187103.89915090462,\n                    195989.84190748248,\n                    198468.47083921975,\n                    194885.05032325233,\n                    195842.15284018134,\n                    191111.97083935005,\n                    186822.94025591918,\n                    189569.6139971288,\n                    191917.1063035356,\n                    187017.16397061732,\n                    185214.37271751274,\n                    188873.28992909336,\n                    192111.93632112874,\n                    189995.6940470095,\n                    194754.4095676854,\n                    190224.2916958571,\n                    191873.73089029174\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-sequential/#results-sequential-mapdb-myjson","title":"results-sequential-MapDB-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 1399848960,\n  \"fileCount\" : 1,\n  \"usedMemoryBytes\" : 31462464,\n  \"cpuBefore\" : 769731000,\n  \"cpuAfter\" : 3332809000,\n  \"startTime\" : 1689623718364083,\n  \"endTime\" : 1694851860176208,\n  \"cpuUsage\" : 0.04902464569831984\n}\n</code></pre>"},{"location":"why-hestiastore/out-sequential/#results-sequential-mapdbjson","title":"results-sequential-MapDB.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestMapDBSequential.readSequential\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=MapDBSequential\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 1527.9789090701086,\n            \"scoreError\" : 228.215230350306,\n            \"scoreConfidence\" : [\n                1299.7636787198026,\n                1756.1941394204146\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 1090.761945951313,\n                \"50.0\" : 1411.1236203864682,\n                \"90.0\" : 1971.777273993194,\n                \"95.0\" : 1995.0192649747119,\n                \"99.0\" : 2001.3620984483193,\n                \"99.9\" : 2001.3620984483193,\n                \"99.99\" : 2001.3620984483193,\n                \"99.999\" : 2001.3620984483193,\n                \"99.9999\" : 2001.3620984483193,\n                \"100.0\" : 2001.3620984483193\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    1980.2193202029612,\n                    1799.0554072176392,\n                    1746.3774023452725,\n                    1966.0394570221047,\n                    2001.3620984483193,\n                    1955.017761157936,\n                    1966.1492431866825,\n                    1753.9545467605956,\n                    1724.4810704849083,\n                    1777.462895610476,\n                    1445.6085835239599,\n                    1411.1236203864682,\n                    1474.871640194682,\n                    1156.897990609939,\n                    1127.2417106851822,\n                    1322.745669290138,\n                    1268.1836611199321,\n                    1090.761945951313,\n                    1175.9389600674933,\n                    1375.5142570523617,\n                    1317.1282852871068,\n                    1395.8129273532918,\n                    1292.3779771455322,\n                    1326.9411137886998,\n                    1348.205181859728\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-sequential/#results-sequential-rocksdb-myjson","title":"results-sequential-RocksDB-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 339977964,\n  \"fileCount\" : 12,\n  \"usedMemoryBytes\" : 31820496,\n  \"cpuBefore\" : 764980000,\n  \"cpuAfter\" : 1492221000,\n  \"startTime\" : 1696542435579291,\n  \"endTime\" : 1697276855630958,\n  \"cpuUsage\" : 0.09902248697449029\n}\n</code></pre>"},{"location":"why-hestiastore/out-sequential/#results-sequential-rocksdbjson","title":"results-sequential-RocksDB.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestRocksDBSequential.readSequential\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=RocksDBSequential\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 109550.92199554075,\n            \"scoreError\" : 10512.710963722118,\n            \"scoreConfidence\" : [\n                99038.21103181863,\n                120063.63295926286\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 84995.28542762823,\n                \"50.0\" : 105854.72446603306,\n                \"90.0\" : 134000.91427096617,\n                \"95.0\" : 136675.9566224093,\n                \"99.0\" : 136716.71773725914,\n                \"99.9\" : 136716.71773725914,\n                \"99.99\" : 136716.71773725914,\n                \"99.999\" : 136716.71773725914,\n                \"99.9999\" : 136716.71773725914,\n                \"100.0\" : 136716.71773725914\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    96192.28900175112,\n                    98677.08420993124,\n                    102449.99957338264,\n                    105634.93036960272,\n                    102474.29135688648,\n                    113627.64127716908,\n                    114310.33693810109,\n                    114442.7830600832,\n                    119967.77141802286,\n                    112527.36307250132,\n                    114575.93837877734,\n                    117137.70558750023,\n                    105854.72446603306,\n                    84995.28542762823,\n                    93430.56299680991,\n                    93438.21018397433,\n                    99910.11169119111,\n                    117520.68636346157,\n                    99931.16898666933,\n                    94249.24893787173,\n                    102407.58589720975,\n                    129438.80672028179,\n                    132280.95888199267,\n                    136580.8473544264,\n                    136716.71773725914\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-write/","title":"\ud83d\udcca HestiaStore Benchmark Results","text":""},{"location":"why-hestiastore/out-write/#test-conditions","title":"\ud83e\uddea Test Conditions","text":"<ul> <li>Every benchmark in the plain-load suite runs inside the same controlled JVM environment with identical JVM flags and hardware resources. Runs start by wiping the working directory supplied through the <code>dir</code> system property, so each trial writes into a fresh, empty location.</li> <li>Execution stays single-threaded from warm-up through measurement. The test focuses purely on how quickly one writer can push key/value pairs into the storage engine without any coordination overhead from additional threads.</li> <li>Warm-up phases fill the database as aggressively as possible for several 20-second stretches. This stage is meant to trigger JIT compilation, populate caches, and let LevelDB settle into steady-state behaviour before any numbers are recorded.</li> <li>Measurement phases repeat the same single-threaded write loop. Throughput is observed over multiple 20-second intervals to capture stable, sustained insert performance rather than a burst.</li> <li>Each write operation uses a deterministic pseudo-random long (seed <code>324432L</code>) to generate a unique hash string via <code>HashDataProvider</code>. The payload is the constant text <code>\"opice skace po stromech\"</code>, so variability comes exclusively from the changing keys.</li> <li>After measurements complete, the map is closed and the directory remains available for inspection. The log records how many keys were created, providing a quick sanity check that the run processed the expected volume.</li> <li>Test was performed at Mac mini 2024, 16 GB, macOS 15.6.1 (24G90).</li> </ul>"},{"location":"why-hestiastore/out-write/#benchmark-results","title":"\ud83c\udfc1 Benchmark Results","text":"Engine Score [ops/s] ScoreError Confidence Interval [ops/s] Occupied space CPU Usage ChronicleMap 5 954 1 765 4 189 .. 7 719 20.54 GB 7% H2 13 458 5 144 8 314 .. 18 601 8 KB 21% LevelDB 45 263 10 913 34 350 .. 56 176 1.4 GB 17% MapDB 2 946 326 2 620 .. 3 272 496 MB 14% RocksDB 305 712 78 929 226 783 .. 384 641 7.74 GB 6% <p>meaning of columns:</p> <ul> <li>Engine: name of the benchmarked engine (as derived from the JSON filename)</li> <li>Score [ops/s]: number of operations per second (higher is better)</li> <li>ScoreError: error margin of the score (lower is better). It's computed as <code>z * (stdev / sqrt(n)) where</code></li> <li><code>z</code> is the z-score for the desired confidence level (1.96 for 95%)</li> <li><code>stdev</code> is the standard deviation of the measurements</li> <li><code>n</code> is the number of measurements</li> <li>Confidence Interval: 95% confidence interval of the score (lower and upper bound). This means that the true mean is likely between this interval of ops/sec. Negative values are possible if the error margin is larger than the score itself.</li> <li>Occupied space : amount of disk space occupied by the engine's data structures (lower is better). It is measured after flushing last data to disk.</li> <li>CPU Usage: average CPU usage during the benchmark (lower is better). Please note, that it includes all system processes, not only the benchmarked engine.</li> </ul>"},{"location":"why-hestiastore/out-write/#raw-json-files","title":"\ud83d\udcc4 Raw JSON Files","text":""},{"location":"why-hestiastore/out-write/#results-write-chroniclemap-myjson","title":"results-write-ChronicleMap-my.json","text":"<pre><code>{\n    \"totalDirectorySize\": 22049472512,\n    \"fileCount\": 1,\n    \"usedMemoryBytes\": 27947104,\n    \"cpuBefore\": 603274000,\n    \"cpuAfter\": 1097938000,\n    \"startTime\": 261108533575583,\n    \"endTime\": 261814614016416,\n    \"cpuUsage\": 0.07005774008077877\n}\n</code></pre>"},{"location":"why-hestiastore/out-write/#results-write-chroniclemapjson","title":"results-write-ChronicleMap.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestChronicleMap.write\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Xmx10000m\",\n            \"--add-opens=java.base/java.lang=ALL-UNNAMED\",\n            \"--add-opens=java.base/java.lang.reflect=ALL-UNNAMED\",\n            \"--add-opens=java.base/java.io=ALL-UNNAMED\",\n            \"--add-opens=java.base/java.nio=ALL-UNNAMED\",\n            \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.code=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.comp=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.main=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.model=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.processing=ALL-UNNAMED\",\n            \"--add-opens=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED\",\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=ChronicleMap\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 5954.200373600311,\n            \"scoreError\" : 1764.90521298729,\n            \"scoreConfidence\" : [\n                4189.295160613021,\n                7719.105586587601\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 3329.640674916557,\n                \"50.0\" : 5243.630014833653,\n                \"90.0\" : 10595.405308112338,\n                \"95.0\" : 12253.872048619854,\n                \"99.0\" : 12744.707905617659,\n                \"99.9\" : 12744.707905617659,\n                \"99.99\" : 12744.707905617659,\n                \"99.999\" : 12744.707905617659,\n                \"99.9999\" : 12744.707905617659,\n                \"100.0\" : 12744.707905617659\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    11108.58838229165,\n                    12744.707905617659,\n                    10253.283258659461,\n                    7641.776148398347,\n                    7065.625130993228,\n                    6245.862123216896,\n                    5726.337056869982,\n                    5546.963290190697,\n                    5312.460633150494,\n                    5243.630014833653,\n                    5127.273627636177,\n                    4443.333421250085,\n                    4570.51338982323,\n                    4905.02077335985,\n                    5196.530108141268,\n                    3329.640674916557,\n                    3829.198465291546,\n                    7600.50765102385,\n                    6131.287347339458,\n                    5543.996756565399,\n                    4995.077322180827,\n                    4772.012372724426,\n                    3916.673020709464,\n                    3514.748215845972,\n                    4089.96224897762\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-write/#results-write-h2-myjson","title":"results-write-H2-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 8192,\n  \"fileCount\" : 1,\n  \"usedMemoryBytes\" : 27133320,\n  \"cpuBefore\" : 1035200000,\n  \"cpuAfter\" : 2479448000,\n  \"startTime\" : 258786636432958,\n  \"endTime\" : 259488720650875,\n  \"cpuUsage\" : 0.2057086547657931\n}\n</code></pre>"},{"location":"why-hestiastore/out-write/#results-write-h2json","title":"results-write-H2.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestH2.write\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Xmx10000m\",\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=H2\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 13457.521378452284,\n            \"scoreError\" : 5143.803121401257,\n            \"scoreConfidence\" : [\n                8313.718257051027,\n                18601.32449985354\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 4492.791786534924,\n                \"50.0\" : 13209.02178518997,\n                \"90.0\" : 21853.77922605105,\n                \"95.0\" : 29631.650534341064,\n                \"99.0\" : 32749.29402436372,\n                \"99.9\" : 32749.29402436372,\n                \"99.99\" : 32749.29402436372,\n                \"99.999\" : 32749.29402436372,\n                \"99.9999\" : 32749.29402436372,\n                \"100.0\" : 32749.29402436372\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    12623.654285480005,\n                    11839.343622462286,\n                    14436.728477425342,\n                    13633.043439408131,\n                    16157.467112454447,\n                    6412.401044547184,\n                    6265.687494095522,\n                    18612.922946912684,\n                    16219.969068239285,\n                    12831.893972457165,\n                    18859.80952879762,\n                    4813.230127324451,\n                    13209.02178518997,\n                    22357.14905762156,\n                    18073.648245485118,\n                    8538.249998991463,\n                    32749.29402436372,\n                    18767.680908037615,\n                    21518.199338337377,\n                    8642.521511716302,\n                    17435.121051959966,\n                    7154.46408403396,\n                    4670.397715531453,\n                    4492.791786534924,\n                    6123.3438338994565\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-write/#results-write-hestiastorebasic-myjson","title":"results-write-HestiaStoreBasic-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 3889698398,\n  \"fileCount\" : 38,\n  \"usedMemoryBytes\" : 28606416,\n  \"cpuBefore\" : 552320000,\n  \"cpuAfter\" : 729480000,\n  \"startTime\" : 1401225500145958,\n  \"endTime\" : 1401225903588125,\n  \"cpuUsage\" : 43.91211789222816\n}\n</code></pre>"},{"location":"why-hestiastore/out-write/#results-write-hestiastorebasicjson","title":"results-write-HestiaStoreBasic.json","text":"<pre><code>[\n]\n</code></pre>"},{"location":"why-hestiastore/out-write/#results-write-hestiastorecompress-myjson","title":"results-write-HestiaStoreCompress-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 3889698398,\n  \"fileCount\" : 38,\n  \"usedMemoryBytes\" : 28689648,\n  \"cpuBefore\" : 529915000,\n  \"cpuAfter\" : 697688000,\n  \"startTime\" : 1401226232499375,\n  \"endTime\" : 1401226604772458,\n  \"cpuUsage\" : 45.067185263029074\n}\n</code></pre>"},{"location":"why-hestiastore/out-write/#results-write-hestiastorecompressjson","title":"results-write-HestiaStoreCompress.json","text":"<pre><code>[\n]\n</code></pre>"},{"location":"why-hestiastore/out-write/#results-write-leveldb-myjson","title":"results-write-LevelDB-my.json","text":"<pre><code>{\n    \"totalDirectorySize\": 1508330468,\n    \"fileCount\": 754,\n    \"usedMemoryBytes\": 27954256,\n    \"cpuBefore\": 1001444000,\n    \"cpuAfter\": 2178345000,\n    \"startTime\": 260193098308000,\n    \"endTime\": 260895356813250,\n    \"cpuUsage\": 0.1675880023099229\n}\n</code></pre>"},{"location":"why-hestiastore/out-write/#results-write-leveldbjson","title":"results-write-LevelDB.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestLevelDB.write\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Xmx10000m\",\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=LevelDB\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 45262.57575204393,\n            \"scoreError\" : 10913.06838787145,\n            \"scoreConfidence\" : [\n                34349.50736417248,\n                56175.64413991538\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 29965.68796327744,\n                \"50.0\" : 44015.87124729542,\n                \"90.0\" : 59895.29706634639,\n                \"95.0\" : 73862.46532443774,\n                \"99.0\" : 79758.10180257274,\n                \"99.9\" : 79758.10180257274,\n                \"99.99\" : 79758.10180257274,\n                \"99.999\" : 79758.10180257274,\n                \"99.9999\" : 79758.10180257274,\n                \"100.0\" : 79758.10180257274\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    79758.10180257274,\n                    39978.85200469439,\n                    59754.841638050995,\n                    30051.495747236888,\n                    59710.167696545956,\n                    30759.864807986865,\n                    59261.071048612,\n                    30156.92824167015,\n                    45828.62436018362,\n                    44015.87124729542,\n                    30051.946454605757,\n                    42438.257643466684,\n                    47017.87854596809,\n                    30308.896587398805,\n                    59599.16125957037,\n                    29965.68796327744,\n                    30159.1527061617,\n                    59564.58477647708,\n                    30361.845682382078,\n                    30347.55964289471,\n                    59253.07772711731,\n                    32183.0951788089,\n                    57862.32461600187,\n                    60105.98020878948,\n                    53069.126213328724\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-write/#results-write-mapdb-myjson","title":"results-write-MapDB-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 520093696,\n  \"fileCount\" : 1,\n  \"usedMemoryBytes\" : 27726856,\n  \"cpuBefore\" : 1110727000,\n  \"cpuAfter\" : 2088963000,\n  \"startTime\" : 259489732026041,\n  \"endTime\" : 260192350201750,\n  \"cpuUsage\" : 0.13922725511802747\n}\n</code></pre>"},{"location":"why-hestiastore/out-write/#results-write-mapdbjson","title":"results-write-MapDB.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestMapDB.write\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Xmx10000m\",\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=MapDB\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 2945.9818242698652,\n            \"scoreError\" : 325.59501594188,\n            \"scoreConfidence\" : [\n                2620.3868083279854,\n                3271.576840211745\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 2272.884921159144,\n                \"50.0\" : 2903.932835903283,\n                \"90.0\" : 3477.3894675973625,\n                \"95.0\" : 3980.5774803791674,\n                \"99.0\" : 4191.114455891415,\n                \"99.9\" : 4191.114455891415,\n                \"99.99\" : 4191.114455891415,\n                \"99.999\" : 4191.114455891415,\n                \"99.9999\" : 4191.114455891415,\n                \"100.0\" : 4191.114455891415\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    3355.129200069919,\n                    4191.114455891415,\n                    3489.3245375172587,\n                    2438.7143275091894,\n                    2579.0322123944043,\n                    3197.038563619707,\n                    3107.3043335466377,\n                    3384.5420569152025,\n                    3026.214223864194,\n                    2955.4727465665437,\n                    2938.449490331077,\n                    2444.3275358242336,\n                    2447.5359296109964,\n                    2572.5750258119742,\n                    2629.0978967032243,\n                    2641.6700332863193,\n                    2903.932835903283,\n                    2821.317530851881,\n                    2272.884921159144,\n                    2668.589365207391,\n                    3178.0194113129796,\n                    3265.484517502896,\n                    3469.4327543174313,\n                    2897.2634039647874,\n                    2775.078297064553\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/out-write/#results-write-rocksdb-myjson","title":"results-write-RocksDB-my.json","text":"<pre><code>{\n  \"totalDirectorySize\" : 8306458361,\n  \"fileCount\" : 143,\n  \"usedMemoryBytes\" : 29881472,\n  \"cpuBefore\" : 524178000,\n  \"cpuAfter\" : 978766000,\n  \"startTime\" : 256399236974125,\n  \"endTime\" : 257100509331250,\n  \"cpuUsage\" : 0.0648233165590143\n}\n</code></pre>"},{"location":"why-hestiastore/out-write/#results-write-rocksdbjson","title":"results-write-RocksDB.json","text":"<pre><code>[\n    {\n        \"jmhVersion\" : \"1.37\",\n        \"benchmark\" : \"org.hestiastore.index.benchmark.plainload.TestRocksDB.write\",\n        \"mode\" : \"thrpt\",\n        \"threads\" : 1,\n        \"forks\" : 1,\n        \"jvm\" : \"/opt/homebrew/Cellar/openjdk@21/21.0.7/libexec/openjdk.jdk/Contents/Home/bin/java\",\n        \"jvmArgs\" : [\n            \"-Ddir=/Volumes/ponrava/test-index\",\n            \"-Dengine=RocksDB\"\n        ],\n        \"jdkVersion\" : \"21.0.7\",\n        \"vmName\" : \"OpenJDK 64-Bit Server VM\",\n        \"vmVersion\" : \"21.0.7\",\n        \"warmupIterations\" : 10,\n        \"warmupTime\" : \"20 s\",\n        \"warmupBatchSize\" : 1,\n        \"measurementIterations\" : 25,\n        \"measurementTime\" : \"20 s\",\n        \"measurementBatchSize\" : 1,\n        \"primaryMetric\" : {\n            \"score\" : 305711.8304543295,\n            \"scoreError\" : 78928.99893168075,\n            \"scoreConfidence\" : [\n                226782.83152264875,\n                384640.82938601024\n            ],\n            \"scorePercentiles\" : {\n                \"0.0\" : 64828.117457250206,\n                \"50.0\" : 303051.53710396215,\n                \"90.0\" : 440196.02218382317,\n                \"95.0\" : 451632.3622033773,\n                \"99.0\" : 452870.0724388766,\n                \"99.9\" : 452870.0724388766,\n                \"99.99\" : 452870.0724388766,\n                \"99.999\" : 452870.0724388766,\n                \"99.9999\" : 452870.0724388766,\n                \"100.0\" : 452870.0724388766\n            },\n            \"scoreUnit\" : \"ops/s\",\n            \"rawData\" : [\n                [\n                    452870.0724388766,\n                    403842.93155871733,\n                    294725.1949589473,\n                    344879.7186297531,\n                    252451.31054124268,\n                    270367.7563971078,\n                    265577.0790884009,\n                    211979.36739862378,\n                    70666.26824904919,\n                    414677.1507219246,\n                    64828.117457250206,\n                    405398.2212372984,\n                    386439.0605128443,\n                    419727.7994319734,\n                    434497.12253711926,\n                    448744.3716538789,\n                    235897.63938900485,\n                    297712.0697930812,\n                    303794.303173526,\n                    360040.8711513881,\n                    257499.74121652535,\n                    303051.53710396215,\n                    185201.4305266164,\n                    303600.34376763576,\n                    254326.28242349048\n                ]\n            ]\n        },\n        \"secondaryMetrics\" : {\n        }\n    }\n]\n</code></pre>"},{"location":"why-hestiastore/quality/","title":"\u2705 Quality &amp; Testing","text":"<p>An overview of how HestiaStore ensures consistent quality:</p> <ul> <li>Automated CI builds and test execution on every change.</li> <li>Code coverage reporting and trend tracking.</li> <li>Static analysis and quality gates (e.g., SonarCloud).</li> <li>Dependency vulnerability scanning (OWASP Dependency Check).</li> </ul> <p>Links and details to be expanded.</p>"}]}